---
layout: event
title: "Allegro Tech Talks Warszawa #7 - BigData"
time: 1542128400000
venue_address_1: al. Jana Pawła II 22, Warszawa
venue_city: Warsaw
venue_name: Allegro Office Q22
status: past
id: 255992902
registration: 
---

<p>Cześć,
  <br/>zapraszamy na 7 spotkanie w ramach Allegro Tech Talks w Warszawie! Tym razem tematem przewodnim będzie BigData!</p>
<p>REJESTRACJA:
  <br/>
  <a href="https://www.eventbrite.com/e/allegro-tech-talks-warszawa-7-tickets-52039666042" class="linkified">https://www.eventbrite.com/e/allegro-tech-talks-warszawa-7-tickets-52039666042</a> - prosimy o rejestrację poprzez przycisk "Register" przy opisie wydarzenia. Po zarejestrowaniu otrzymasz e-mail z potwierdzeniem oraz Twoim biletem.</p>
<p>Agenda:
  <br/>17:45 - rejestracja + pizza</p>
<p>18:15 - przywitanie</p>
<p>18:15-19:00 - Przetwarzania realtime dla analityków i data scientistów (+Q&amp;A) - Marcin Kuthan</p>
<p>19:00 - 19:45 - Ewolucja czy rewolucja? GCP + BigQuery + Airflow (+Q&amp;A) - Tomek Klaudel</p>
<p>1. Przetwarzania realtime dla analityków i data scientistów</p>
<p>Przetwarzania realtime w dużej skali są zazwyczaj domeną “data engineers”, wymagają sporej wiedzy technicznej o frameworkach oraz doświadczenia w tworzeniu, uruchamianiu i monitorowaniu przetwarzań. Pokażemy wam jak przetwarzamy miliardy zdarzeń dziennie
  za pomocą Kafka Streams uruchamianych na klastrze Mesos, oraz w jaki sposób skalujemy i monitorujemy ich działanie. Dowiecie się jakie wyzwania pojawiają się przy próbie prostego złączenia dwóch strumieni danych. Opowiemy też w jaki sposób ładujemy
  tak przetworzone dane na klaster Apache Druid i do czego je wykorzystujemy, zobaczycie na żywo co klikają klienci Allegro w trakcie naszego meetup-u.</p>
<p>Ale to nie koniec opowieści, chcemy aby przetwarzania realtime były dostępne dla zespołów, które nie chcą doktoryzować się we frameworkach do przetwarzań. Chcemy aby strony Allegro automatycznie dostosowywały treść do aktualnych potrzeb klientów. Oznacza
  to, że trzeba wpuścić analityków i data scientistów na szynę danych i dać im narzędzia do prostego przetwarzania danych “realtime”, a jednocześnie nie puścić serwerowni z dymem.</p>
<p>Czy rozwiązania z Google Cloud Platform takie jak Pub/Sub, Dataflow czy BigQuery są odpowiedzią na nasze potrzeby?</p>
<p>Marcin Kuthan
  <br/>Inżynier big data, lider zespołu odpowiedzialnego za zbieranie danych clickstream z platformy Allegro (web i mobile). Wraz z zespołem wdrażał usługi oparte na Kafka Streams, oraz uruchamiał klaster Apache Druid na potrzeby analityki. Wolne chwile spędza
  na rowerze.</p>
<p>2. Ewolucja czy rewolucja? GCP + BigQuery + Airflow</p>
<p>Dlaczego warto rozważyć migrację do publicznej chmury - na przykładzie GCP. Dotychczas mieliśmy kilka podejść do stworzenia spójnego środowiska dla analityków i zespołów deweloperskich. Dotychczasowe narzędzia - własny klaster Hadoop, Hive, Hue, Oozie.
  Następnie osobne DC, Presto, Alation, in-house Airflow i synchronizacja danych. Oba podejścia wymagają sporych nakładów pracy w utrzymaniu, rozwiązywaniu problemów.</p>
<p>Sytuacja wraz z dorastaniem narzędzi wokół BigData bardzo się zmieniła. Wygląda na to, że narzędzia dostępne w Google Cloud Platform adresują bardzo dużo naszych dotychczasowych „bolączek” - wydajność, skalowalność, spójność środowiska.</p>
<p>Oczywiście nie obyło się bez problemów. Chcemy się podzielić tym co odkryliśmy, jak podejść do migracji i o czym pamiętać w takim przypadku. Skupimy się na Google’owej wersji Apache Airflow - Composer. Obejrzymy sobie też BigQuery oraz jak synchronizujemy
  dane między naszym klastrem a GCP.</p>
<p>Opowiemy też jak wspomóc zespoły deweloperskie w korzystaniu z GCP w sytuacji, gdy mamy 2 (lub więcej) datacenter.</p>
<p>Tomek Klaudel
  <br/>Inżynier big data, lider zespołu przygotowującego dane dla innych zespołów, nie tylko analitycznych. Zespół przetwarza dane, tworzy narzędzia dla innych zespołów, żeby wspomóc je w rozpoczęciu pracy z danymi. W czasie wolnym - ojciec.</p>
