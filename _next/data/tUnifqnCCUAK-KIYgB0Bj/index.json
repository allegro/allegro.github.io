{"pageProps":{"posts":[{"title":"My first days at Allegro","link":"https://blog.allegro.tech/2021/02/first-days-at-allegro.html","pubDate":"Mon, 15 Feb 2021 00:00:00 +0100","authors":{"author":[{"name":["Krzysztof Przychodzki"],"photo":["https://blog.allegro.tech/img/authors/krzysztof.przychodzki.jpg"],"url":["https://blog.allegro.tech/authors/krzysztof.przychodzki"]}]},"content":"<p>So you’re new to <a href=\"https://blog.allegro.tech/about-us/\">Allegro</a>, have just finished your tech onboarding and are stunned with information overflow? Or perhaps you\nare planning to join Allegro and don’t know what it looks like in here? I am about to try and describe how I felt just a few months ago and what startled or\ndismayed me. I hope this short article will answer all your concerns.</p>\n\n<p>But first I would like to give you a little background of my professional work life.</p>\n\n<h2 id=\"background\">Background</h2>\n\n<p>Long before becoming a developer, I was a chemical engineer, and I was designing distilleries, cosmetic and oil refinery plants, etc. During my work, I often\nused Microsoft Excel and VBA as my main apps for solving complex problems. I enjoyed it more than the job I was assigned. So I decided to take a Java bootcamp\nand afterwards I got hired by an IT company in my area. At my first IT job, I was hired as a junior, but I was already treated as a regular programmer.\nIt was nice, I owe them a lot, but on the other hand the only feedback I got was when something was not working. From the beginning, I was fully aware of how\nbasic my knowledge was after the bootcamp. While catching up I realized that at this company I would not develop anymore, and I would be working with legacy\ntechnologies throughout the rest of my career.</p>\n\n<h2 id=\"a-piece-of-cake\">A piece of cake?</h2>\n\n<p>The recruitment process at Allegro — let me tell you straight, is not a piece of cake. I am pretty sure that a year after the recruitment meeting you are\nstill going to remember the questions you were torpedoed with.</p>\n\n<p>First of all — after applying, you will receive an email with all the necessary information about the recruitment process. Each step is briefly\ncharacterized, so you know what is going to happen next and what to expect. The first step is a task on DevSkiller. Then, you will take part in technical\ninterviews in the form of conversation about the technologies and architecture of modern IT systems, touching on aspects of design, performance, monitoring, etc.\nIt was very different from the previous recruitment processes in which I sometimes spoke to people who had no clue what I was talking about. At Allegro I really\nappreciate that these meetings are with real professionals! What is more, during my recruitments I have never encountered such a human approach. They treat you\nlike a human, not like a robot. That’s cool. In the last step you will meet with a team leader and people from HR.</p>\n\n<h2 id=\"first-days\">First days</h2>\n\n<p>My first days at Allegro — what a rollercoaster, considering the current coronavirus situation. Onboarding normally takes 3 days and is conducted at the\ncompany’s headquarters in Poznań. There is another two-day long technical onboarding for technical employees at the workplace. However, due to the COVID-19\nsituation, the entire onboarding process was carried out remotely. This tech onboarding is conducted with a workshop on how to create a new service and\neverything that goes with it like deploying, running and maintaining. I understood why (this was not possible remotely), but it is a pity that it did not take\nplace. Fortunately, everybody wants to help you with any issue you have — and this is the great power of Allegro: people and their readiness to help.\nIt is natural for everyone that a new person needs support and time to get to know the organization. Moreover, if you are a junior like me, and have never\nwritten a line of code for example in Kotlin, you can spend some time learning a new programming language and everybody is okay with that.</p>\n\n<p>Someone might ask: What is difficult in the beginning? That depends, because everything is new. I work in a team responsible for Allegro Smart! loyalty\nprogramme — ensuring proper marking of offers qualified for free delivery — so the most confusing thing for me was the business complexity of those\nprocesses.</p>\n\n<p>Nevertheless, there are over 1200 microservices communicating with each other — a status quo you have to deal with. Thousands of decisions were made and\na lot of them don’t sound reasonable when you first hear about them, but after a day passes, everything starts to clarify. I think this is what everyone deals\nwith when they come to a new place.</p>\n\n<h2 id=\"somebody-is-reading-my-code\">Somebody is reading my code</h2>\n\n<p>One of my favorite things at Allegro is code review. Virtually nothing gets merged unless it is reviewed and approved. Code review is mandatory and necessary\n— reading others’ code is not only about finding bugs, it is mainly to provide code that is clear, understandable, and maintainable. For me, it is mainly\nfor learning and better understanding our services and business domain.</p>\n\n<p>Another great thing is pair programming. During these sessions it is easier to understand the business domain and to catch the wider context of our services.</p>\n\n<h2 id=\"unit-integration-and-end-to-end-testing\">Unit, integration, and end-to-end testing</h2>\n\n<p>As I wrote, unreviewed code will not be deployed to production and without tests it is not going to pass the review. At Allegro, each change in code needs\nto be tested. We write unit and integration tests, and we work with two test environments.</p>\n\n<p>One is totally a developer’s playground where you make your ‘little Allegro’. It is called phoenix. Every team has its own phoenix env for\nexperiments. However, it has some issues. Since there are so many dependencies to other services, your already set up environment may not work properly until it\nis manually updated. So a very common situation is that before you start testing your change, you need to spend some time to get the whole environment working.\nThis is frustrating, especially in the beginning.</p>\n\n<p>The second one is a pre-prod sandbox — it is like normal Allegro, but unlike the dev environment, the sandbox is more consistent and works almost like\nprod. So there are a lot of possibilities to test your change and it’s good to have this feeling of confidence.</p>\n\n<p>Sometimes despite all these tests, code reviews, etc. a mistake happens — the app is already deployed to production, and our clients are complaining.\nWe have to act quickly to fix the error. I really appreciate that we look for bugs, not the guilty party. When somebody makes a mistake, we don’t blame each\nother, but we look for the best solution to the problem and fix it.</p>\n\n<h2 id=\"hack-the-day\">Hack the day</h2>\n\n<p>Sometimes teams do internal hackathons (called fedex-days) — we divide into two or three teams and we work on subjects that we choose. We want\nto try a new programming language — we just do it; make an application for sharing memes — perfectly fine; write an extension for Slack — why\nnot, go have some fun! Usually, we spend two working days getting off work. That’s very refreshing.</p>\n\n<p>I also know that once in a while there are hackathons for the whole Allegro — but I didn’t have a chance to participate.</p>\n\n<h2 id=\"dobrze-tu-być\">Dobrze tu być?</h2>\n\n<p>At Allegro we all understand the great importance of ensuring the code we are working on is of the highest quality. We care about our services to the point\nwhere we sometimes spend hours discussing if it is better to throw an exception or just 404?</p>\n\n<p>However, Allegro is not only about programming, it is a place with a lot of experts in many other disciplines — and every one of us has a straightforward\ngoal to make Allegro the best place/platform not only for shoppers and sellers, but also for each other.</p>\n\n<p>Hope you enjoyed this article. I know it sounds like a chorus of praise, but in my situation it arises from my work experiences — for me, it is really a\n“Good to be here!” place. Or as we say <a href=\"https://www.linkedin.com/company/allegro-pl/life/team\">#dobrzetubyć</a>.</p>\n","contentSnippet":"So you’re new to Allegro, have just finished your tech onboarding and are stunned with information overflow? Or perhaps you\nare planning to join Allegro and don’t know what it looks like in here? I am about to try and describe how I felt just a few months ago and what startled or\ndismayed me. I hope this short article will answer all your concerns.\nBut first I would like to give you a little background of my professional work life.\nBackground\nLong before becoming a developer, I was a chemical engineer, and I was designing distilleries, cosmetic and oil refinery plants, etc. During my work, I often\nused Microsoft Excel and VBA as my main apps for solving complex problems. I enjoyed it more than the job I was assigned. So I decided to take a Java bootcamp\nand afterwards I got hired by an IT company in my area. At my first IT job, I was hired as a junior, but I was already treated as a regular programmer.\nIt was nice, I owe them a lot, but on the other hand the only feedback I got was when something was not working. From the beginning, I was fully aware of how\nbasic my knowledge was after the bootcamp. While catching up I realized that at this company I would not develop anymore, and I would be working with legacy\ntechnologies throughout the rest of my career.\nA piece of cake?\nThe recruitment process at Allegro — let me tell you straight, is not a piece of cake. I am pretty sure that a year after the recruitment meeting you are\nstill going to remember the questions you were torpedoed with.\nFirst of all — after applying, you will receive an email with all the necessary information about the recruitment process. Each step is briefly\ncharacterized, so you know what is going to happen next and what to expect. The first step is a task on DevSkiller. Then, you will take part in technical\ninterviews in the form of conversation about the technologies and architecture of modern IT systems, touching on aspects of design, performance, monitoring, etc.\nIt was very different from the previous recruitment processes in which I sometimes spoke to people who had no clue what I was talking about. At Allegro I really\nappreciate that these meetings are with real professionals! What is more, during my recruitments I have never encountered such a human approach. They treat you\nlike a human, not like a robot. That’s cool. In the last step you will meet with a team leader and people from HR.\nFirst days\nMy first days at Allegro — what a rollercoaster, considering the current coronavirus situation. Onboarding normally takes 3 days and is conducted at the\ncompany’s headquarters in Poznań. There is another two-day long technical onboarding for technical employees at the workplace. However, due to the COVID-19\nsituation, the entire onboarding process was carried out remotely. This tech onboarding is conducted with a workshop on how to create a new service and\neverything that goes with it like deploying, running and maintaining. I understood why (this was not possible remotely), but it is a pity that it did not take\nplace. Fortunately, everybody wants to help you with any issue you have — and this is the great power of Allegro: people and their readiness to help.\nIt is natural for everyone that a new person needs support and time to get to know the organization. Moreover, if you are a junior like me, and have never\nwritten a line of code for example in Kotlin, you can spend some time learning a new programming language and everybody is okay with that.\nSomeone might ask: What is difficult in the beginning? That depends, because everything is new. I work in a team responsible for Allegro Smart! loyalty\nprogramme — ensuring proper marking of offers qualified for free delivery — so the most confusing thing for me was the business complexity of those\nprocesses.\nNevertheless, there are over 1200 microservices communicating with each other — a status quo you have to deal with. Thousands of decisions were made and\na lot of them don’t sound reasonable when you first hear about them, but after a day passes, everything starts to clarify. I think this is what everyone deals\nwith when they come to a new place.\nSomebody is reading my code\nOne of my favorite things at Allegro is code review. Virtually nothing gets merged unless it is reviewed and approved. Code review is mandatory and necessary\n— reading others’ code is not only about finding bugs, it is mainly to provide code that is clear, understandable, and maintainable. For me, it is mainly\nfor learning and better understanding our services and business domain.\nAnother great thing is pair programming. During these sessions it is easier to understand the business domain and to catch the wider context of our services.\nUnit, integration, and end-to-end testing\nAs I wrote, unreviewed code will not be deployed to production and without tests it is not going to pass the review. At Allegro, each change in code needs\nto be tested. We write unit and integration tests, and we work with two test environments.\nOne is totally a developer’s playground where you make your ‘little Allegro’. It is called phoenix. Every team has its own phoenix env for\nexperiments. However, it has some issues. Since there are so many dependencies to other services, your already set up environment may not work properly until it\nis manually updated. So a very common situation is that before you start testing your change, you need to spend some time to get the whole environment working.\nThis is frustrating, especially in the beginning.\nThe second one is a pre-prod sandbox — it is like normal Allegro, but unlike the dev environment, the sandbox is more consistent and works almost like\nprod. So there are a lot of possibilities to test your change and it’s good to have this feeling of confidence.\nSometimes despite all these tests, code reviews, etc. a mistake happens — the app is already deployed to production, and our clients are complaining.\nWe have to act quickly to fix the error. I really appreciate that we look for bugs, not the guilty party. When somebody makes a mistake, we don’t blame each\nother, but we look for the best solution to the problem and fix it.\nHack the day\nSometimes teams do internal hackathons (called fedex-days) — we divide into two or three teams and we work on subjects that we choose. We want\nto try a new programming language — we just do it; make an application for sharing memes — perfectly fine; write an extension for Slack — why\nnot, go have some fun! Usually, we spend two working days getting off work. That’s very refreshing.\nI also know that once in a while there are hackathons for the whole Allegro — but I didn’t have a chance to participate.\nDobrze tu być?\nAt Allegro we all understand the great importance of ensuring the code we are working on is of the highest quality. We care about our services to the point\nwhere we sometimes spend hours discussing if it is better to throw an exception or just 404?\nHowever, Allegro is not only about programming, it is a place with a lot of experts in many other disciplines — and every one of us has a straightforward\ngoal to make Allegro the best place/platform not only for shoppers and sellers, but also for each other.\nHope you enjoyed this article. I know it sounds like a chorus of praise, but in my situation it arises from my work experiences — for me, it is really a\n“Good to be here!” place. Or as we say #dobrzetubyć.","guid":"https://blog.allegro.tech/2021/02/first-days-at-allegro.html","categories":["tech"],"isoDate":"2021-02-14T23:00:00.000Z","thumbnail":"images/post-headers/default.jpg"},{"title":"Implement stateless authentication like a pro using OAuth: A 100% correct approach","link":"https://blog.allegro.tech/2021/02/oauth-stateless-login.html","pubDate":"Mon, 01 Feb 2021 00:00:00 +0100","authors":{"author":[{"name":["Karol Kuc"],"photo":["https://blog.allegro.tech/img/authors/karol.kuc.jpg"],"url":["https://blog.allegro.tech/authors/karol.kuc"]}]},"content":"<p>Many of us spend most of their software development careers improving and extending applications protected by pre-existing security mechanisms. That’s why\nwe rarely address problems related directly to authentication and authorization unless we build apps from scratch.\nRegardless of your experience I still hope you will find this article interesting.\nIt’s not meant to be a tutorial. I would like to focus on clarifying basic concepts and highlighting common misconceptions.</p>\n\n<h4 id=\"alpha-disclaimer\">Alpha disclaimer:</h4>\n<p>Sorry to disappoint you, but the title of this article is just a cheap click-bait.\nIt doesn’t matter whether you started reading because you disagree with\nthe title or because you hoped to find the holy grail\nof securing web applications. There is a chance you will find something for yourself anyway.</p>\n\n<h4 id=\"beta-disclaimer\">Beta disclaimer:</h4>\n<p>This post is not meant to answer the “how does the login with Facebook work“ question.\nWe will spend some time on it, but just to provoke a discussion, not to go through a tutorial.</p>\n\n<h4 id=\"gamma-disclaimer\">Gamma disclaimer:</h4>\n<p>I have never built a full-fledged auth process from scratch myself in a commercial web app.\nI’m no security expert, so you read at your own risk.</p>\n\n<h2 id=\"authentication-vs-authorization\">Authentication vs Authorization</h2>\n<p>Wow! If you are still reading, cool!\nLet’s start with a short recap on these two basic concepts. Authentication is about verifying <strong>who you are</strong>.\nYou need to prove your identity, that’s all. You may authenticate, for example, by using a password or your fingerprint.\nYou may need to use a token (hard or soft), an authentication-app-generated code or a text message sent to your phone number.\nI hope you tend to use at least two of these. Anyway, once you’ve done it, you are in.</p>\n\n<p><img src=\"/img/articles/2021-02-01-oauth-stateless-login/password.png\" alt=\"Say the password and enter\" /></p>\n\n<p>Authorization is about verifying <em>what you are allowed to do</em>. I’m not going to copy-paste bookish definitions here. For example, when entering a university library, you authenticate by presenting your ID.\nThe librarian checks the authenticity of the document and analyzes whether you are the one on the photograph or not.\nThe authorization process starts right away.\nTo keep things simple, the librarian checks, based on your confirmed identity, whether you are a professor or a student. This implies which books you can borrow and how many you can take home.\nThis is where the privileges or permissions come into play.</p>\n\n<p><img src=\"/img/articles/2021-02-01-oauth-stateless-login/authorization.jpeg\" alt=\"But only if you are a friend\" /></p>\n\n<p>If you are already bored with the most obvious meme I could use, let me give you another obvious example of authorization.\nWhen entering a military zone, you also present your ID for authentication. You can’t get in unless you ARE, e.g. a military officer. It’s not the authentication that failed then.\nThe guardian knows who you are now.\nYou are forbidden to enter as you have not been granted such an authority.</p>\n\n<p>One last meme, I promise.</p>\n\n<p><img src=\"/img/articles/2021-02-01-oauth-stateless-login/authority_not_granted.png\" alt=\"Authority is not granted to you\" /></p>\n\n<h2 id=\"oauth-20-why-is-it-not-about-authenticating-the-user\">OAuth 2.0: why is it not about authenticating the user?</h2>\n\n<p>I started with the above trivia because the concepts of authorization and authentication are mingled so often that, even when you know them, you may still get a headache\nwhen reading about OAuth (from now on, I will omit the 2.0 specification version).</p>\n\n<p>What is OAuth? I hate copy-pasting definitions from official docs and other blogs. What I hate even more is copy-pasting them, rephrasing and selling as my own.\nSo, unless you already did, please stop reading and check out the <a href=\"https://tools.ietf.org/html/rfc6749\">docs</a>, <a href=\"https://oauth.net/2/\">this website</a> and then <a href=\"https://auth0.com/docs/protocols/protocol-oauth2\">the auth0 website</a>. If you liked the latter, you may also check out <a href=\"https://auth0.com/blog/\">their blog</a> in general.\nI assume you’ve read the suggested docs so I will move on to using the traditional OAuth lingo (grants, flows, resources, third-party servers etc).</p>\n\n<h3 id=\"oauth-flows-revisited\">OAuth flows revisited</h3>\n<p>The OAuth specification describes this framework as available in <a href=\"https://tools.ietf.org/html/rfc6749#section-1.3\">four flavours</a>:</p>\n<ul>\n  <li>Authorization Code</li>\n  <li>Implicit</li>\n  <li>Resource Owner Credentials</li>\n  <li>Client Credentials</li>\n</ul>\n\n<p>So 2012.</p>\n\n<p><img src=\"/img/articles/2021-02-01-oauth-stateless-login/i-built-a-time-machine-to-travel-through-time.jpg\" alt=\"Ten years later\" /></p>\n\n<p>The <a href=\"https://auth0.com/docs/flows/resource-owner-password-flow\">Resource Owner Credentials</a> (User Credentials) flow is already legacy and <a href=\"https://tools.ietf.org/html/draft-ietf-oauth-security-topics-13#section-3.4\">officially banned by IETF</a>.\n    The latest OAuth 2.0 Security Best Current Practice disallows the password grant entirely.</p>\n\n<p><a href=\"https://oauth.net/2/grant-types/password/\">Source</a></p>\n\n<p>The same applies to the <a href=\"https://oauth.net/2/grant-types/implicit/\">Implicit</a> flow, also discouraged in its <a href=\"https://tools.ietf.org/html/rfc6749#section-1.3.2\">original form</a>.\nSome experts analyze whether it is already <a href=\"https://developer.okta.com/blog/2019/05/01/is-the-oauth-implicit-flow-dead\">dead</a> or not.\nIn the following two <em>“login with..“</em> examples you should understand OAuth as limited to the <a href=\"https://auth0.com/docs/flows/authorization-code-flow\">Authorization Code</a> grant. The <a href=\"https://auth0.com/docs/flows/client-credentials-flow\">Client Credentials</a> grant is a completely different story, handled\nin a further part of the article.</p>\n\n<h3 id=\"its-hello-world-time\">It’s <strong>Hello World</strong> Time</h3>\n<p>I prefer to think of OAuth as a set of guidelines and best practices that help you solve a common problem related to authorization, as opposed to a strict framework or protocol.\nThis common problem may be for instance the following. Damn, how the hell did these guys implement the cool <em>log in with Facebook</em> feature in their app (not so cool anymore, let’s be honest it’s 2021 not 2015).\nLet’s call this app fb64.com .\nTo be precise, how did those guys use the Facebook API to delegate the process of asking the user whether or not fb64.com can access some of their Facebook account data?\nYou may find an analogous scenario described in virtually EVERY SINGLE blog post referring to OAuth, but there is a good reason why I do it here too. I use it because it sucks and introduces misunderstandigs regarding real-life\n<strong>delegated authorization</strong> and <strong>log in with a third party identity provider</strong> scenarios.\nLet’s consider the dumbest example ever.</p>\n\n<ul>\n  <li>The <em>resource</em> is your Facebook wall and everything you’ve posted on it.</li>\n  <li>The <em>resource owner</em> is, well, just you.</li>\n  <li>The <em>resource server</em> is Facebook.</li>\n  <li>The <em>client</em> is the fb64.com app. Its cool feature is presenting your 10 recent posts\nas Base64 encoded strings.</li>\n  <li>The <em>authorization server</em> is also Facebook itself.</li>\n</ul>\n\n<p><img src=\"/img/articles/2021-02-01-oauth-stateless-login/legit.jpg\" alt=\"Phishing level master\" /></p>\n\n<p>So, in the above scenario, you get the cool feature without giving these guys your Facebook login and password. When you are redirected to the Facebook login page\nfirst you give your consent for fb64.com to access a specific subset of your profile data. Next you type in your credentials (you send them to facebook.com, not to fb64.com) and\nwhen you get redirected back to fb64.com you are already logged-in and see an awesome black-and-white website with your Base64 encoded posts.</p>\n\n<p>What have you done? You’ve <strong>granted</strong> some <strong>client</strong> app access to your <strong>protected resource</strong> data without sharing your <strong>credentials</strong> with it. Full stop.</p>\n\n<p>But is that what we meant? Is that the kind of <strong>resource sharing</strong> and <strong>delegated authorization</strong> you expect when you want to, for example, log in to Booking, via Facebook, to book a hotel room but don’t want to set up an account?</p>\n\n<p><img src=\"/img/articles/2021-02-01-oauth-stateless-login/no-no-no-thank-you.jpg\" alt=\"Thanks but thanks\" /></p>\n\n<h3 id=\"its-real-life-scenario-time\">It’s <strong>Real Life Scenario</strong> Time</h3>\n\n<p>Let’s come up with a more <em>real-life-ish</em> scenario.\nWhen you choose a <strong>third party identity provider login</strong> you probably expect to get an account without wasting time to sign-up and come up with Yet Another Password.\nBefore you click <em>continue as John Smith</em> you are assured that:</p>\n<ul>\n  <li>what the website will have access to are your name, surname, email and profile picture,</li>\n  <li>it will not be allowed to post on your wall.</li>\n</ul>\n\n<p>So what is the resource you are granting some app access to? Your identity! You use Facebook to let Booking know your name, surname, email and picture.\nObviously, in the meantime, Facebook needs to check and confirm that <em>you are who you say you are</em>, e.g. john.smith@somemail.com. So sharing your identity,\nwhich is the actual resource you are authorizing booking.com to access, implies being authenticated behind the scenes. <strong>Proving</strong> your identity is <strong>authentication</strong> while <strong>sharing</strong> it is <strong>authorization</strong>.</p>\n<h4 id=\"digression\">Digression:</h4>\n<p>Seriously, how often do people use and authorize post, tweet or photo importing apps? Apps that post to your wall? I cannot name even one. Importing contacts from Gmail or one social network while logging in to another\nsocial network is the only everyday use case I can think of, apart from identity resource sharing.</p>\n\n<h3 id=\"oauth-based-authentication\">OAuth based <strong>“authentication“</strong></h3>\n\n<p>OAuth does not say a word about the way the authorization server should authenticate the user before letting them authorize the client app.\nHow does OAuth help in authenticating the user then? It doesn’t! The authentication happens transparently to the client app, sort of in-between the ongoing authorization process.\nIt depends on the authorization server’s internal authentication implementation. When you get redirected to Facebook and you type\nin your email and password and hit enter, it could even trigger basic authentication underneath and send your Base64 encoded plain-text credentials in the <em>Authorization</em> (sic!) HTTP header.\nOr it could trigger a three factor authentication process including a hard token and a magic link sent to your email.\nNever forget that OAuth is an authorization framework when you go to job interviews.</p>\n\n<h4 id=\"first-get-in\">First, get in</h4>\n\n<p>Why do we say that we <em>log in to Booking with Facebook using OAuth</em>? I guess the reason of one of the common misunderstandings regarding the purpose of the framework is just this unfortunate mental shortcut. And the fact that most of us think <em>log in</em> === <em>authenticate</em> (which is not wrong, I refuse to elaborate on the semantics in this case).\nWe should rather say that we <em>authorize Booking to delegate authentication to Facebook and use the confirmed identity</em>, in the background of logging in to Booking. We do all that instead of logging in to Booking directly.\nI guess it’s just too long and convoluted ;).</p>\n\n<h4 id=\"then-stay-inside\">Then, stay inside</h4>\n<p>I can’t stress this enough: OAuth based authorization implies authentication performed by the authorization server. But to keep you logged in, the client app needs\nto use some other mechanism underneath, e.g. HTTP session based on cookies. That’s another blank spot on the OAuth map. Which is not wrong, it’s just not the concern of this framework.\nJust, please, don’t use the access token as a session ID.</p>\n\n<h4 id=\"authorizer-and-authorizee-\">Authorizer and authorizee ;)</h4>\n<p>You build the sign-in layer of your app using OAuth as the authorization framework. The user gets authenticated by the third party and allows your app to access their identity data. Let’s clarify another common misconception\nrelated to the part OAuth plays here. Who is the one being authorized? The user? No, it’s your app. The client app is being authorized - by the user - to access a resource, via the authorization server.\nDoes that imply authorizing the user, too? Yes, it may when you think of it the other way round. You cannot authorize the client app to do something that you are not yourself authorized to do in the resource server.\nThis is where <a href=\"https://tools.ietf.org/html/rfc6749#page-23\">access token scopes</a> grow out of resource owner privileges, permissions, roles etc.\nCan the shared identity resource also contain some permissions and privileges granted to the user within the authorization server? Yes, they can and they often do, e.g. as a <em>permissions</em> or <em>roles</em> field in the access token returned by the authorization server.\nI specifically think of <a href=\"https://auth0.com/docs/tokens/access-tokens\">Json Web Tokens</a> and their custom claims.\nForgive me for not pasting in an example of a JWT, but I’m allergic to pasting things you have probably seen a hundred times elsewhere.</p>\n\n<h2 id=\"oauth-based-authentication-no-quotes\">OAuth based <strong>authentication</strong> (no quotes)</h2>\n\n<p>Yep, here <strong>100% correct approach</strong> is not just a clickbait title.</p>\n\n<p>A corresponding authentication framework which you can use to implement the identity layer of your\napplication is <a href=\"https://auth0.com/docs/protocols/openid-connect-protocol\">Open ID Connect</a>.</p>\n\n<p>While the purpose of OAuth is Delegated Authorization, what describes OpenID Connect best is Federated Identity Management.\nWhat does it mean?</p>\n\n<p>OK, I lied. Brace yourselves, for a citation is coming:</p>\n\n<blockquote>\n  <p>While OAuth 2.0 is about resource access and sharing, OIDC is all about user authentication.\nIts purpose is to give you one login for multiple sites.\nEach time you need to log in to a website using OIDC, you are redirected to your OpenID site where you login,\nand then taken back to the website.</p>\n</blockquote>\n\n<p>I guess you’ve just said: “Wait, wait, whaaat???“</p>\n\n<blockquote>\n  <p>… you are redirected to your OpenID site where you login, and then taken back to the website</p>\n</blockquote>\n\n<p>Now you may think: “We’ve just gone through that, haven’t we? You’ve just swapped one framework for another to make the post longer.“</p>\n\n<p>No, it’s just that OIDC is not loosely based on OAuth, it’s actually plugged into it, filling all the authentication\ngaps we’ve mentioned before. It’s best suited to develop your own Identity Provider or more likely an internal security component\nof a system you are building. It may also be a universal login and authorization service for a broader environment of applications\nand systems in your or your client’s company. We will not dig deeper into OIDC in this post, as it deserves\na post of its own. The fact that OIDC perfectly fits into OAuth is best illustrated by the following. While OAuth lets you control access to given resources (like user identity)\nby issuing either JWT access tokens or UUID access tokens, OIDC handles user identity with ID tokens. Access tokens are opaque from the client’s perspective.\nID tokens MUST be a JWT user identity state representation because the ID token, unlike the access token, is readable for the client.\nOIDC does all what was <em>behind the scenes</em> and <em>depending on the identity provider</em> implementation.</p>\n\n<h2 id=\"client-authentication\">Client authentication</h2>\n\n<p>The biggest lie in OAuth is that it has nothing to do with authentication. It’s true only for the resource owner.\nThe client needs to authenticate itself every time it asks for an access token. Usually it implies sending the client ID and secret as Basic Authentication plain-text string.\nSPA applications have no way of “storing“ the secret securely, as it would have to be included in the source code.\nThat’s why the simplified Implicit flow, devised for JavaScript applications as they were understood almost a decade ago, is now officially\nbanned. It omits the client secret, uses only the client ID, and imposes several other limitations. As per storing the client secret on the client side, when using the Authorization Code grant, mobile and native apps have been believed to be\nsecure enough to do it. <a href=\"https://www.youtube.com/watch?v=H6MxsFMAoP8\">They are not</a>.\nThe only way traditional Authorization Code grant can be used securely is by rendering your web application server-side.\nA huge step forward for OAuth for SPA and mobile apps is enriching the Authorization Code with <a href=\"https://developer.okta.com/blog/2018/12/13/oauth-2-for-native-and-mobile-apps\">PKCE</a>, which I only link here,\nas it deserves an article of its own.</p>\n\n<h2 id=\"stateless-is-a-techie-euphemism-for-useless\">Stateless is a techie euphemism for useless</h2>\n\n<p>Another matter I find very often misunderstood is how OAuth as an authorization framework and OIDC as an authentication framework\ncan be used to secure an app using a completely stateless implementation.</p>\n\n<p>Implementing (<strong>fully</strong>) stateless authorization and authentication mechanisms requires\nabandoning the traditional battle-tested server-side sessions with identifiers stored client-side as cookies\nfor (stateless) Json Web Tokens (or some other isomorphic solution).</p>\n\n<p>By stateless JWT I mean an approach where the whole identity and access control context is fetched <strong>once</strong> when the user logs in (or upon token refresh). Then it’s sent back and forth in form of JWT tokens as cookies (sic!), HTTP request headers or body.\nI will not dig into the pros and cons of where to store and how to transfer JWTs as this would mean copying and pasting half\nof the software-literate Internet. You will find the details in the expert articles I link in further sections.</p>\n\n<p>I treat this section as a challenge to prove that an article can be built virtually out of citations only.\nWith minimal intellectual effort from the author. Let’s get started:\n    Unlike sessions - which can be invalidated by the server\n    whenever it feels like it - individual stateless JWT tokens\n    cannot be invalidated. By design, they will be valid until\n    they expire, no matter what happens. This means that you cannot,\n    for example, invalidate the session of an attacker after detecting\n    a compromise. You also cannot invalidate old sessions when a user\n    changes their password.</p>\n\n<blockquote>\n  <p>You are essentially powerless, and cannot ‘kill’ a session\nwithout building complex (and stateful!) infrastructure to\nexplicitly detect and reject them, defeating the entire\npoint of using stateless JWT tokens to begin with.</p>\n</blockquote>\n\n<p>The above and below citations by Sven Slootweg come from <a href=\"http://cryto.net/%7Ejoepie91/blog/2016/06/13/stop-using-jwt-for-sessions/\">this</a>\nand <a href=\"http://cryto.net/~joepie91/blog/2016/06/19/stop-using-jwt-for-sessions-part-2-why-your-solution-doesnt-work/\">this</a> article.</p>\n\n<p>Regarding the highlighted session invalidation concerns, the same applies to challenges related to revoking only some of the user privileges without logging them out, so I will not\ndifferentiate between these two cases.\nBy stateful JWT you should understand any hybrid that tries to balance what is necessary to store (and potentially invalidate)\nserver-side and the data that can be stored in the browser and transferred in an encoded, encrypted and signed form with every request\nuntil it automatically expires. The question is: if something does not require a real-time invalidation - be it a user session or a user privilege -\nare we still talking about stateless authentication and authorization? Or just about some user data that may be stale so there is no need to fetch them\nwith every request? That’s an entirely different story.</p>\n\n<p>I have an auto-reloading citation clip in my gun:</p>\n\n<blockquote>\n  <p>This can mean that a token contains some outdated information like an old website URL that somebody changed\nin their profile - but more seriously, it can also mean somebody has a token with a role of admin,\neven though you’ve just revoked their admin role. Because you can’t invalidate tokens either,\nthere’s no way for you to remove their administrator access,\nshort of shutting down the entire system.</p>\n</blockquote>\n\n<p>I would appreciate if you suggested an example of an application\nthat needs some security mechanisms but it is not critical to be able to\nrevoke or invalidate them in real time.</p>\n\n<p>I have once read that losing a JWT token is like losing your house keys. Be it true or not, you can always say that\nyou can leave your apartment door open for several minutes if you just go down to the groceries and will be right back. Of course you can, you could also\nleave your bank account without logging out on a university library computer, there is a good chance the session will expire before someone\nsteals your money. The same applies to the expiry of a JWT session or access token which is not either whitelisted\nor blacklisted server-side.</p>\n\n<p>When I imagine myself discovering that once I’ve changed my Gmail account password using the desktop app\nI’m still logged in (even if only for the remaining four minutes) on my mobile app then… Well it’s embarrassing and frightening at the same time, when you think of all the accounts\nyou could potentially reset passwords for by taking over someone’s email account. But yeah, go ahead and just remove the user’s token from local storage.</p>\n\n<p>OK, sit down, it’s citation break:</p>\n\n<blockquote>\n  <p>If you are concerned about somebody intercepting your session cookie,\nyou should just be using TLS instead - any kind of session implementation\nwill be interceptable if you don’t use TLS, including JWT.</p>\n</blockquote>\n\n<p>Me discovering someone took over the evergreen refresh token to my mail account:</p>\n\n<p><img src=\"/img/articles/2021-02-01-oauth-stateless-login/refresh_token.jpg\" alt=\"Me discovering someone took over the evergreen refresh token linked to my mail account\" /></p>\n\n<p>There is no you-meme-it-wrong record I couldn’t break.</p>\n\n<p>Another citation (same source):</p>\n<blockquote>\n  <p>Simply put, using cookies is not optional, regardless of whether you use JWT or not.</p>\n</blockquote>\n\n<p>Yet Another Citation</p>\n\n<blockquote>\n  <p>True statelessness and revocation are mutually exclusive.</p>\n</blockquote>\n\n<p>Please do look it up in <a href=\"https://lmgtfy.app/?q=%22statelessness+and+revocation+are+mutually+exclusive%22\">these articles</a>\nor at least notice how many of them come up.</p>\n\n<p>Now that I’m done with throwing angry links at you, let’s focus on the other side of the problem.</p>\n\n<h2 id=\"statelessness-is-a-key-to-easier-scalability\">Statelessness is a key to easier scalability</h2>\n\n<p>It’s not true that introducing stateless elements to your authentication and authorization is something wrong.\nMy intention was to play bad cop to emphasize things you have to be careful about.\nHDD (Hype Driven Development) is a bad practice in general, but as far as security is concerned it’s the shortest path to\ngetting hacked.\nEvery reasonable JWT-based security implementation is a hybrid of stateless, token-based solutions and stateful,\nserver-side-stored solutions. If moving away from traditional, fully stateful implementations is a challenge for\nyour team, you may start <a href=\"https://auth0.com/blog/stateless-auth-for-stateful-minds/\">with this auth0 article</a>\nwhich may make the mind shift easier.</p>\n\n<p>One of the key points from my point of view:</p>\n\n<blockquote>\n  <p>One of the cool things about session IDs is that they are opaque.\n“Opaque” means no data can be extracted from it by third parties (other than the issuer).\nThe association between session ID and data is entirely done server-side.\nAre there any other ways of achieving something of the sort without relying on state?\nEnter cryptography.</p>\n</blockquote>\n\n<p>The general idea, as mentioned in the OIDC section, is that access tokens should be\nseen by the client as UUIDs or other meaningless text. It’s the server that should be able to interpret them.\nThat obviously works for traditional session IDs and can be achieved in the JWT-based\napproach using encryption. Obviously, to achieve revocation you need to store those tokens\nanyway.</p>\n\n<p>This is where the critics of introducing JWTs in the main authorization flows, as opposed to one-time operations or server-side machine-to-machine communication, hit you hard:\n<img src=\"/img/articles/2021-02-01-oauth-stateless-login/jwt-flowchart.png\" alt=\"Authority is not granted to you\" />\n<a href=\"http://cryto.net/~joepie91/blog/2016/06/19/stop-using-jwt-for-sessions-part-2-why-your-solution-doesnt-work/\">Source: again the Slootweg article</a></p>\n\n<p>This positions us somewhere between the second and the third “swimlane“ on the above chart, which means\nkeeping a blacklist of revocations to invalidate or storing ID in the token and rest of the data server-side.</p>\n\n<p>It brings you either to:</p>\n\n<blockquote>\n  <p>Your blacklisting/authenticaiton server goes down. What now?\nOnce the attacker takes down the server he has free roam and there is nothing\nyou can do to stop him.</p>\n</blockquote>\n\n<p>or to:</p>\n\n<blockquote>\n  <p>Congratulations! You’ve reinvented sessions, with all their problems (notably,\ntheir need for central state) and gained nothing in the process. But the implementation\nyou are using is less battle tested and you run a higher risk of vulnerabilities.</p>\n</blockquote>\n\n<p>Yay, time for pasting a JWT body example like in all these other articles:</p>\n\n<div class=\"language-json highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"w\">    </span><span class=\"p\">{</span><span class=\"w\">\n        </span><span class=\"nl\">\"sub\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"some_user_id\"</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"err\">//no</span><span class=\"w\"> </span><span class=\"err\">reasonable</span><span class=\"w\"> </span><span class=\"err\">case</span><span class=\"w\"> </span><span class=\"err\">for</span><span class=\"w\"> </span><span class=\"err\">it</span><span class=\"w\"> </span><span class=\"err\">to</span><span class=\"w\"> </span><span class=\"err\">go</span><span class=\"w\"> </span><span class=\"err\">stale</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"err\">but</span><span class=\"w\"> </span><span class=\"err\">dangerous</span><span class=\"w\"> </span><span class=\"err\">if</span><span class=\"w\"> </span><span class=\"err\">compromised</span><span class=\"w\">\n        </span><span class=\"nl\">\"name\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"Jason William Toak\"</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"err\">//highly</span><span class=\"w\"> </span><span class=\"err\">unlikely</span><span class=\"w\"> </span><span class=\"err\">to</span><span class=\"w\"> </span><span class=\"err\">change</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"err\">but</span><span class=\"w\"> </span><span class=\"err\">it's</span><span class=\"w\"> </span><span class=\"err\">sensitive</span><span class=\"w\"> </span><span class=\"err\">data</span><span class=\"w\">\n        </span><span class=\"nl\">\"email\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"j.w.toak@somemail.com\"</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"err\">//stale</span><span class=\"w\"> </span><span class=\"err\">email</span><span class=\"w\"> </span><span class=\"err\">may</span><span class=\"w\"> </span><span class=\"err\">obviously</span><span class=\"w\"> </span><span class=\"err\">be</span><span class=\"w\"> </span><span class=\"err\">a</span><span class=\"w\"> </span><span class=\"err\">security</span><span class=\"w\"> </span><span class=\"err\">issue</span><span class=\"w\">\n        </span><span class=\"nl\">\"scope\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"p\">[</span><span class=\"w\">\n            </span><span class=\"s2\">\"admin\"</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"err\">//this</span><span class=\"w\"> </span><span class=\"err\">really</span><span class=\"w\"> </span><span class=\"err\">sucks</span><span class=\"w\"> </span><span class=\"err\">if</span><span class=\"w\"> </span><span class=\"err\">the</span><span class=\"w\"> </span><span class=\"err\">token</span><span class=\"w\"> </span><span class=\"err\">is</span><span class=\"w\"> </span><span class=\"err\">not</span><span class=\"w\"> </span><span class=\"err\">revocable</span><span class=\"w\">\n            </span><span class=\"s2\">\"user\"</span><span class=\"w\">\n        </span><span class=\"p\">]</span><span class=\"w\">\n    </span><span class=\"p\">}</span><span class=\"w\">\n</span></code></pre></div></div>\n\n<p>Obviously, the above token needs to be <a href=\"https://tools.ietf.org/html/rfc7515\">signed</a> so that you are sure that no one changed\nits content and it needs to be <a href=\"https://tools.ietf.org/html/rfc7516\">encrypted</a> as it contains sensitive data. There is a <a href=\"https://auth0.com/blog/json-web-token-signing-algorithms-overview/\">choice of algorithms</a> available.\nAn interesting fact <a href=\"https://auth0.com/blog/stateless-auth-for-stateful-minds/#The-Technical-Magic--JWTs--Digital-Signatures-and-Encryption\">mentioned</a> by Sebastian Peyrott from the auth0 team:</p>\n\n<blockquote>\n  <p>A typical encryption scheme uses an already signed JWT as the payload for encryption. This is known as a nested JWT. It is acceptable to use the same key for encryption and validation.</p>\n</blockquote>\n\n<p>I guess there is a bottom line that somehow finds a common denominator for both Slootweg and Peyrott manifestos. I do not put these articles in opposition to each other, just as points of view on the same problems from different angles and with different bias.\nThis bottom line, as I see it, is as follows:</p>\n\n<ul>\n  <li>Although JWTs are validated cryptographically, most systems need to use some storage for tokens anyway. By most systems I mean all using refresh tokens, which on its own should, in my opinion, mean virtually each end every token-based application.</li>\n  <li>The storage, depending on the application’s requirements, would be either a blacklist or a whitelist of tokens. These\ncan include short-lived access tokens, long-lived refresh tokens, ID tokens or a cartesian product of all the mentioned options.</li>\n  <li>The only way for the token storage to prevent leakage is checking every applicable token\nagainst the white/black list.</li>\n  <li>You can either call it “reinventing sessions“ or “negating some benefits of the token-based approach“, but revocation and statelessness\nare mutually exclusive and nothing can change it.</li>\n  <li>The more you reduce the tokens’ TTL, the less <em>benefit of statelessness</em> you get. I mean that reducing access tokens’ TTL to a reasonable minimum is probably a must-have in\nmost real-life scenarios. Refreshing them is based on refresh tokens stored server-side anyway and a short access tokens’ TTL requires frequent calls to a single point of failure.\nOne still benefits on using a semi-stateless solution, yet the fully stateless one is usually fiction. “The Benefits of Going Stateless“, a slogan which starts Peyrott’s article, is actually just a figure of speech.</li>\n  <li>For some systems even a five seconds long access token validity may be a security breach.</li>\n  <li>Rolling the signing key is always an option, but it’s like a nuclear suicide attack on your users’ sessions.</li>\n  <li>Even if you check every token against some kind of storage, this does not mean you are back\nto centralized sessions, you still greatly reduce the number of HTTP calls regarding user identity, privilege and other\ndata you choose to fit into a token.</li>\n  <li>Let’s emphasize that again: there is a huge difference between accessing a VALID/INVALID key-value storage with every request versus querying\na set of storages for distributed user-related data, and aggregating them for every request.</li>\n  <li>In a microservice architecture it’s a COLOSSAL, crucial change, which means a difference\nbetween a system that barely crawls and one that performs very well.</li>\n  <li>In an event-driven microservice architecture, you can handle both user log-outs and privilege revocation or stale data\nwith events invalidating corresponding tokens. Generating a new token does not necessarily mean fetching all data again, it depends on the granularity\nof information included in the event.</li>\n  <li>Does it mean latency, possible race conditions, eventual consistency? Well, yeah, just as pretty much everything does nowadays.</li>\n</ul>\n\n<h2 id=\"summary\">Summary</h2>\n\n<p><em>Authorization, authentication, statelessness, revocation, tokens, sessions.</em></p>\n\n<p>Lots of stuff easy to misunderstand, implement the wrong way, oversimplify and wrongly criticise.</p>\n\n<p><em>OAuth, JWT, OpenID Connect, Authorization Code Grant, Implicit Flow, User and Client Credentials, Third-Party Identity Providers,\nDelegated Authorization, Federated Identity Management.</em></p>\n\n<p>That’s again a lot of words and it’s not even a sentence, though a nice SEO booster ;)  We’ve gone more or less through\nall of that, but studying all these terms in detail here was impossible. Should there be at least one thing this article made clear to you and if you\nfeel some misconceptions have been clarified, I’m very happy. If there is at least one comment\nbelow this article which will prove me wrong in some or most of views on this broad subject, I will be\nmore than happy to learn something and exchange opinions. As I’ve mentioned in\nthe beginning of the article, security is not something most of us can study, implement and\nwork with every day. Please feel invited to share your knowledge and experience, also including stuff\nthat looks good on paper, but didn’t work for you in production. I will appreciate it a lot.</p>\n","contentSnippet":"Many of us spend most of their software development careers improving and extending applications protected by pre-existing security mechanisms. That’s why\nwe rarely address problems related directly to authentication and authorization unless we build apps from scratch.\nRegardless of your experience I still hope you will find this article interesting.\nIt’s not meant to be a tutorial. I would like to focus on clarifying basic concepts and highlighting common misconceptions.\nAlpha disclaimer:\nSorry to disappoint you, but the title of this article is just a cheap click-bait.\nIt doesn’t matter whether you started reading because you disagree with\nthe title or because you hoped to find the holy grail\nof securing web applications. There is a chance you will find something for yourself anyway.\nBeta disclaimer:\nThis post is not meant to answer the “how does the login with Facebook work“ question.\nWe will spend some time on it, but just to provoke a discussion, not to go through a tutorial.\nGamma disclaimer:\nI have never built a full-fledged auth process from scratch myself in a commercial web app.\nI’m no security expert, so you read at your own risk.\nAuthentication vs Authorization\nWow! If you are still reading, cool!\nLet’s start with a short recap on these two basic concepts. Authentication is about verifying who you are.\nYou need to prove your identity, that’s all. You may authenticate, for example, by using a password or your fingerprint.\nYou may need to use a token (hard or soft), an authentication-app-generated code or a text message sent to your phone number.\nI hope you tend to use at least two of these. Anyway, once you’ve done it, you are in.\n\nAuthorization is about verifying what you are allowed to do. I’m not going to copy-paste bookish definitions here. For example, when entering a university library, you authenticate by presenting your ID.\nThe librarian checks the authenticity of the document and analyzes whether you are the one on the photograph or not.\nThe authorization process starts right away.\nTo keep things simple, the librarian checks, based on your confirmed identity, whether you are a professor or a student. This implies which books you can borrow and how many you can take home.\nThis is where the privileges or permissions come into play.\n\nIf you are already bored with the most obvious meme I could use, let me give you another obvious example of authorization.\nWhen entering a military zone, you also present your ID for authentication. You can’t get in unless you ARE, e.g. a military officer. It’s not the authentication that failed then.\nThe guardian knows who you are now.\nYou are forbidden to enter as you have not been granted such an authority.\nOne last meme, I promise.\n\nOAuth 2.0: why is it not about authenticating the user?\nI started with the above trivia because the concepts of authorization and authentication are mingled so often that, even when you know them, you may still get a headache\nwhen reading about OAuth (from now on, I will omit the 2.0 specification version).\nWhat is OAuth? I hate copy-pasting definitions from official docs and other blogs. What I hate even more is copy-pasting them, rephrasing and selling as my own.\nSo, unless you already did, please stop reading and check out the docs, this website and then the auth0 website. If you liked the latter, you may also check out their blog in general.\nI assume you’ve read the suggested docs so I will move on to using the traditional OAuth lingo (grants, flows, resources, third-party servers etc).\nOAuth flows revisited\nThe OAuth specification describes this framework as available in four flavours:\nAuthorization Code\nImplicit\nResource Owner Credentials\nClient Credentials\nSo 2012.\n\nThe Resource Owner Credentials (User Credentials) flow is already legacy and officially banned by IETF.\n    The latest OAuth 2.0 Security Best Current Practice disallows the password grant entirely.\nSource\nThe same applies to the Implicit flow, also discouraged in its original form.\nSome experts analyze whether it is already dead or not.\nIn the following two “login with..“ examples you should understand OAuth as limited to the Authorization Code grant. The Client Credentials grant is a completely different story, handled\nin a further part of the article.\nIt’s Hello World Time\nI prefer to think of OAuth as a set of guidelines and best practices that help you solve a common problem related to authorization, as opposed to a strict framework or protocol.\nThis common problem may be for instance the following. Damn, how the hell did these guys implement the cool log in with Facebook feature in their app (not so cool anymore, let’s be honest it’s 2021 not 2015).\nLet’s call this app fb64.com .\nTo be precise, how did those guys use the Facebook API to delegate the process of asking the user whether or not fb64.com can access some of their Facebook account data?\nYou may find an analogous scenario described in virtually EVERY SINGLE blog post referring to OAuth, but there is a good reason why I do it here too. I use it because it sucks and introduces misunderstandigs regarding real-life\ndelegated authorization and log in with a third party identity provider scenarios.\nLet’s consider the dumbest example ever.\nThe resource is your Facebook wall and everything you’ve posted on it.\nThe resource owner is, well, just you.\nThe resource server is Facebook.\nThe client is the fb64.com app. Its cool feature is presenting your 10 recent posts\nas Base64 encoded strings.\nThe authorization server is also Facebook itself.\n\nSo, in the above scenario, you get the cool feature without giving these guys your Facebook login and password. When you are redirected to the Facebook login page\nfirst you give your consent for fb64.com to access a specific subset of your profile data. Next you type in your credentials (you send them to facebook.com, not to fb64.com) and\nwhen you get redirected back to fb64.com you are already logged-in and see an awesome black-and-white website with your Base64 encoded posts.\nWhat have you done? You’ve granted some client app access to your protected resource data without sharing your credentials with it. Full stop.\nBut is that what we meant? Is that the kind of resource sharing and delegated authorization you expect when you want to, for example, log in to Booking, via Facebook, to book a hotel room but don’t want to set up an account?\n\nIt’s Real Life Scenario Time\nLet’s come up with a more real-life-ish scenario.\nWhen you choose a third party identity provider login you probably expect to get an account without wasting time to sign-up and come up with Yet Another Password.\nBefore you click continue as John Smith you are assured that:\nwhat the website will have access to are your name, surname, email and profile picture,\nit will not be allowed to post on your wall.\nSo what is the resource you are granting some app access to? Your identity! You use Facebook to let Booking know your name, surname, email and picture.\nObviously, in the meantime, Facebook needs to check and confirm that you are who you say you are, e.g. john.smith@somemail.com. So sharing your identity,\nwhich is the actual resource you are authorizing booking.com to access, implies being authenticated behind the scenes. Proving your identity is authentication while sharing it is authorization.\nDigression:\nSeriously, how often do people use and authorize post, tweet or photo importing apps? Apps that post to your wall? I cannot name even one. Importing contacts from Gmail or one social network while logging in to another\nsocial network is the only everyday use case I can think of, apart from identity resource sharing.\nOAuth based “authentication“\nOAuth does not say a word about the way the authorization server should authenticate the user before letting them authorize the client app.\nHow does OAuth help in authenticating the user then? It doesn’t! The authentication happens transparently to the client app, sort of in-between the ongoing authorization process.\nIt depends on the authorization server’s internal authentication implementation. When you get redirected to Facebook and you type\nin your email and password and hit enter, it could even trigger basic authentication underneath and send your Base64 encoded plain-text credentials in the Authorization (sic!) HTTP header.\nOr it could trigger a three factor authentication process including a hard token and a magic link sent to your email.\nNever forget that OAuth is an authorization framework when you go to job interviews.\nFirst, get in\nWhy do we say that we log in to Booking with Facebook using OAuth? I guess the reason of one of the common misunderstandings regarding the purpose of the framework is just this unfortunate mental shortcut. And the fact that most of us think log in === authenticate (which is not wrong, I refuse to elaborate on the semantics in this case).\nWe should rather say that we authorize Booking to delegate authentication to Facebook and use the confirmed identity, in the background of logging in to Booking. We do all that instead of logging in to Booking directly.\nI guess it’s just too long and convoluted ;).\nThen, stay inside\nI can’t stress this enough: OAuth based authorization implies authentication performed by the authorization server. But to keep you logged in, the client app needs\nto use some other mechanism underneath, e.g. HTTP session based on cookies. That’s another blank spot on the OAuth map. Which is not wrong, it’s just not the concern of this framework.\nJust, please, don’t use the access token as a session ID.\nAuthorizer and authorizee ;)\nYou build the sign-in layer of your app using OAuth as the authorization framework. The user gets authenticated by the third party and allows your app to access their identity data. Let’s clarify another common misconception\nrelated to the part OAuth plays here. Who is the one being authorized? The user? No, it’s your app. The client app is being authorized - by the user - to access a resource, via the authorization server.\nDoes that imply authorizing the user, too? Yes, it may when you think of it the other way round. You cannot authorize the client app to do something that you are not yourself authorized to do in the resource server.\nThis is where access token scopes grow out of resource owner privileges, permissions, roles etc.\nCan the shared identity resource also contain some permissions and privileges granted to the user within the authorization server? Yes, they can and they often do, e.g. as a permissions or roles field in the access token returned by the authorization server.\nI specifically think of Json Web Tokens and their custom claims.\nForgive me for not pasting in an example of a JWT, but I’m allergic to pasting things you have probably seen a hundred times elsewhere.\nOAuth based authentication (no quotes)\nYep, here 100% correct approach is not just a clickbait title.\nA corresponding authentication framework which you can use to implement the identity layer of your\napplication is Open ID Connect.\nWhile the purpose of OAuth is Delegated Authorization, what describes OpenID Connect best is Federated Identity Management.\nWhat does it mean?\nOK, I lied. Brace yourselves, for a citation is coming:\nWhile OAuth 2.0 is about resource access and sharing, OIDC is all about user authentication.\nIts purpose is to give you one login for multiple sites.\nEach time you need to log in to a website using OIDC, you are redirected to your OpenID site where you login,\nand then taken back to the website.\nI guess you’ve just said: “Wait, wait, whaaat???“\n… you are redirected to your OpenID site where you login, and then taken back to the website\nNow you may think: “We’ve just gone through that, haven’t we? You’ve just swapped one framework for another to make the post longer.“\nNo, it’s just that OIDC is not loosely based on OAuth, it’s actually plugged into it, filling all the authentication\ngaps we’ve mentioned before. It’s best suited to develop your own Identity Provider or more likely an internal security component\nof a system you are building. It may also be a universal login and authorization service for a broader environment of applications\nand systems in your or your client’s company. We will not dig deeper into OIDC in this post, as it deserves\na post of its own. The fact that OIDC perfectly fits into OAuth is best illustrated by the following. While OAuth lets you control access to given resources (like user identity)\nby issuing either JWT access tokens or UUID access tokens, OIDC handles user identity with ID tokens. Access tokens are opaque from the client’s perspective.\nID tokens MUST be a JWT user identity state representation because the ID token, unlike the access token, is readable for the client.\nOIDC does all what was behind the scenes and depending on the identity provider implementation.\nClient authentication\nThe biggest lie in OAuth is that it has nothing to do with authentication. It’s true only for the resource owner.\nThe client needs to authenticate itself every time it asks for an access token. Usually it implies sending the client ID and secret as Basic Authentication plain-text string.\nSPA applications have no way of “storing“ the secret securely, as it would have to be included in the source code.\nThat’s why the simplified Implicit flow, devised for JavaScript applications as they were understood almost a decade ago, is now officially\nbanned. It omits the client secret, uses only the client ID, and imposes several other limitations. As per storing the client secret on the client side, when using the Authorization Code grant, mobile and native apps have been believed to be\nsecure enough to do it. They are not.\nThe only way traditional Authorization Code grant can be used securely is by rendering your web application server-side.\nA huge step forward for OAuth for SPA and mobile apps is enriching the Authorization Code with PKCE, which I only link here,\nas it deserves an article of its own.\nStateless is a techie euphemism for useless\nAnother matter I find very often misunderstood is how OAuth as an authorization framework and OIDC as an authentication framework\ncan be used to secure an app using a completely stateless implementation.\nImplementing (fully) stateless authorization and authentication mechanisms requires\nabandoning the traditional battle-tested server-side sessions with identifiers stored client-side as cookies\nfor (stateless) Json Web Tokens (or some other isomorphic solution).\nBy stateless JWT I mean an approach where the whole identity and access control context is fetched once when the user logs in (or upon token refresh). Then it’s sent back and forth in form of JWT tokens as cookies (sic!), HTTP request headers or body.\nI will not dig into the pros and cons of where to store and how to transfer JWTs as this would mean copying and pasting half\nof the software-literate Internet. You will find the details in the expert articles I link in further sections.\nI treat this section as a challenge to prove that an article can be built virtually out of citations only.\nWith minimal intellectual effort from the author. Let’s get started:\n    Unlike sessions - which can be invalidated by the server\n    whenever it feels like it - individual stateless JWT tokens\n    cannot be invalidated. By design, they will be valid until\n    they expire, no matter what happens. This means that you cannot,\n    for example, invalidate the session of an attacker after detecting\n    a compromise. You also cannot invalidate old sessions when a user\n    changes their password.\nYou are essentially powerless, and cannot ‘kill’ a session\nwithout building complex (and stateful!) infrastructure to\nexplicitly detect and reject them, defeating the entire\npoint of using stateless JWT tokens to begin with.\nThe above and below citations by Sven Slootweg come from this\nand this article.\nRegarding the highlighted session invalidation concerns, the same applies to challenges related to revoking only some of the user privileges without logging them out, so I will not\ndifferentiate between these two cases.\nBy stateful JWT you should understand any hybrid that tries to balance what is necessary to store (and potentially invalidate)\nserver-side and the data that can be stored in the browser and transferred in an encoded, encrypted and signed form with every request\nuntil it automatically expires. The question is: if something does not require a real-time invalidation - be it a user session or a user privilege -\nare we still talking about stateless authentication and authorization? Or just about some user data that may be stale so there is no need to fetch them\nwith every request? That’s an entirely different story.\nI have an auto-reloading citation clip in my gun:\nThis can mean that a token contains some outdated information like an old website URL that somebody changed\nin their profile - but more seriously, it can also mean somebody has a token with a role of admin,\neven though you’ve just revoked their admin role. Because you can’t invalidate tokens either,\nthere’s no way for you to remove their administrator access,\nshort of shutting down the entire system.\nI would appreciate if you suggested an example of an application\nthat needs some security mechanisms but it is not critical to be able to\nrevoke or invalidate them in real time.\nI have once read that losing a JWT token is like losing your house keys. Be it true or not, you can always say that\nyou can leave your apartment door open for several minutes if you just go down to the groceries and will be right back. Of course you can, you could also\nleave your bank account without logging out on a university library computer, there is a good chance the session will expire before someone\nsteals your money. The same applies to the expiry of a JWT session or access token which is not either whitelisted\nor blacklisted server-side.\nWhen I imagine myself discovering that once I’ve changed my Gmail account password using the desktop app\nI’m still logged in (even if only for the remaining four minutes) on my mobile app then… Well it’s embarrassing and frightening at the same time, when you think of all the accounts\nyou could potentially reset passwords for by taking over someone’s email account. But yeah, go ahead and just remove the user’s token from local storage.\nOK, sit down, it’s citation break:\nIf you are concerned about somebody intercepting your session cookie,\nyou should just be using TLS instead - any kind of session implementation\nwill be interceptable if you don’t use TLS, including JWT.\nMe discovering someone took over the evergreen refresh token to my mail account:\n\nThere is no you-meme-it-wrong record I couldn’t break.\nAnother citation (same source):\nSimply put, using cookies is not optional, regardless of whether you use JWT or not.\nYet Another Citation\nTrue statelessness and revocation are mutually exclusive.\nPlease do look it up in these articles\nor at least notice how many of them come up.\nNow that I’m done with throwing angry links at you, let’s focus on the other side of the problem.\nStatelessness is a key to easier scalability\nIt’s not true that introducing stateless elements to your authentication and authorization is something wrong.\nMy intention was to play bad cop to emphasize things you have to be careful about.\nHDD (Hype Driven Development) is a bad practice in general, but as far as security is concerned it’s the shortest path to\ngetting hacked.\nEvery reasonable JWT-based security implementation is a hybrid of stateless, token-based solutions and stateful,\nserver-side-stored solutions. If moving away from traditional, fully stateful implementations is a challenge for\nyour team, you may start with this auth0 article\nwhich may make the mind shift easier.\nOne of the key points from my point of view:\nOne of the cool things about session IDs is that they are opaque.\n“Opaque” means no data can be extracted from it by third parties (other than the issuer).\nThe association between session ID and data is entirely done server-side.\nAre there any other ways of achieving something of the sort without relying on state?\nEnter cryptography.\nThe general idea, as mentioned in the OIDC section, is that access tokens should be\nseen by the client as UUIDs or other meaningless text. It’s the server that should be able to interpret them.\nThat obviously works for traditional session IDs and can be achieved in the JWT-based\napproach using encryption. Obviously, to achieve revocation you need to store those tokens\nanyway.\nThis is where the critics of introducing JWTs in the main authorization flows, as opposed to one-time operations or server-side machine-to-machine communication, hit you hard:\n\nSource: again the Slootweg article\nThis positions us somewhere between the second and the third “swimlane“ on the above chart, which means\nkeeping a blacklist of revocations to invalidate or storing ID in the token and rest of the data server-side.\nIt brings you either to:\nYour blacklisting/authenticaiton server goes down. What now?\nOnce the attacker takes down the server he has free roam and there is nothing\nyou can do to stop him.\nor to:\nCongratulations! You’ve reinvented sessions, with all their problems (notably,\ntheir need for central state) and gained nothing in the process. But the implementation\nyou are using is less battle tested and you run a higher risk of vulnerabilities.\nYay, time for pasting a JWT body example like in all these other articles:\n\n    {\n        \"sub\": \"some_user_id\", //no reasonable case for it to go stale, but dangerous if compromised\n        \"name\": \"Jason William Toak\", //highly unlikely to change, but it's sensitive data\n        \"email\": \"j.w.toak@somemail.com\", //stale email may obviously be a security issue\n        \"scope\": [\n            \"admin\", //this really sucks if the token is not revocable\n            \"user\"\n        ]\n    }\n\n\nObviously, the above token needs to be signed so that you are sure that no one changed\nits content and it needs to be encrypted as it contains sensitive data. There is a choice of algorithms available.\nAn interesting fact mentioned by Sebastian Peyrott from the auth0 team:\nA typical encryption scheme uses an already signed JWT as the payload for encryption. This is known as a nested JWT. It is acceptable to use the same key for encryption and validation.\nI guess there is a bottom line that somehow finds a common denominator for both Slootweg and Peyrott manifestos. I do not put these articles in opposition to each other, just as points of view on the same problems from different angles and with different bias.\nThis bottom line, as I see it, is as follows:\nAlthough JWTs are validated cryptographically, most systems need to use some storage for tokens anyway. By most systems I mean all using refresh tokens, which on its own should, in my opinion, mean virtually each end every token-based application.\nThe storage, depending on the application’s requirements, would be either a blacklist or a whitelist of tokens. These\ncan include short-lived access tokens, long-lived refresh tokens, ID tokens or a cartesian product of all the mentioned options.\nThe only way for the token storage to prevent leakage is checking every applicable token\nagainst the white/black list.\nYou can either call it “reinventing sessions“ or “negating some benefits of the token-based approach“, but revocation and statelessness\nare mutually exclusive and nothing can change it.\nThe more you reduce the tokens’ TTL, the less benefit of statelessness you get. I mean that reducing access tokens’ TTL to a reasonable minimum is probably a must-have in\nmost real-life scenarios. Refreshing them is based on refresh tokens stored server-side anyway and a short access tokens’ TTL requires frequent calls to a single point of failure.\nOne still benefits on using a semi-stateless solution, yet the fully stateless one is usually fiction. “The Benefits of Going Stateless“, a slogan which starts Peyrott’s article, is actually just a figure of speech.\nFor some systems even a five seconds long access token validity may be a security breach.\nRolling the signing key is always an option, but it’s like a nuclear suicide attack on your users’ sessions.\nEven if you check every token against some kind of storage, this does not mean you are back\nto centralized sessions, you still greatly reduce the number of HTTP calls regarding user identity, privilege and other\ndata you choose to fit into a token.\nLet’s emphasize that again: there is a huge difference between accessing a VALID/INVALID key-value storage with every request versus querying\na set of storages for distributed user-related data, and aggregating them for every request.\nIn a microservice architecture it’s a COLOSSAL, crucial change, which means a difference\nbetween a system that barely crawls and one that performs very well.\nIn an event-driven microservice architecture, you can handle both user log-outs and privilege revocation or stale data\nwith events invalidating corresponding tokens. Generating a new token does not necessarily mean fetching all data again, it depends on the granularity\nof information included in the event.\nDoes it mean latency, possible race conditions, eventual consistency? Well, yeah, just as pretty much everything does nowadays.\nSummary\nAuthorization, authentication, statelessness, revocation, tokens, sessions.\nLots of stuff easy to misunderstand, implement the wrong way, oversimplify and wrongly criticise.\nOAuth, JWT, OpenID Connect, Authorization Code Grant, Implicit Flow, User and Client Credentials, Third-Party Identity Providers,\nDelegated Authorization, Federated Identity Management.\nThat’s again a lot of words and it’s not even a sentence, though a nice SEO booster ;)  We’ve gone more or less through\nall of that, but studying all these terms in detail here was impossible. Should there be at least one thing this article made clear to you and if you\nfeel some misconceptions have been clarified, I’m very happy. If there is at least one comment\nbelow this article which will prove me wrong in some or most of views on this broad subject, I will be\nmore than happy to learn something and exchange opinions. As I’ve mentioned in\nthe beginning of the article, security is not something most of us can study, implement and\nwork with every day. Please feel invited to share your knowledge and experience, also including stuff\nthat looks good on paper, but didn’t work for you in production. I will appreciate it a lot.","guid":"https://blog.allegro.tech/2021/02/oauth-stateless-login.html","categories":["tech","security","oauth","authentication","authorization","jwt","openid-connect"],"isoDate":"2021-01-31T23:00:00.000Z","thumbnail":"images/post-headers/default.jpg"},{"title":"Impact of data model on MongoDB database size","link":"https://blog.allegro.tech/2021/01/impact-of-the-data-model-on-the-MongoDB-database-size.html","pubDate":"Thu, 14 Jan 2021 00:00:00 +0100","authors":{"author":[{"name":["Michał Knasiecki"],"photo":["https://blog.allegro.tech/img/authors/michal.knasiecki.jpg"],"url":["https://blog.allegro.tech/authors/michal.knasiecki"]}]},"content":"<p>So I was tuning one of our services in order to speed up some MongoDB queries. Incidentally, my attention was caught by\nthe size of one of the collections that contains archived objects and therefore is rarely used. Unfortunately I wasn’t\nable to reduce the size of the documents stored there, but I started to wonder: is it possible to store the same data\nin a more compact way? Mongo stores <code class=\"language-plaintext highlighter-rouge\">JSON</code> that allows many different ways of expressing similar data, so there seems\nto be room for improvements.</p>\n\n<p>It is worth asking: why make such an effort in the era of Big Data and unlimited resources? There are several reasons.</p>\n\n<p>First of all, the resources are not unlimited and at the end we have physical drives that cost money to buy, replace,\nmaintain, and be supplied with power.</p>\n\n<p>Secondly, less stored data results in less time to access it. Moreover, less data means that more of it will fit into\ncache, so the next data access will be an order of magnitude faster.</p>\n\n<p>I decided to do some experiments and check how the model design affects the size of database files.</p>\n\n<p>I used a local MongoDB Community Edition 4.4 installation and I initially tested collections containing 1\nmillion and 10 million documents. One of the variants contained up to 100 million, but the results were proportional\n(nearly linear). Therefore, in the end I decided to stop at 1M collections, because loading the data was simply much faster.</p>\n\n<p>Having access to local database files, I could easily check the size of the files storing individual collections.\nHowever, it turned out to be unnecessary, because the same data can be obtained with the command:</p>\n\n<div class=\"language-sql highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"n\">db</span><span class=\"p\">.</span><span class=\"n\">getCollection</span><span class=\"p\">(</span><span class=\"s1\">'COLLECTION_NAME'</span><span class=\"p\">).</span><span class=\"n\">stats</span><span class=\"p\">()</span>\n</code></pre></div></div>\n\n<p><img src=\"/img/articles/2021-01-14-impact-of-the-data-model-on-the-MongoDB-database-size/collection-stats.png\" alt=\"Collection stats\" /></p>\n\n<p>The following fields are expressed in bytes:</p>\n\n<ul>\n  <li><code class=\"language-plaintext highlighter-rouge\">size</code>: size of all collection documents, before compression,</li>\n  <li><code class=\"language-plaintext highlighter-rouge\">avgObjSize</code>: average document size, before compression,</li>\n  <li><code class=\"language-plaintext highlighter-rouge\">storageSize</code>: file size on the disk; this is the value after the data is compressed, the same value is returned by\n<code class=\"language-plaintext highlighter-rouge\">ls</code> command,</li>\n  <li><code class=\"language-plaintext highlighter-rouge\">freeStorageSize</code>: size of unused space allocated for the data; Mongo does not increase the file size byte-by-byte,\nbut allocates a percentage of the current file size and this value indicates how much data will still fit into the file.</li>\n</ul>\n\n<p>To present the results I used (storageSize - freeStorageSize) value that indicates the actual space occupied by the data.</p>\n\n<p>My local MongoDB instance had compression enabled. Not every storage engine has this option enabled by default, so when\nyou start your own analysis it is worth checking how it is in your particular case.</p>\n\n<h3 id=\"id-field-type\">Id field type</h3>\n\n<p>In the beginning I decided to check <code class=\"language-plaintext highlighter-rouge\">ID</code> fields. Not the collection primary key, mind you – it is ‘ObjectId’ type by\ndefault and generally shouldn’t be changed. I decided to focus on user\nand offers identifiers which, although numerical, are often saved as String in Mongo. I believe it comes partly due to\nthe contract of our services - identifiers in <code class=\"language-plaintext highlighter-rouge\">JSON</code> most often come as Strings and in this form they are later stored\nin our databases.</p>\n\n<p>Let’s start with some theory: the number of <code class=\"language-plaintext highlighter-rouge\">int32</code> type in Mongo has a fixed size of 4 bytes. The same number\nwritten as a <code class=\"language-plaintext highlighter-rouge\">String</code> of characters has a size dependent on the number of digits; additionally it contains the length of\nthe text (4 bytes) and a terminator character. For example, the text “0” is 12 bytes long and “1234567890” is 25 bytes\nlong. So in theory we should get interesting results, but what does it look like in reality?</p>\n\n<p>I prepared 2 collections, one million documents each, containing the following documents:</p>\n\n<div class=\"language-json highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"p\">{</span><span class=\"w\"> </span><span class=\"nl\">\"_id\"</span><span class=\"w\"> </span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"mi\">1</span><span class=\"w\"> </span><span class=\"p\">}</span><span class=\"w\">\n</span></code></pre></div></div>\n\n<p>and</p>\n\n<div class=\"language-json highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"p\">{</span><span class=\"w\"> </span><span class=\"nl\">\"_id\"</span><span class=\"w\"> </span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"1\"</span><span class=\"w\"> </span><span class=\"p\">}</span><span class=\"w\">\n</span></code></pre></div></div>\n\n<p>The values of identifiers were consecutive natural numbers. Here is the comparison of results:</p>\n\n<p><img src=\"/img/articles/2021-01-14-impact-of-the-data-model-on-the-MongoDB-database-size/chart-id.png\" alt=\"String vs int32 size\" /></p>\n\n<p>As you can see the difference is significant since the size on disk decreased by half. Additionally, it is worth\nnoting here that my data is synthetic and the identifiers start from 1. The advantage of a numerical identifier over a\n<code class=\"language-plaintext highlighter-rouge\">String</code> increases the more digits a number has, so benefits should be even better for the real life data.</p>\n\n<p>Encouraged by the success I decided to check if field type had any influence on the size of an index created on it. In\nthis case, unfortunately, I got disappointed: the sizes were similar. This is due to the fact that MongoDB\nuses a hash function when creating indexes, so physically both indexes are composed of numerical values.\nHowever, if we are dealing with hashing, maybe at least searching by index in a numerical field is faster?</p>\n\n<p>I made a comparison of searching for a million and 10 million documents by indexed key in a random but the same order\nfor both collections. Again, a missed shot: both tests ended up with a similar result, so the conclusion is this:\nit is worth using numerical identifiers, because they require less disk space, but we will not get additional benefits\nassociated with indexes on these fields.</p>\n\n<p><img src=\"/img/articles/2021-01-14-impact-of-the-data-model-on-the-MongoDB-database-size/chart-search-1M.png\" alt=\"String vs int32 search time 1M\" /></p>\n\n<p><img src=\"/img/articles/2021-01-14-impact-of-the-data-model-on-the-MongoDB-database-size/chart-search-10M.png\" alt=\"String vs int32 search time 10M\" /></p>\n\n<h3 id=\"simple-and-complex-structures\">Simple and complex structures</h3>\n\n<p>Let’s move on to more complex data. Our models are often built from smaller structures grouping data such as user,\norder or offer. I decided to compare the size of documents storing such structures and their flat counterparts:</p>\n\n<div class=\"language-json highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"p\">{</span><span class=\"nl\">\"_id\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"mi\">1</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"nl\">\"user\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"p\">{</span><span class=\"nl\">\"name\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"Jan\"</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"nl\">\"surname\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"Nowak\"</span><span class=\"p\">}}</span><span class=\"w\">\n</span></code></pre></div></div>\n\n<p>and</p>\n\n<div class=\"language-json highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"p\">{</span><span class=\"nl\">\"_id\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"mi\">1</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"nl\">\"userName\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"Jan\"</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"nl\">\"userSurname\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"Nowak\"</span><span class=\"p\">}</span><span class=\"w\">\n</span></code></pre></div></div>\n\n<p>In both cases we store identical data, the documents differ only in the schema. Take a look at the result:</p>\n\n<p><img src=\"/img/articles/2021-01-14-impact-of-the-data-model-on-the-MongoDB-database-size/chart-struct-1.png\" alt=\"complex vs simple size\" /></p>\n\n<p>There is a slight reduction by 0.4MB. It may seem not much compared to the effect we achieved for the field\ncontaining an ID. However, we have to bear in mind that in this case we were dealing with a more complex document. It\ncontained – in addition to the compared fields – a numerical type identifier that, as we remember from previous\nexperiments, takes up about 5MB of disk space.</p>\n\n<p>Taking this into account in the above results we are talking about a decrease from 3.4MB to 3MB. It actually looks\nbetter as percentage - we saved 12% of the space needed to store personal data.</p>\n\n<p>Let’s go back to the discussed documents for a moment:</p>\n\n<div class=\"language-json highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"p\">{</span><span class=\"nl\">\"_id\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"mi\">1</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"nl\">\"user\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"p\">{</span><span class=\"nl\">\"name\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"Jan\"</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"nl\">\"surname\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"Nowak\"</span><span class=\"p\">}}</span><span class=\"w\">\n</span></code></pre></div></div>\n\n<p>and</p>\n\n<div class=\"language-json highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"p\">{</span><span class=\"nl\">\"_id\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"mi\">1</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"nl\">\"userName\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"Jan\"</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"nl\">\"userSurname\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"Nowak\"</span><span class=\"p\">}</span><span class=\"w\">\n</span></code></pre></div></div>\n\n<p>A watchful eye will notice that I used longer field names in the document after flattening. So instead of <code class=\"language-plaintext highlighter-rouge\">user.name</code>\nand <code class=\"language-plaintext highlighter-rouge\">user.surname</code> I made <code class=\"language-plaintext highlighter-rouge\">userName</code> and <code class=\"language-plaintext highlighter-rouge\">userSurname</code>. I did it automatically, a bit unconsciously, to make the\nresulting <code class=\"language-plaintext highlighter-rouge\">JSON</code> more readable.  However, if by changing only the schema of the document from compound to flat we managed\nto reduce the size of the data, maybe it is worth to go a step further and shorten the field names?</p>\n\n<p>I decided to add a third document for comparison, flattened and with shorter field names:</p>\n\n<div class=\"language-json highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"p\">{</span><span class=\"nl\">\"_id\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"mi\">1</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"nl\">\"name\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"Jan\"</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"nl\">\"surname\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"Nowak\"</span><span class=\"p\">}</span><span class=\"w\">\n</span></code></pre></div></div>\n\n<p>The results are shown in the chart below:</p>\n\n<p><img src=\"/img/articles/2021-01-14-impact-of-the-data-model-on-the-MongoDB-database-size/chart-struct-2.png\" alt=\"complex vs simple vs short size\" /></p>\n\n<p>The result is even better than just flattening. Apart from the document’s key size, we achieved a decrease from\n3.4MB to 2MB. Why does this happen even though we store exactly the same data?</p>\n\n<p>The reason for the decrease is the nature of NoSQL databases that, unlike the relational ones, do not have a schema\ndefined at the level of the entire collection. If someone is very stubborn, they can store user data, offers, orders and\npayments in one collection. It would still be possible to read, index and search that data. This is because the\ndatabase, in addition to the data itself, stores its schema with each document. Thus, the total size of a document\nconsists of its data and its schema. And that explains the whole puzzle. By reducing the size of the schema, we also\nreduce the size of each document, i.e. the size of the final file with the collection data. It is worth taking this into\naccount when designing a collection schema in order not to blow it up too much. Of course, you cannot go to extremes, because\nthat would lead us to fields named <code class=\"language-plaintext highlighter-rouge\">a</code>, <code class=\"language-plaintext highlighter-rouge\">b</code> and <code class=\"language-plaintext highlighter-rouge\">c</code>, what would make the collection completely illegible for a human.</p>\n\n<p>For very large collections, however, this approach is used, an example of which is the MongoDB operation log\nthat contains fields called:</p>\n\n<ul>\n  <li>h</li>\n  <li>op</li>\n  <li>ns</li>\n  <li>o</li>\n  <li>o2</li>\n  <li>b</li>\n</ul>\n\n<h3 id=\"empty-fields\">Empty fields</h3>\n\n<p>Since we are at the document’s schema, it is still worth looking at the problem of blank fields. In the case of\n<code class=\"language-plaintext highlighter-rouge\">JSON</code> the lack of value in a certain field can be written in two ways, either directly - by writing null in its value -\nor by not serializing the field at all. I prepared a comparison of documents with the following structure:</p>\n\n<div class=\"language-json highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"p\">{</span><span class=\"w\"> </span><span class=\"nl\">\"id\"</span><span class=\"w\"> </span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"mi\">1</span><span class=\"w\"> </span><span class=\"p\">}</span><span class=\"w\">\n</span></code></pre></div></div>\n\n<p>and</p>\n\n<div class=\"language-json highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"p\">{</span><span class=\"w\"> </span><span class=\"nl\">\"id\"</span><span class=\"w\"> </span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"mi\">1</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"nl\">\"phone\"</span><span class=\"w\"> </span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"kc\">null</span><span class=\"p\">}</span><span class=\"w\">\n</span></code></pre></div></div>\n\n<p>The meaning of the data in both documents is identical - the user has no phone number. However, the schema of the second\ndocument is different from the first one because it contains two fields.</p>\n\n<p>Here are the results:</p>\n\n<p><img src=\"/img/articles/2021-01-14-impact-of-the-data-model-on-the-MongoDB-database-size/chart-null.png\" alt=\"null vs empty size\" /></p>\n\n<p>The results are quite surprising: saving a million null’s is quite expensive as it takes more than 1MB on a disk.</p>\n\n<h3 id=\"enumerated-types\">Enumerated types</h3>\n\n<p>Now, let’s also look at the enumerated types. You can store them in two ways, either by using a label:</p>\n\n<div class=\"language-json highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"p\">{</span><span class=\"nl\">\"_id\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"mi\">1</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"nl\">\"source\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"ALLEGRO\"</span><span class=\"p\">}</span><span class=\"w\">\n</span></code></pre></div></div>\n\n<p>or by ordinal value:</p>\n\n<div class=\"language-json highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"p\">{</span><span class=\"nl\">\"_id\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"mi\">1</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"nl\">\"source\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"mi\">1</span><span class=\"p\">}</span><span class=\"w\">\n</span></code></pre></div></div>\n\n<p>Here an analogy with the first experiment can be found: again we replace a character string with a numerical value.\nSince we got a great result the first time, maybe we could repeat it here?</p>\n\n<p><img src=\"/img/articles/2021-01-14-impact-of-the-data-model-on-the-MongoDB-database-size/chart-enums-1.png\" alt=\"label vs index size\" /></p>\n\n<p>Unfortunately, the result is disappointing and the reduction in size is small. However, if we think more deeply, we will\ncome to the conclusion that it could not have been otherwise. The enumerated types are not unique identifiers. We are\ndealing only with a few possible values, appearing many times in the whole collection, so it’s a great chance for\nMongoDB data compression to prove its worth. Most likely the values from the first collection have been automatically replaced by the\nengine to its corresponding ordinal values. Additionally, we don’t have any profit on the schema here, because they are\nthe same in both cases. The situation might be different for collections that do not have data compression enabled.</p>\n\n<p>This is a good time to take a closer look at how <a href=\"https://www.mongodb.com/blog/post/new-compression-options-mongodb-30\">snappy</a>\ncompression works in MongoDB. I’ve prepared two more collections, with identical data, but with data compression\nturned off. The results are shown in the chart below, compiled together with the collections with data compression turned on.</p>\n\n<p><img src=\"/img/articles/2021-01-14-impact-of-the-data-model-on-the-MongoDB-database-size/chart-enums-2.png\" alt=\"snappy vs plain size\" /></p>\n\n<p>It is clear that the use of an ordinal value instead of a label of the enumerated type brings considerable profit for\ncollections with data compression disabled. In case of lack of compression it is definitely worth considering using numerical\nvalues.</p>\n\n<h3 id=\"useless-_class-field\">Useless _class field</h3>\n\n<p>If we use <code class=\"language-plaintext highlighter-rouge\">spring-data</code>, each of our collections additionally contains a <code class=\"language-plaintext highlighter-rouge\">_class</code> field storing the package and the\nname of the entity class containing the mapping for documents from this collection. This mechanism is used to support\ninheritance of model classes, that is not widely used. In most cases this field stores exactly the same values for each\ndocument what makes it useless. And how much does it cost? Let’s compare the following documents:</p>\n\n<div class=\"language-json highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"p\">{</span><span class=\"nl\">\"_id\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"mi\">1</span><span class=\"p\">}</span><span class=\"w\">\n</span></code></pre></div></div>\n\n<p>and</p>\n\n<div class=\"language-json highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"p\">{</span><span class=\"nl\">\"_id\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"mi\">1</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"nl\">\"_class\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"pl.allegro.some.project.domain.sample\"</span><span class=\"p\">}</span><span class=\"w\">\n</span></code></pre></div></div>\n\n<p><img src=\"/img/articles/2021-01-14-impact-of-the-data-model-on-the-MongoDB-database-size/chart-class.png\" alt=\"_class field size\" /></p>\n\n<p>The difference is considerable, over 50%. Bearing in mind that the compression is enabled, I believe that the impact of\nthe data itself is small and the result is caused by the schema of the collection containing only the key being half as small\nas the one with the key (1 field vs 2 fields). After removal of the <code class=\"language-plaintext highlighter-rouge\">_class</code> field from a document with more fields,\nthe difference expressed in percent will be much smaller of course. However, storing useless data does not make sense.</p>\n\n<h3 id=\"useless-indexes\">Useless indexes</h3>\n\n<p>When investigating the case with identifiers stored as strings, I checked whether manipulating the data type could reduce\nthe index size. Unfortunately I did not succeed, so I decided to focus on the problem of redundant indexes.</p>\n\n<p>It is usually a good practice to cover each query with an index so that the number of <code class=\"language-plaintext highlighter-rouge\">collscan</code> operations is as small as\npossible. This often leads to situations where we have too many indexes. We add more queries, create new indexes for\nthem, and often it turns out that this newly created index takes over the queries of another one.\nAs a result, we have to maintain many unnecessary indexes, wasting disk space and CPU time.</p>\n\n<p>Fortunately, it is possible to check the number of index uses with the query:</p>\n\n<div class=\"language-sql highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"n\">db</span><span class=\"p\">.</span><span class=\"n\">COLLECTION_NAME</span><span class=\"p\">.</span><span class=\"k\">aggregate</span><span class=\"p\">([</span><span class=\"err\">{$</span><span class=\"n\">indexStats</span><span class=\"p\">:</span><span class=\"err\">{}}</span><span class=\"p\">])</span>\n</code></pre></div></div>\n\n<p>It allows you to find indexes that are not used so you can safely delete them.</p>\n\n<p>And finally, one more thing. Indexes can be removed quickly, but they take much longer to rebuild. This means that the\nconsequences of removal of an index by mistake can be severe. Therefore it is better to make sure that the index to be\ndeleted is definitely not used by any query. The latest MongoDB 4.4 provides a command that helps to avoid errors:</p>\n\n<div class=\"language-sql highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"n\">db</span><span class=\"p\">.</span><span class=\"n\">COLLECTION_NAME</span><span class=\"p\">.</span><span class=\"n\">hideIndex</span><span class=\"p\">(</span><span class=\"o\">&lt;</span><span class=\"k\">index</span><span class=\"o\">&gt;</span><span class=\"p\">)</span>\n</code></pre></div></div>\n\n<p>The above-mentioned command hides the index from the query optimizer. It does not take this index into account when\nbuilding the query execution plan, but the index is still updated when modifying documents. Thanks to that, if it turns\nout that it was needed, we are able to restore it immediately and it will still be up-to-date.</p>\n\n<h3 id=\"conclusion\">Conclusion</h3>\n\n<p>Using a few simple techniques, preparing several versions of the schema and using stats() command we are able to design\na model which does not overload our infrastructure. I encourage you to experiment with your own databases. Maybe you too\ncan save some disk space and CPU time.</p>\n","contentSnippet":"So I was tuning one of our services in order to speed up some MongoDB queries. Incidentally, my attention was caught by\nthe size of one of the collections that contains archived objects and therefore is rarely used. Unfortunately I wasn’t\nable to reduce the size of the documents stored there, but I started to wonder: is it possible to store the same data\nin a more compact way? Mongo stores JSON that allows many different ways of expressing similar data, so there seems\nto be room for improvements.\nIt is worth asking: why make such an effort in the era of Big Data and unlimited resources? There are several reasons.\nFirst of all, the resources are not unlimited and at the end we have physical drives that cost money to buy, replace,\nmaintain, and be supplied with power.\nSecondly, less stored data results in less time to access it. Moreover, less data means that more of it will fit into\ncache, so the next data access will be an order of magnitude faster.\nI decided to do some experiments and check how the model design affects the size of database files.\nI used a local MongoDB Community Edition 4.4 installation and I initially tested collections containing 1\nmillion and 10 million documents. One of the variants contained up to 100 million, but the results were proportional\n(nearly linear). Therefore, in the end I decided to stop at 1M collections, because loading the data was simply much faster.\nHaving access to local database files, I could easily check the size of the files storing individual collections.\nHowever, it turned out to be unnecessary, because the same data can be obtained with the command:\n\ndb.getCollection('COLLECTION_NAME').stats()\n\n\n\nThe following fields are expressed in bytes:\nsize: size of all collection documents, before compression,\navgObjSize: average document size, before compression,\nstorageSize: file size on the disk; this is the value after the data is compressed, the same value is returned by\nls command,\nfreeStorageSize: size of unused space allocated for the data; Mongo does not increase the file size byte-by-byte,\nbut allocates a percentage of the current file size and this value indicates how much data will still fit into the file.\nTo present the results I used (storageSize - freeStorageSize) value that indicates the actual space occupied by the data.\nMy local MongoDB instance had compression enabled. Not every storage engine has this option enabled by default, so when\nyou start your own analysis it is worth checking how it is in your particular case.\nId field type\nIn the beginning I decided to check ID fields. Not the collection primary key, mind you – it is ‘ObjectId’ type by\ndefault and generally shouldn’t be changed. I decided to focus on user\nand offers identifiers which, although numerical, are often saved as String in Mongo. I believe it comes partly due to\nthe contract of our services - identifiers in JSON most often come as Strings and in this form they are later stored\nin our databases.\nLet’s start with some theory: the number of int32 type in Mongo has a fixed size of 4 bytes. The same number\nwritten as a String of characters has a size dependent on the number of digits; additionally it contains the length of\nthe text (4 bytes) and a terminator character. For example, the text “0” is 12 bytes long and “1234567890” is 25 bytes\nlong. So in theory we should get interesting results, but what does it look like in reality?\nI prepared 2 collections, one million documents each, containing the following documents:\n\n{ \"_id\" : 1 }\n\n\nand\n\n{ \"_id\" : \"1\" }\n\n\nThe values of identifiers were consecutive natural numbers. Here is the comparison of results:\n\nAs you can see the difference is significant since the size on disk decreased by half. Additionally, it is worth\nnoting here that my data is synthetic and the identifiers start from 1. The advantage of a numerical identifier over a\nString increases the more digits a number has, so benefits should be even better for the real life data.\nEncouraged by the success I decided to check if field type had any influence on the size of an index created on it. In\nthis case, unfortunately, I got disappointed: the sizes were similar. This is due to the fact that MongoDB\nuses a hash function when creating indexes, so physically both indexes are composed of numerical values.\nHowever, if we are dealing with hashing, maybe at least searching by index in a numerical field is faster?\nI made a comparison of searching for a million and 10 million documents by indexed key in a random but the same order\nfor both collections. Again, a missed shot: both tests ended up with a similar result, so the conclusion is this:\nit is worth using numerical identifiers, because they require less disk space, but we will not get additional benefits\nassociated with indexes on these fields.\n\n\nSimple and complex structures\nLet’s move on to more complex data. Our models are often built from smaller structures grouping data such as user,\norder or offer. I decided to compare the size of documents storing such structures and their flat counterparts:\n\n{\"_id\": 1, \"user\": {\"name\": \"Jan\", \"surname\": \"Nowak\"}}\n\n\nand\n\n{\"_id\": 1, \"userName\": \"Jan\", \"userSurname\": \"Nowak\"}\n\n\nIn both cases we store identical data, the documents differ only in the schema. Take a look at the result:\n\nThere is a slight reduction by 0.4MB. It may seem not much compared to the effect we achieved for the field\ncontaining an ID. However, we have to bear in mind that in this case we were dealing with a more complex document. It\ncontained – in addition to the compared fields – a numerical type identifier that, as we remember from previous\nexperiments, takes up about 5MB of disk space.\nTaking this into account in the above results we are talking about a decrease from 3.4MB to 3MB. It actually looks\nbetter as percentage - we saved 12% of the space needed to store personal data.\nLet’s go back to the discussed documents for a moment:\n\n{\"_id\": 1, \"user\": {\"name\": \"Jan\", \"surname\": \"Nowak\"}}\n\n\nand\n\n{\"_id\": 1, \"userName\": \"Jan\", \"userSurname\": \"Nowak\"}\n\n\nA watchful eye will notice that I used longer field names in the document after flattening. So instead of user.name\nand user.surname I made userName and userSurname. I did it automatically, a bit unconsciously, to make the\nresulting JSON more readable.  However, if by changing only the schema of the document from compound to flat we managed\nto reduce the size of the data, maybe it is worth to go a step further and shorten the field names?\nI decided to add a third document for comparison, flattened and with shorter field names:\n\n{\"_id\": 1, \"name\": \"Jan\", \"surname\": \"Nowak\"}\n\n\nThe results are shown in the chart below:\n\nThe result is even better than just flattening. Apart from the document’s key size, we achieved a decrease from\n3.4MB to 2MB. Why does this happen even though we store exactly the same data?\nThe reason for the decrease is the nature of NoSQL databases that, unlike the relational ones, do not have a schema\ndefined at the level of the entire collection. If someone is very stubborn, they can store user data, offers, orders and\npayments in one collection. It would still be possible to read, index and search that data. This is because the\ndatabase, in addition to the data itself, stores its schema with each document. Thus, the total size of a document\nconsists of its data and its schema. And that explains the whole puzzle. By reducing the size of the schema, we also\nreduce the size of each document, i.e. the size of the final file with the collection data. It is worth taking this into\naccount when designing a collection schema in order not to blow it up too much. Of course, you cannot go to extremes, because\nthat would lead us to fields named a, b and c, what would make the collection completely illegible for a human.\nFor very large collections, however, this approach is used, an example of which is the MongoDB operation log\nthat contains fields called:\nh\n  \nop\nns\no\n  \no2\nb\n\n\nEmpty fields\nSince we are at the document’s schema, it is still worth looking at the problem of blank fields. In the case of\nJSON the lack of value in a certain field can be written in two ways, either directly - by writing null in its value -\nor by not serializing the field at all. I prepared a comparison of documents with the following structure:\n\n{ \"id\" : 1 }\n\n\nand\n\n{ \"id\" : 1, \"phone\" : null}\n\n\nThe meaning of the data in both documents is identical - the user has no phone number. However, the schema of the second\ndocument is different from the first one because it contains two fields.\nHere are the results:\n\nThe results are quite surprising: saving a million null’s is quite expensive as it takes more than 1MB on a disk.\nEnumerated types\nNow, let’s also look at the enumerated types. You can store them in two ways, either by using a label:\n\n{\"_id\": 1, \"source\": \"ALLEGRO\"}\n\n\nor by ordinal value:\n\n{\"_id\": 1, \"source\": 1}\n\n\nHere an analogy with the first experiment can be found: again we replace a character string with a numerical value.\nSince we got a great result the first time, maybe we could repeat it here?\n\nUnfortunately, the result is disappointing and the reduction in size is small. However, if we think more deeply, we will\ncome to the conclusion that it could not have been otherwise. The enumerated types are not unique identifiers. We are\ndealing only with a few possible values, appearing many times in the whole collection, so it’s a great chance for\nMongoDB data compression to prove its worth. Most likely the values from the first collection have been automatically replaced by the\nengine to its corresponding ordinal values. Additionally, we don’t have any profit on the schema here, because they are\nthe same in both cases. The situation might be different for collections that do not have data compression enabled.\nThis is a good time to take a closer look at how snappy\ncompression works in MongoDB. I’ve prepared two more collections, with identical data, but with data compression\nturned off. The results are shown in the chart below, compiled together with the collections with data compression turned on.\n\nIt is clear that the use of an ordinal value instead of a label of the enumerated type brings considerable profit for\ncollections with data compression disabled. In case of lack of compression it is definitely worth considering using numerical\nvalues.\nUseless _class field\nIf we use spring-data, each of our collections additionally contains a _class field storing the package and the\nname of the entity class containing the mapping for documents from this collection. This mechanism is used to support\ninheritance of model classes, that is not widely used. In most cases this field stores exactly the same values for each\ndocument what makes it useless. And how much does it cost? Let’s compare the following documents:\n\n{\"_id\": 1}\n\n\nand\n\n{\"_id\": 1, \"_class\": \"pl.allegro.some.project.domain.sample\"}\n\n\n\nThe difference is considerable, over 50%. Bearing in mind that the compression is enabled, I believe that the impact of\nthe data itself is small and the result is caused by the schema of the collection containing only the key being half as small\nas the one with the key (1 field vs 2 fields). After removal of the _class field from a document with more fields,\nthe difference expressed in percent will be much smaller of course. However, storing useless data does not make sense.\nUseless indexes\nWhen investigating the case with identifiers stored as strings, I checked whether manipulating the data type could reduce\nthe index size. Unfortunately I did not succeed, so I decided to focus on the problem of redundant indexes.\nIt is usually a good practice to cover each query with an index so that the number of collscan operations is as small as\npossible. This often leads to situations where we have too many indexes. We add more queries, create new indexes for\nthem, and often it turns out that this newly created index takes over the queries of another one.\nAs a result, we have to maintain many unnecessary indexes, wasting disk space and CPU time.\nFortunately, it is possible to check the number of index uses with the query:\n\ndb.COLLECTION_NAME.aggregate([{$indexStats:{}}])\n\n\nIt allows you to find indexes that are not used so you can safely delete them.\nAnd finally, one more thing. Indexes can be removed quickly, but they take much longer to rebuild. This means that the\nconsequences of removal of an index by mistake can be severe. Therefore it is better to make sure that the index to be\ndeleted is definitely not used by any query. The latest MongoDB 4.4 provides a command that helps to avoid errors:\n\ndb.COLLECTION_NAME.hideIndex(<index>)\n\n\nThe above-mentioned command hides the index from the query optimizer. It does not take this index into account when\nbuilding the query execution plan, but the index is still updated when modifying documents. Thanks to that, if it turns\nout that it was needed, we are able to restore it immediately and it will still be up-to-date.\nConclusion\nUsing a few simple techniques, preparing several versions of the schema and using stats() command we are able to design\na model which does not overload our infrastructure. I encourage you to experiment with your own databases. Maybe you too\ncan save some disk space and CPU time.","guid":"https://blog.allegro.tech/2021/01/impact-of-the-data-model-on-the-MongoDB-database-size.html","categories":["mongodb"],"isoDate":"2021-01-13T23:00:00.000Z","thumbnail":"images/post-headers/mongodb.png"},{"title":"Speeding up warm builds in Xcode","link":"https://blog.allegro.tech/2020/12/speeding-up-warm-builds.html","pubDate":"Mon, 28 Dec 2020 00:00:00 +0100","authors":{"author":[{"name":["Maciej Piotrowski"],"photo":["https://blog.allegro.tech/img/authors/maciej.piotrowski.jpg"],"url":["https://blog.allegro.tech/authors/maciej.piotrowski"]}]},"content":"<p>Programmers who have ever developed software for Apple platforms in the early days of <strong>Swift</strong> language might remember ridiculous\ntimes it took to compile the whole project. For large and complicated codebase times used to range from 10 up to 40 minutes.\nOver the years our toolset has improved alongside with compilation times, but slow build times of source code can still be a nightmare.</p>\n\n<p>When we wait a few minutes for a build, we navigate ourselves towards different activities and start e.g. watching funny animal pictures or\nYouTube videos, easily <strong>loosing context</strong> of the task at hand. What becomes annoying for us is <strong>slow feedback</strong> of code correctness.</p>\n\n<p>In the <a href=\"https://allegro.tech/2020/12/speeding-up-ios-builds-with-bazel.html\">past issue</a> my colleague has written about a solution\nto slow <strong>clean</strong> builds.In this post I will focus on <strong>warm</strong> builds improvement.</p>\n\n<h2 id=\"clean-and-incremental-builds\">Clean and incremental builds</h2>\n\n<p>There are two terms used in the realm of <a href=\"https://developer.apple.com/xcode/\">Xcode</a> when it comes to distinguishing types of\ncompilation: <strong>clean</strong> and <strong>incremental</strong> build. The first refers to the time it takes to build a project from scratch. The latter is the time\nit takes to build only whatever changed since the last build and to integrate the changes into a build product.</p>\n\n<p>You might also be familiar with the term <strong>warm</strong> build. It’s used interchangeably with <strong>incremental</strong> build term, but for the sake of this\npost I will be using it to refer to <em>the time it takes to build a product since the last clean build without introducing any source code\nchanges</em>.</p>\n\n<h2 id=\"why-and-what-for\">Why and what for</h2>\n\n<p>Why bothering with improving <strong>warm</strong> build times? Well, for a small projects built on super fast workstations it might take a fraction of a\nsecond to do a warm build, but as projects grow and multiple <em>Build Phases</em> get added to a target so grow the build times. These times\nare noticeable especially when one builds the target without an introduction of changes to the source code.</p>\n\n<p>Before we started improving the warm build of the Allegro app for iOS platform it took 18 seconds to perform the build on our\nContinuous Integration (CI) servers (Mac Mini, 6-Core 3.2 GHz CPU, 32 GB Ram).</p>\n\n<p>Is 18 seconds too much? When you put it into a perspective of 1 year:</p>\n\n<blockquote>\n  <p>18 seconds × 6 builds per hour × 8 hours per day × 20 days per month × 12 months per year = 207360 seconds = 3456 minutes = 57\nhours 36 minutes</p>\n</blockquote>\n\n<p>It means that on average a programmer spends around 57 hours 36 minutes yearly to wait for a feedback if their code is correct. Is it\nmuch? I leave the answer to you, but it definitely hinders developer’s experience and distracts the developer from their job.</p>\n\n<p>To make the developer’s experience better, we, the iOS Mobile Core Team at Allegro, have set the goal to minimize the time developers\nspend between hitting the build button and getting the feedback on their code as quickly as possible.</p>\n\n<p>How could the goal be achieved? Well, before I answer that, let’s put some light onto how to actually measure build times.</p>\n\n<h2 id=\"measurements\">Measurements</h2>\n\n<p>Developers building software for Apple platforms use the <a href=\"https://developer.apple.com/xcode/\">Xcode</a> application which has a command\nline interface called <code class=\"language-plaintext highlighter-rouge\">xcodebuild</code>. The Xcode has an option to output times for build phases from the menu\n<code class=\"language-plaintext highlighter-rouge\">Product &gt; Perform Action &gt; Build With Timing Summary</code> (doesn’t seem to work on Xcode 12.2 at the time of writing this\nblog post). To get build times with <code class=\"language-plaintext highlighter-rouge\">xcodebuild</code> for our Allegro app for each build phase of the main target the following command\ncan be used:</p>\n\n<div class=\"language-sh highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>xcodebuild <span class=\"nt\">-workspace</span> <span class=\"s1\">'Allegro/Allegro.xcworkspace'</span> <span class=\"se\">\\</span>\n<span class=\"nt\">-scheme</span> <span class=\"s1\">'Allegro'</span> <span class=\"se\">\\</span>\n<span class=\"nt\">-configuration</span> <span class=\"s1\">'Debug'</span> <span class=\"se\">\\</span>\n<span class=\"nt\">-sdk</span> <span class=\"s1\">'iphonesimulator'</span> <span class=\"se\">\\</span>\n<span class=\"nt\">-arch</span> <span class=\"s1\">'x86_64'</span> <span class=\"se\">\\</span>\n<span class=\"nt\">-showBuildTimingSummary</span> <span class=\"se\">\\</span>\nbuild <span class=\"se\">\\</span>\n| <span class=\"nb\">sed</span> <span class=\"nt\">-n</span> <span class=\"nt\">-e</span> <span class=\"s1\">'/Build Timing Summary/,$p'</span>\n</code></pre></div></div>\n\n<p>In the case of the Allegro app it outputs the following lines when I do a <strong>clean</strong> build with Xcode 12.2’s <code class=\"language-plaintext highlighter-rouge\">xcodebuild</code>\n(Mac Book Pro 2.2 GHz 6-Core Intel Core i7 CPU, 32 GB RAM):</p>\n\n<div class=\"language-sh highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>Build Timing Summary\n\nCompileC <span class=\"o\">(</span>49 tasks<span class=\"o\">)</span> | 174.459 seconds\n\nCompileSwiftSources <span class=\"o\">(</span>3 tasks<span class=\"o\">)</span> | 31.747 seconds\n\nCompileStoryboard <span class=\"o\">(</span>6 tasks<span class=\"o\">)</span> | 29.057 seconds\n\nPhaseScriptExecution <span class=\"o\">(</span>8 tasks<span class=\"o\">)</span> | 22.320 seconds\n\nDitto <span class=\"o\">(</span>21 tasks<span class=\"o\">)</span> | 22.282 seconds\n\nLd <span class=\"o\">(</span>3 tasks<span class=\"o\">)</span> | 13.432 seconds\n\nCompileAssetCatalog <span class=\"o\">(</span>1 task<span class=\"o\">)</span> | 6.620 seconds\n\nValidateEmbeddedBinary <span class=\"o\">(</span>2 tasks<span class=\"o\">)</span> | 6.528 seconds\n\nCompileXIB <span class=\"o\">(</span>1 task<span class=\"o\">)</span> | 5.000 seconds\n\nCodeSign <span class=\"o\">(</span>3 tasks<span class=\"o\">)</span> | 1.419 seconds\n\nCopyPNGFile <span class=\"o\">(</span>3 tasks<span class=\"o\">)</span> | 1.236 seconds\n\nTouch <span class=\"o\">(</span>4 tasks<span class=\"o\">)</span> | 0.318 seconds\n\nLibtool <span class=\"o\">(</span>1 task<span class=\"o\">)</span> | 0.241 seconds\n\nLinkStoryboards <span class=\"o\">(</span>2 tasks<span class=\"o\">)</span> | 0.108 seconds\n</code></pre></div></div>\n\n<p>There’s a lot of tasks. When it comes to source code compilation process, we can lower down build times by splitting the source code\ninto modular frameworks, using binary caching techniques and adding explicit types for expressions in Swift.</p>\n\n<p>In the case of a <strong>warm build</strong> the only phases listed are:</p>\n\n<div class=\"language-sh highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>Build Timing Summary\n\nPhaseScriptExecution <span class=\"o\">(</span>6 tasks<span class=\"o\">)</span> | 23.350 seconds\n\nValidateEmbeddedBinary <span class=\"o\">(</span>2 tasks<span class=\"o\">)</span> | 2.424 seconds\n\n<span class=\"k\">**</span> BUILD SUCCEEDED <span class=\"k\">**</span> <span class=\"o\">[</span>27.238 sec]\n</code></pre></div></div>\n\n<p>Thanks to performing the <strong>warm build</strong> it can be easily noticed that there’s a room for improvement when it comes to\n<code class=\"language-plaintext highlighter-rouge\">PhaseScriptExecution</code> part of the build process. This is actually the part over which we have the control of. Let’s see, what we can\ndo in order to speed up the build time by playing with what and how scripts get executed.</p>\n\n<h2 id=\"cleaning-up-run-scripts\">Cleaning up run scripts</h2>\n\n<p>First thing we did with for our iOS application target was selecting scripts which can be run only for Release builds. There’s an easy way\nin Xcode to mark them as runnable for such builds only - just select <code class=\"language-plaintext highlighter-rouge\">For install builds only</code> checkbox.</p>\n\n<p><img src=\"/img/articles/2020-12-28-speeding-up-warm-builds/xcode-run-for-release.png\" alt=\"Run script: For install builds only - checkbox in Xcode\" /></p>\n\n<p>What jobs are great for running only for Release builds? We selected a few:</p>\n\n<ul>\n  <li>uploading debug symbols to 3rd party monitoring services</li>\n  <li>setting endpoints or enabling Apple Transport Security (ATS) for Debug/Release builds</li>\n  <li>selecting proper <code class=\"language-plaintext highlighter-rouge\">.plist</code> files for Debug/Release builds</li>\n</ul>\n\n<p>Not all tasks can be selected as Release - only. Some of them need to be run for Debug and Release builds, but they don’t have to be\nrun for every build. Xcode 12 introduced a neat feature - running the script based on dependency analysis.</p>\n\n<p><img src=\"/img/articles/2020-12-28-speeding-up-warm-builds/xcode-dependency-analysis.png\" alt=\"Run script: Based on dependency analysis - checkbox in Xcode\" /></p>\n\n<p>Selecting the checkbox isn’t enough to benefit from dependency analysis. Xcode analyses dependencies of a script, i.e. it verifies if the\ninputs of the script have changed since the last run and if the outputs of the script exist. The potential problem occurred for scripts in our\nproject - they didn’t have explicit inputs and outputs defined so we couldn’t tap into the brand new feature of Xcode.</p>\n\n<h2 id=\"defining-inputs-and-outputs-for-scripts\">Defining inputs and outputs for scripts</h2>\n\n<p>One of the scripts in our project which is time-consuming copies bundles with resources of each module. Our Xcode workspace consists\nof multiple projects. The main project contains the application target which depends on modules built by other projects. The projects\ncontain static frameworks with resources. The resources for each framework are wrapped in <code class=\"language-plaintext highlighter-rouge\">.bundle</code> wrapper and are embedded in\nthe framework. All frameworks are linked statically to the application and their bundles are copied by the script to the application\nwrapper (<code class=\"language-plaintext highlighter-rouge\">.app</code>).</p>\n\n<p>The list with <code class=\"language-plaintext highlighter-rouge\">.bundle</code> files to be copied became an input to our script. We also created a list with paths to which bundles are copied.\nXcode uses a <code class=\"language-plaintext highlighter-rouge\">.xcfilelist</code> format for such lists, but it’s just a file with newline-separated values. The\n<code class=\"language-plaintext highlighter-rouge\">copy-bundles-input.xcfilelist</code> input to our script looks as such:</p>\n\n<div class=\"language-sh highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"si\">$(</span>BUILT_PRODUCTS_DIR<span class=\"si\">)</span>/ModuleX.framework/ModuleX.bundle\n<span class=\"si\">$(</span>BUILT_PRODUCTS_DIR<span class=\"si\">)</span>/ModuleY.framework/ModuleY.bundle\n<span class=\"si\">$(</span>BUILT_PRODUCTS_DIR<span class=\"si\">)</span>/ModuleZ.framework/ModuleZ.bundle\n</code></pre></div></div>\n\n<p>and the <code class=\"language-plaintext highlighter-rouge\">copy-bundles-output.xcfilelist</code> output:</p>\n\n<div class=\"language-sh highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"si\">$(</span>TARGET_BUILD_DIR<span class=\"si\">)</span>/<span class=\"si\">$(</span>EXECUTABLE_FOLDER_PATH<span class=\"si\">)</span>/ModuleX.bundle\n<span class=\"si\">$(</span>TARGET_BUILD_DIR<span class=\"si\">)</span>/<span class=\"si\">$(</span>EXECUTABLE_FOLDER_PATH<span class=\"si\">)</span>/ModuleY.bundle\n<span class=\"si\">$(</span>TARGET_BUILD_DIR<span class=\"si\">)</span>/<span class=\"si\">$(</span>EXECUTABLE_FOLDER_PATH<span class=\"si\">)</span>/ModuleZ.bundle\n</code></pre></div></div>\n\n<p>File lists can be accessed in a script through environment variables. Each script can have many of them and they are indexed from 0:</p>\n\n<ul>\n  <li><code class=\"language-plaintext highlighter-rouge\">SCRIPT_INPUT_FILE_LIST_0</code></li>\n  <li>…</li>\n  <li><code class=\"language-plaintext highlighter-rouge\">SCRIPT_INPUT_FILE_LIST_1024</code></li>\n  <li><code class=\"language-plaintext highlighter-rouge\">SCRIPT_OUTPUT_FILE_LIST_0</code></li>\n  <li>…</li>\n  <li><code class=\"language-plaintext highlighter-rouge\">SCRIPT_OUTPUT_FILE_LIST_1024</code></li>\n</ul>\n\n<p>There is also a possibility to use input and output files instead of a list (not shown on the screens):</p>\n\n<ul>\n  <li><code class=\"language-plaintext highlighter-rouge\">SCRIPT_INPUT_FILE_0</code></li>\n  <li>…</li>\n  <li><code class=\"language-plaintext highlighter-rouge\">SCRIPT_INPUT_FILE_1024</code></li>\n  <li><code class=\"language-plaintext highlighter-rouge\">SCRIPT_OUTPUT_FILE_0</code></li>\n  <li>…</li>\n  <li><code class=\"language-plaintext highlighter-rouge\">SCRIPT_OUTPUT_FILE_1024</code></li>\n  <li>and additionally the <code class=\"language-plaintext highlighter-rouge\">SCRIPT_INPUT_FILE_COUNT</code> and <code class=\"language-plaintext highlighter-rouge\">SCRIPT_OUTPUT_FILE_COUNT</code> can be used</li>\n</ul>\n\n<p>We based our script copying resource bundles only on file lists and it’s actually quite simple - it just copies files from the\ninput file list to the destination which is the path to the executable.</p>\n\n<div class=\"language-sh highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"nv\">destination</span><span class=\"o\">=</span><span class=\"s2\">\"</span><span class=\"k\">${</span><span class=\"nv\">TARGET_BUILD_DIR</span><span class=\"k\">}</span><span class=\"s2\">/</span><span class=\"k\">${</span><span class=\"nv\">EXECUTABLE_FOLDER_PATH</span><span class=\"k\">}</span><span class=\"s2\">\"</span>\n<span class=\"nb\">grep</span> <span class=\"nt\">-v</span> <span class=\"s1\">'^ *#'</span> &lt; <span class=\"s2\">\"</span><span class=\"k\">${</span><span class=\"nv\">SCRIPT_INPUT_FILE_LIST_0</span><span class=\"k\">}</span><span class=\"s2\">\"</span> | <span class=\"k\">while </span><span class=\"nv\">IFS</span><span class=\"o\">=</span> <span class=\"nb\">read</span> <span class=\"nt\">-r</span> bundle_path\n<span class=\"k\">do\n    if</span> <span class=\"o\">[</span> <span class=\"nt\">-d</span> <span class=\"s2\">\"</span><span class=\"nv\">$bundle_path</span><span class=\"s2\">\"</span> <span class=\"o\">]</span><span class=\"p\">;</span> <span class=\"k\">then\n        </span>rsync <span class=\"nt\">-auv</span> <span class=\"s2\">\"</span><span class=\"k\">${</span><span class=\"nv\">bundle_path</span><span class=\"k\">}</span><span class=\"s2\">\"</span> <span class=\"s2\">\"</span><span class=\"k\">${</span><span class=\"nv\">destination</span><span class=\"k\">}</span><span class=\"s2\">\"</span> <span class=\"o\">||</span> <span class=\"nb\">exit </span>1\n    <span class=\"k\">fi\ndone</span>\n</code></pre></div></div>\n\n<p>In the end we tapped into using Xcode’s dependency analysis for a few run scripts and it allowed us to improve warm build time.</p>\n\n<div class=\"language-sh highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>Build Timing Summary\n\nPhaseScriptExecution <span class=\"o\">(</span>6 tasks<span class=\"o\">)</span> | 3.666 seconds\n\nValidateEmbeddedBinary <span class=\"o\">(</span>2 tasks<span class=\"o\">)</span> | 2.314 seconds\n\n<span class=\"k\">**</span> BUILD SUCCEEDED <span class=\"k\">**</span> <span class=\"o\">[</span>7.500 sec]\n</code></pre></div></div>\n\n<p><img src=\"/img/articles/2020-12-28-speeding-up-warm-builds/warm-build-graph.png\" alt=\"Allegro iOS - graph depicting Warm Build Time change over months\" /></p>\n\n<p>At the time of writing the <strong>warm build time</strong> on our CI machines takes <strong>4 seconds</strong>. The overall goal of speeding up builds is so that\nthe <strong>clean build</strong> time becomes equal to <strong>warm build</strong>.</p>\n\n<h2 id=\"links\">Links</h2>\n\n<p>Some useful links related to improving compilation times for Xcode projects:</p>\n\n<ul>\n  <li><a href=\"https://www.onswiftwings.com/posts/build-time-optimization-part1/\">Xcode Build Time Optimization - Part 1</a></li>\n  <li><a href=\"https://www.onswiftwings.com/posts/build-time-optimization-part2/\">Xcode Build Time Optimization - Part 2</a></li>\n  <li><a href=\"https://eisel.me/signing\">Disabling code signing for Debug builds</a></li>\n</ul>\n","contentSnippet":"Programmers who have ever developed software for Apple platforms in the early days of Swift language might remember ridiculous\ntimes it took to compile the whole project. For large and complicated codebase times used to range from 10 up to 40 minutes.\nOver the years our toolset has improved alongside with compilation times, but slow build times of source code can still be a nightmare.\nWhen we wait a few minutes for a build, we navigate ourselves towards different activities and start e.g. watching funny animal pictures or\nYouTube videos, easily loosing context of the task at hand. What becomes annoying for us is slow feedback of code correctness.\nIn the past issue my colleague has written about a solution\nto slow clean builds.In this post I will focus on warm builds improvement.\nClean and incremental builds\nThere are two terms used in the realm of Xcode when it comes to distinguishing types of\ncompilation: clean and incremental build. The first refers to the time it takes to build a project from scratch. The latter is the time\nit takes to build only whatever changed since the last build and to integrate the changes into a build product.\nYou might also be familiar with the term warm build. It’s used interchangeably with incremental build term, but for the sake of this\npost I will be using it to refer to the time it takes to build a product since the last clean build without introducing any source code\nchanges.\nWhy and what for\nWhy bothering with improving warm build times? Well, for a small projects built on super fast workstations it might take a fraction of a\nsecond to do a warm build, but as projects grow and multiple Build Phases get added to a target so grow the build times. These times\nare noticeable especially when one builds the target without an introduction of changes to the source code.\nBefore we started improving the warm build of the Allegro app for iOS platform it took 18 seconds to perform the build on our\nContinuous Integration (CI) servers (Mac Mini, 6-Core 3.2 GHz CPU, 32 GB Ram).\nIs 18 seconds too much? When you put it into a perspective of 1 year:\n18 seconds × 6 builds per hour × 8 hours per day × 20 days per month × 12 months per year = 207360 seconds = 3456 minutes = 57\nhours 36 minutes\nIt means that on average a programmer spends around 57 hours 36 minutes yearly to wait for a feedback if their code is correct. Is it\nmuch? I leave the answer to you, but it definitely hinders developer’s experience and distracts the developer from their job.\nTo make the developer’s experience better, we, the iOS Mobile Core Team at Allegro, have set the goal to minimize the time developers\nspend between hitting the build button and getting the feedback on their code as quickly as possible.\nHow could the goal be achieved? Well, before I answer that, let’s put some light onto how to actually measure build times.\nMeasurements\nDevelopers building software for Apple platforms use the Xcode application which has a command\nline interface called xcodebuild. The Xcode has an option to output times for build phases from the menu\nProduct > Perform Action > Build With Timing Summary (doesn’t seem to work on Xcode 12.2 at the time of writing this\nblog post). To get build times with xcodebuild for our Allegro app for each build phase of the main target the following command\ncan be used:\n\nxcodebuild -workspace 'Allegro/Allegro.xcworkspace' \\\n-scheme 'Allegro' \\\n-configuration 'Debug' \\\n-sdk 'iphonesimulator' \\\n-arch 'x86_64' \\\n-showBuildTimingSummary \\\nbuild \\\n| sed -n -e '/Build Timing Summary/,$p'\n\n\nIn the case of the Allegro app it outputs the following lines when I do a clean build with Xcode 12.2’s xcodebuild\n(Mac Book Pro 2.2 GHz 6-Core Intel Core i7 CPU, 32 GB RAM):\n\nBuild Timing Summary\n\nCompileC (49 tasks) | 174.459 seconds\n\nCompileSwiftSources (3 tasks) | 31.747 seconds\n\nCompileStoryboard (6 tasks) | 29.057 seconds\n\nPhaseScriptExecution (8 tasks) | 22.320 seconds\n\nDitto (21 tasks) | 22.282 seconds\n\nLd (3 tasks) | 13.432 seconds\n\nCompileAssetCatalog (1 task) | 6.620 seconds\n\nValidateEmbeddedBinary (2 tasks) | 6.528 seconds\n\nCompileXIB (1 task) | 5.000 seconds\n\nCodeSign (3 tasks) | 1.419 seconds\n\nCopyPNGFile (3 tasks) | 1.236 seconds\n\nTouch (4 tasks) | 0.318 seconds\n\nLibtool (1 task) | 0.241 seconds\n\nLinkStoryboards (2 tasks) | 0.108 seconds\n\n\nThere’s a lot of tasks. When it comes to source code compilation process, we can lower down build times by splitting the source code\ninto modular frameworks, using binary caching techniques and adding explicit types for expressions in Swift.\nIn the case of a warm build the only phases listed are:\n\nBuild Timing Summary\n\nPhaseScriptExecution (6 tasks) | 23.350 seconds\n\nValidateEmbeddedBinary (2 tasks) | 2.424 seconds\n\n** BUILD SUCCEEDED ** [27.238 sec]\n\n\nThanks to performing the warm build it can be easily noticed that there’s a room for improvement when it comes to\nPhaseScriptExecution part of the build process. This is actually the part over which we have the control of. Let’s see, what we can\ndo in order to speed up the build time by playing with what and how scripts get executed.\nCleaning up run scripts\nFirst thing we did with for our iOS application target was selecting scripts which can be run only for Release builds. There’s an easy way\nin Xcode to mark them as runnable for such builds only - just select For install builds only checkbox.\n\nWhat jobs are great for running only for Release builds? We selected a few:\nuploading debug symbols to 3rd party monitoring services\nsetting endpoints or enabling Apple Transport Security (ATS) for Debug/Release builds\nselecting proper .plist files for Debug/Release builds\nNot all tasks can be selected as Release - only. Some of them need to be run for Debug and Release builds, but they don’t have to be\nrun for every build. Xcode 12 introduced a neat feature - running the script based on dependency analysis.\n\nSelecting the checkbox isn’t enough to benefit from dependency analysis. Xcode analyses dependencies of a script, i.e. it verifies if the\ninputs of the script have changed since the last run and if the outputs of the script exist. The potential problem occurred for scripts in our\nproject - they didn’t have explicit inputs and outputs defined so we couldn’t tap into the brand new feature of Xcode.\nDefining inputs and outputs for scripts\nOne of the scripts in our project which is time-consuming copies bundles with resources of each module. Our Xcode workspace consists\nof multiple projects. The main project contains the application target which depends on modules built by other projects. The projects\ncontain static frameworks with resources. The resources for each framework are wrapped in .bundle wrapper and are embedded in\nthe framework. All frameworks are linked statically to the application and their bundles are copied by the script to the application\nwrapper (.app).\nThe list with .bundle files to be copied became an input to our script. We also created a list with paths to which bundles are copied.\nXcode uses a .xcfilelist format for such lists, but it’s just a file with newline-separated values. The\ncopy-bundles-input.xcfilelist input to our script looks as such:\n\n$(BUILT_PRODUCTS_DIR)/ModuleX.framework/ModuleX.bundle\n$(BUILT_PRODUCTS_DIR)/ModuleY.framework/ModuleY.bundle\n$(BUILT_PRODUCTS_DIR)/ModuleZ.framework/ModuleZ.bundle\n\n\nand the copy-bundles-output.xcfilelist output:\n\n$(TARGET_BUILD_DIR)/$(EXECUTABLE_FOLDER_PATH)/ModuleX.bundle\n$(TARGET_BUILD_DIR)/$(EXECUTABLE_FOLDER_PATH)/ModuleY.bundle\n$(TARGET_BUILD_DIR)/$(EXECUTABLE_FOLDER_PATH)/ModuleZ.bundle\n\n\nFile lists can be accessed in a script through environment variables. Each script can have many of them and they are indexed from 0:\nSCRIPT_INPUT_FILE_LIST_0\n…\n  \nSCRIPT_INPUT_FILE_LIST_1024\nSCRIPT_OUTPUT_FILE_LIST_0\n…\n  \nSCRIPT_OUTPUT_FILE_LIST_1024\nThere is also a possibility to use input and output files instead of a list (not shown on the screens):\nSCRIPT_INPUT_FILE_0\n…\n  \nSCRIPT_INPUT_FILE_1024\nSCRIPT_OUTPUT_FILE_0\n…\n  \nSCRIPT_OUTPUT_FILE_1024\nand additionally the SCRIPT_INPUT_FILE_COUNT and SCRIPT_OUTPUT_FILE_COUNT can be used\nWe based our script copying resource bundles only on file lists and it’s actually quite simple - it just copies files from the\ninput file list to the destination which is the path to the executable.\n\ndestination=\"${TARGET_BUILD_DIR}/${EXECUTABLE_FOLDER_PATH}\"\ngrep -v '^ *#' < \"${SCRIPT_INPUT_FILE_LIST_0}\" | while IFS= read -r bundle_path\ndo\n    if [ -d \"$bundle_path\" ]; then\n        rsync -auv \"${bundle_path}\" \"${destination}\" || exit 1\n    fi\ndone\n\n\nIn the end we tapped into using Xcode’s dependency analysis for a few run scripts and it allowed us to improve warm build time.\n\nBuild Timing Summary\n\nPhaseScriptExecution (6 tasks) | 3.666 seconds\n\nValidateEmbeddedBinary (2 tasks) | 2.314 seconds\n\n** BUILD SUCCEEDED ** [7.500 sec]\n\n\n\nAt the time of writing the warm build time on our CI machines takes 4 seconds. The overall goal of speeding up builds is so that\nthe clean build time becomes equal to warm build.\nLinks\nSome useful links related to improving compilation times for Xcode projects:\nXcode Build Time Optimization - Part 1\nXcode Build Time Optimization - Part 2\nDisabling code signing for Debug builds","guid":"https://blog.allegro.tech/2020/12/speeding-up-warm-builds.html","categories":["ios","xcode","swift","objectivec"],"isoDate":"2020-12-27T23:00:00.000Z","thumbnail":"images/post-headers/xcode.png"}],"jobs":[{"id":"743999737489890","name":"Software Engineer (Java/Kotlin) - Delivery Experience","uuid":"739265ed-e9b3-46a1-b42e-08ef503c9ead","refNumber":"REF2584V","company":{"identifier":"Allegro","name":"Allegro"},"releasedDate":"2021-03-03T06:11:20.000Z","location":{"city":"Warszawa, Poznań, Kraków","country":"pl","remote":false},"industry":{"id":"internet","label":"Internet"},"department":{"id":"1013462","label":"Technology - Product Development"},"function":{"id":"engineering","label":"Engineering"},"typeOfEmployment":{"label":"Full-time"},"experienceLevel":{"id":"mid_senior_level","label":"Mid-Senior Level"},"customField":[{"fieldId":"58c15608e4b01d4b19ddf790","fieldLabel":"Proces rekrutacji","valueId":"c807eec2-8a53-4b55-b7c5-c03180f2059b","valueLabel":"IT Allegro"},{"fieldId":"COUNTRY","fieldLabel":"Country","valueId":"pl","valueLabel":"Poland"},{"fieldId":"58c13159e4b01d4b19ddf729","fieldLabel":"Department","valueId":"1013462","valueLabel":"Technology - Product Development"},{"fieldId":"58c13159e4b01d4b19ddf728","fieldLabel":"Brands","valueId":"4ccb4fab-6c3f-4ed0-9140-8533fe17447f","valueLabel":"Allegro"}],"ref":"https://api.smartrecruiters.com/v1/companies/allegro/postings/743999737489890","creator":{"name":"Paulina Partyka"},"language":{"code":"pl","label":"Polish","labelNative":"polski"}},{"id":"743999737406330","name":"Software Engineer (Java/Kotlin) - Merchant Experience","uuid":"d6e0540f-91cb-4e12-a0c5-54681e8cd66f","refNumber":"REF2567O","company":{"identifier":"Allegro","name":"Allegro"},"releasedDate":"2021-03-02T18:52:08.000Z","location":{"city":"Warszawa,Kraków,Poznań,Toruń","country":"pl","remote":false},"industry":{"id":"internet","label":"Internet"},"department":{"id":"1013462","label":"Technology - Product Development"},"function":{"id":"information_technology","label":"Information Technology"},"typeOfEmployment":{"label":"Full-time"},"experienceLevel":{"id":"mid_senior_level","label":"Mid-Senior Level"},"customField":[{"fieldId":"58c1575ee4b01d4b19ddf797","fieldLabel":"Kraków","valueId":"2cfcfcc1-da44-43f7-8eaa-3907faf2797c","valueLabel":"Tak"},{"fieldId":"58c15608e4b01d4b19ddf790","fieldLabel":"Proces rekrutacji","valueId":"c807eec2-8a53-4b55-b7c5-c03180f2059b","valueLabel":"IT Allegro"},{"fieldId":"COUNTRY","fieldLabel":"Country","valueId":"pl","valueLabel":"Poland"},{"fieldId":"58c158e9e4b0614667d5973e","fieldLabel":"Więcej lokalizacji","valueId":"3a186e8d-a82c-4955-af81-fcef9a63928a","valueLabel":"Tak"},{"fieldId":"58c13159e4b01d4b19ddf729","fieldLabel":"Department","valueId":"1013462","valueLabel":"Technology - Product Development"},{"fieldId":"58c13159e4b01d4b19ddf728","fieldLabel":"Brands","valueId":"4ccb4fab-6c3f-4ed0-9140-8533fe17447f","valueLabel":"Allegro"},{"fieldId":"58c1576ee4b0614667d59732","fieldLabel":"Toruń","valueId":"987e8884-bad1-4a8f-b149-99e4184cc221","valueLabel":"Tak"},{"fieldId":"58c156b9e4b01d4b19ddf792","fieldLabel":"Poznań","valueId":"ac20917b-cb6a-4280-aff3-4fae2532a33e","valueLabel":"Tak"},{"fieldId":"58c156e6e4b01d4b19ddf793","fieldLabel":"Warszawa","valueId":"6a428533-1586-4372-89ec-65fb45366363","valueLabel":"Tak"},{"fieldId":"5cdab2c84cedfd0006e7758c","fieldLabel":"Key words","valueLabel":"java, kotlin, scala"}],"ref":"https://api.smartrecruiters.com/v1/companies/allegro/postings/743999737406330","creator":{"name":"Katarzyna Faber"},"language":{"code":"pl","label":"Polish","labelNative":"polski"}},{"id":"743999737380043","name":"Product Manager (Delivery Experience)","uuid":"95a32a73-ebbe-47a8-aa18-e9ef4a738b4a","refNumber":"REF2245O","company":{"identifier":"Allegro","name":"Allegro"},"releasedDate":"2021-03-02T17:01:02.000Z","location":{"city":"Warszawa, Poznań","country":"pl","remote":false},"industry":{"id":"internet","label":"Internet"},"department":{"id":"1013462","label":"Technology - Product Development"},"function":{"id":"information_technology","label":"Information Technology"},"typeOfEmployment":{"label":"Full-time"},"experienceLevel":{"id":"mid_senior_level","label":"Mid-Senior Level"},"customField":[{"fieldId":"58c15608e4b01d4b19ddf790","fieldLabel":"Proces rekrutacji","valueId":"c807eec2-8a53-4b55-b7c5-c03180f2059b","valueLabel":"IT Allegro"},{"fieldId":"COUNTRY","fieldLabel":"Country","valueId":"pl","valueLabel":"Poland"},{"fieldId":"58c13159e4b01d4b19ddf729","fieldLabel":"Department","valueId":"1013462","valueLabel":"Technology - Product Development"},{"fieldId":"58c158e9e4b0614667d5973e","fieldLabel":"Więcej lokalizacji","valueId":"3a186e8d-a82c-4955-af81-fcef9a63928a","valueLabel":"Tak"},{"fieldId":"58c13159e4b01d4b19ddf728","fieldLabel":"Brands","valueId":"4ccb4fab-6c3f-4ed0-9140-8533fe17447f","valueLabel":"Allegro"},{"fieldId":"58c156b9e4b01d4b19ddf792","fieldLabel":"Poznań","valueId":"ac20917b-cb6a-4280-aff3-4fae2532a33e","valueLabel":"Tak"},{"fieldId":"58c156e6e4b01d4b19ddf793","fieldLabel":"Warszawa","valueId":"6a428533-1586-4372-89ec-65fb45366363","valueLabel":"Tak"}],"ref":"https://api.smartrecruiters.com/v1/companies/allegro/postings/743999737380043","creator":{"name":"Klaudia Sipurzyńska"},"language":{"code":"pl","label":"Polish","labelNative":"polski"}},{"id":"743999737133250","name":"IT Support Specialist","uuid":"d3cab909-a609-4ef2-a373-589719f7ce35","refNumber":"REF2542M","company":{"identifier":"Allegro","name":"Allegro"},"releasedDate":"2021-03-01T15:18:09.000Z","location":{"city":"Warszawa, Grodzisk Mazowiecki","country":"pl","remote":false},"industry":{"id":"internet","label":"Internet"},"department":{"id":"1013275","label":"IT Business Services"},"function":{"id":"engineering","label":"Engineering"},"typeOfEmployment":{"label":"Full-time"},"experienceLevel":{"id":"mid_senior_level","label":"Mid-Senior Level"},"customField":[{"fieldId":"58c15608e4b01d4b19ddf790","fieldLabel":"Proces rekrutacji","valueId":"c807eec2-8a53-4b55-b7c5-c03180f2059b","valueLabel":"IT Allegro"},{"fieldId":"COUNTRY","fieldLabel":"Country","valueId":"pl","valueLabel":"Poland"},{"fieldId":"58c13159e4b01d4b19ddf729","fieldLabel":"Department","valueId":"1013275","valueLabel":"IT Business Services"},{"fieldId":"58c13159e4b01d4b19ddf728","fieldLabel":"Brands","valueId":"4ccb4fab-6c3f-4ed0-9140-8533fe17447f","valueLabel":"Allegro"}],"ref":"https://api.smartrecruiters.com/v1/companies/allegro/postings/743999737133250","creator":{"name":"Paulina Partyka"},"language":{"code":"pl","label":"Polish","labelNative":"polski"}},{"id":"743999737101238","name":"DevOps Engineer (Systems Administrator)","uuid":"3d90db2b-4299-4653-a3bb-2ac9e9eca72f","refNumber":"REF2527R","company":{"identifier":"Allegro","name":"Allegro"},"releasedDate":"2021-03-01T13:14:28.000Z","location":{"city":"Toruń, Warszawa","country":"pl","remote":false},"industry":{"id":"internet","label":"Internet"},"department":{"id":"1013462","label":"Technology - Product Development"},"function":{"id":"engineering","label":"Engineering"},"typeOfEmployment":{"label":"Full-time"},"experienceLevel":{"id":"mid_senior_level","label":"Mid-Senior Level"},"customField":[{"fieldId":"58c15608e4b01d4b19ddf790","fieldLabel":"Proces rekrutacji","valueId":"c807eec2-8a53-4b55-b7c5-c03180f2059b","valueLabel":"IT Allegro"},{"fieldId":"COUNTRY","fieldLabel":"Country","valueId":"pl","valueLabel":"Poland"},{"fieldId":"58c13159e4b01d4b19ddf729","fieldLabel":"Department","valueId":"1013462","valueLabel":"Technology - Product Development"},{"fieldId":"58c13159e4b01d4b19ddf728","fieldLabel":"Brands","valueId":"4ccb4fab-6c3f-4ed0-9140-8533fe17447f","valueLabel":"Allegro"}],"ref":"https://api.smartrecruiters.com/v1/companies/allegro/postings/743999737101238","creator":{"name":"Paulina Partyka"},"language":{"code":"pl","label":"Polish","labelNative":"polski"}}],"events":[{"created":1614684480000,"duration":7200000,"id":"276687714","name":"#15 Allegro Tech Live - Jak technicznie dbać o produkt?","date_in_series_pattern":false,"status":"upcoming","time":1616086800000,"local_date":"2021-03-18","local_time":"18:00","updated":1614684512000,"utc_offset":3600000,"waitlist_count":0,"yes_rsvp_count":38,"venue":{"id":26906060,"name":"Online event","repinned":false,"country":"","localized_country_name":""},"is_online_event":true,"group":{"created":1425052059000,"name":"allegro Tech","id":18465254,"join_mode":"open","lat":52.2599983215332,"lon":21.020000457763672,"urlname":"allegrotech","who":"Techs","localized_location":"Warsaw, Poland","state":"","country":"pl","region":"en_US","timezone":"Europe/Warsaw"},"link":"https://www.meetup.com/allegrotech/events/276687714/","description":"Allegro Tech Live to nowa, w 100% zdalna odsłona naszych stacjonarnych meetupów Allegro Tech Talks. Zazwyczaj spotykaliśmy się w naszych biurach, ale tym razem to…","how_to_find_us":"https://www.youtube.com/channel/UC66wC6RBjFk6CuVz7wdjJ5g","visibility":"public","member_pay_fee":false},{"created":1613735379000,"duration":97200000,"id":"276458683","name":"Allegro Tech Labs #6 Online: Poznać Reacta - poziom podstawowy","date_in_series_pattern":false,"status":"upcoming","time":1615392000000,"local_date":"2021-03-10","local_time":"17:00","updated":1613735420000,"utc_offset":3600000,"waitlist_count":0,"yes_rsvp_count":46,"venue":{"id":26906060,"name":"Online event","repinned":false,"country":"","localized_country_name":""},"is_online_event":true,"group":{"created":1425052059000,"name":"allegro Tech","id":18465254,"join_mode":"open","lat":52.2599983215332,"lon":21.020000457763672,"urlname":"allegrotech","who":"Techs","localized_location":"Warsaw, Poland","state":"","country":"pl","region":"en_US","timezone":"Europe/Warsaw"},"link":"https://www.meetup.com/allegrotech/events/276458683/","description":"***REJESTRACJA***Prosimy o rejestrację poprzez https://app.evenea.pl/event/allegro-tech-labs-6/ Po zarejestrowaniu otrzymasz e-mail z potwierdzeniem rejestracji, Twoim biletem oraz linkiem do warsztatów online. ***SZCZEGÓŁY***Allegro Tech Labs #6 to idealna…","visibility":"public","member_pay_fee":false},{"created":1613559191000,"duration":5400000,"id":"276415681","name":"Allegro Tech Live #14 - Zwinne przywództwo w czasach zmiany","date_in_series_pattern":false,"status":"past","time":1614272400000,"local_date":"2021-02-25","local_time":"18:00","updated":1614281669000,"utc_offset":3600000,"waitlist_count":0,"yes_rsvp_count":139,"venue":{"id":26906060,"name":"Online event","repinned":false,"country":"","localized_country_name":""},"is_online_event":true,"group":{"created":1425052059000,"name":"allegro Tech","id":18465254,"join_mode":"open","lat":52.2599983215332,"lon":21.020000457763672,"urlname":"allegrotech","who":"Techs","localized_location":"Warsaw, Poland","state":"","country":"pl","region":"en_US","timezone":"Europe/Warsaw"},"link":"https://www.meetup.com/allegrotech/events/276415681/","description":"Allegro Tech Live to nowa (w 100% zdalna) odsłona naszych stacjonarnych meetupów Allegro Tech Talks. Zazwyczaj spotykaliśmy się w naszych biurach, ale tym razem to…","how_to_find_us":"https://youtu.be/yX-hZlsPEHM","visibility":"public","member_pay_fee":false},{"created":1596114124000,"duration":5400000,"id":"272249143","name":"Allegro Tech Talks #13 - Cyfrodziewczyny","date_in_series_pattern":false,"status":"past","time":1596643200000,"local_date":"2020-08-05","local_time":"18:00","updated":1596650397000,"utc_offset":7200000,"waitlist_count":0,"yes_rsvp_count":40,"venue":{"id":26906060,"name":"Online event","repinned":false,"country":"","localized_country_name":""},"is_online_event":true,"group":{"created":1425052059000,"name":"allegro Tech","id":18465254,"join_mode":"open","lat":52.2599983215332,"lon":21.020000457763672,"urlname":"allegrotech","who":"Techs","localized_location":"Warsaw, Poland","state":"","country":"pl","region":"en_US","timezone":"Europe/Warsaw"},"link":"https://www.meetup.com/allegrotech/events/272249143/","description":"Allegro Tech Live to nowa (w 100% zdalna) odsłona naszych stacjonarnych meetupów Allegro Tech Talks. Zazwyczaj spotykaliśmy się w naszych biurach, ale tym razem to…","how_to_find_us":"https://www.facebook.com/allegro.tech/","visibility":"public","member_pay_fee":false}],"podcasts":[{"creator":{"name":["Dariusz Jędrzejczyk"]},"title":"Service Mesh oraz praca programisty w Allegro","link":"https://podcast.allegro.tech/service_mesh_oraz_praca_programisty_w_allegro","pubDate":"Thu, 25 Feb 2021 00:00:00 GMT","author":{"name":["Dariusz Jędrzejczyk"]},"enclosure":{"url":"https://www.buzzsprout.com/887914/7917649-sezon-ii-2-service-mesh-oraz-praca-praca-programisty-w-allegro-dariusz-jedrzejczak.mp3","type":"audio/mpeg"},"content":"Czym jest Service Mesh? Jak został wdrożony do Allegro dla ponad 1000 usług na produkcji? Dlaczego usprawnianie pracy programistom jest dla nas ważne i dlaczego programista to klient? Jak wytwarzamy produkty używane przez zespoły deweloperskie? Kim jest Principal w Allegro? Jak może wyglądać ścieżka kariery programisty? Na te pytania odpowie Dariusz Jędrzejczyk - Principal Software Engineer.","contentSnippet":"Czym jest Service Mesh? Jak został wdrożony do Allegro dla ponad 1000 usług na produkcji? Dlaczego usprawnianie pracy programistom jest dla nas ważne i dlaczego programista to klient? Jak wytwarzamy produkty używane przez zespoły deweloperskie? Kim jest Principal w Allegro? Jak może wyglądać ścieżka kariery programisty? Na te pytania odpowie Dariusz Jędrzejczyk - Principal Software Engineer.","guid":"https://podcast.allegro.tech/service_mesh_oraz_praca_programisty_w_allegro","isoDate":"2021-02-25T00:00:00.000Z","itunes":{"author":"Dariusz Jędrzejczyk","summary":"Czym jest Service Mesh? Jak został wdrożony do Allegro dla ponad 1000 usług na produkcji? Dlaczego usprawnianie pracy programistom jest dla nas ważne i dlaczego programista to klient? Jak wytwarzamy produkty używane przez zespoły deweloperskie? Kim jest Principal w Allegro? Jak może wyglądać ścieżka kariery programisty? Na te pytania odpowie Dariusz Jędrzejczyk - Principal Software Engineer.","explicit":"false"}},{"creator":{"name":["Łukasz Ściga"]},"title":"Zbiory danych w Allegro","link":"https://podcast.allegro.tech/zbiory_danych_w_allegro","pubDate":"Wed, 10 Feb 2021 00:00:00 GMT","author":{"name":["Łukasz Ściga"]},"enclosure":{"url":"https://www.buzzsprout.com/887914/7426294-sezon-ii-1-zbiory-danych-w-allegro-lukasz-scigak.mp3","type":"audio/mpeg"},"content":"Czym zajmuje się inżynier danych? Jak powstało Big Data? Jak wykorzystuje się oraz kto korzysta z nowych zbiorów danych? Czym różni się Data Scientist od Data Engineera? Jak połączyć przetwarzanie streamingowe z przetwarzaniem batchowym? Z jakich narzędzi korzysta Data Engineer? Na te pytania odpowie Łukasz Ściga - Data Engineer w Allegro, którego pasją jest programowanie gier komputerowych.","contentSnippet":"Czym zajmuje się inżynier danych? Jak powstało Big Data? Jak wykorzystuje się oraz kto korzysta z nowych zbiorów danych? Czym różni się Data Scientist od Data Engineera? Jak połączyć przetwarzanie streamingowe z przetwarzaniem batchowym? Z jakich narzędzi korzysta Data Engineer? Na te pytania odpowie Łukasz Ściga - Data Engineer w Allegro, którego pasją jest programowanie gier komputerowych.","guid":"https://podcast.allegro.tech/zbiory_danych_w_allegro","isoDate":"2021-02-10T00:00:00.000Z","itunes":{"author":"Łukasz Ściga","summary":"Czym zajmuje się inżynier danych? Jak powstało Big Data? Jak wykorzystuje się oraz kto korzysta z nowych zbiorów danych? Czym różni się Data Scientist od Data Engineera? Jak połączyć przetwarzanie streamingowe z przetwarzaniem batchowym? Z jakich narzędzi korzysta Data Engineer? Na te pytania odpowie Łukasz Ściga - Data Engineer w Allegro, którego pasją jest programowanie gier komputerowych.","explicit":"false"}},{"creator":{"name":["Ireneusz Gawlik"]},"title":"Badania i rozwój ML w Allegro","link":"https://podcast.allegro.tech/badania_i_rozwoj_ml_w_allegro","pubDate":"Mon, 29 Jun 2020 00:00:00 GMT","author":{"name":["Ireneusz Gawlik"]},"enclosure":{"url":"https://www.buzzsprout.com/887914/2998819-13-badania-i-rozwoj-ml-w-allegro-ireneusz-gawlik.mp3","type":"audio/mpeg"},"content":"Gdzie kryje się Machine Learning w Allegro? Jakie projekty już dzisiaj korzystają z mocy sztucznej inteligencji? Jak na codzień pracuje grupa badaczy nie tylko aplikująca, ale też rozwijająca algorytmy uczenia maszynowego? Irek, Team Manager w grupie Allegro ML Research, opowiada więcej o tym jak się tworzy AI w Polsce.","contentSnippet":"Gdzie kryje się Machine Learning w Allegro? Jakie projekty już dzisiaj korzystają z mocy sztucznej inteligencji? Jak na codzień pracuje grupa badaczy nie tylko aplikująca, ale też rozwijająca algorytmy uczenia maszynowego? Irek, Team Manager w grupie Allegro ML Research, opowiada więcej o tym jak się tworzy AI w Polsce.","guid":"https://podcast.allegro.tech/badania_i_rozwoj_ml_w_allegro","isoDate":"2020-06-29T00:00:00.000Z","itunes":{"author":"Ireneusz Gawlik","summary":"Gdzie kryje się Machine Learning w Allegro? Jakie projekty już dzisiaj korzystają z mocy sztucznej inteligencji? Jak na codzień pracuje grupa badaczy nie tylko aplikująca, ale też rozwijająca algorytmy uczenia maszynowego? Irek, Team Manager w grupie Allegro ML Research, opowiada więcej o tym jak się tworzy AI w Polsce.","explicit":"false"}},{"creator":{"name":["Kasia Wróbel"]},"title":"Projektowanie UX w Allegro, czyli wszystko zależy od doświadczenia","link":"https://podcast.allegro.tech/projektowanie_ux_w_allegro","pubDate":"Mon, 15 Jun 2020 00:00:00 GMT","author":{"name":["Kasia Wróbel"]},"enclosure":{"url":"https://www.buzzsprout.com/887914/3011998-11-projektowanie-ux-w-allegro-czyli-wszystko-zalezy-od-doswiadczenia-katarzyna-wrobel.mp3","type":"audio/mpeg"},"content":"Projektowanie na styku produktu, biznesu i potrzeb użytkownika. Budzenie ciekawości, czyli czy jesteś w stanie zmienić coś, co sprawia problem tak wielu osobom na raz? Kasia opowiada o \"układaniu klocków\", których użytkownik wie jak użyć.","contentSnippet":"Projektowanie na styku produktu, biznesu i potrzeb użytkownika. Budzenie ciekawości, czyli czy jesteś w stanie zmienić coś, co sprawia problem tak wielu osobom na raz? Kasia opowiada o \"układaniu klocków\", których użytkownik wie jak użyć.","guid":"https://podcast.allegro.tech/projektowanie_ux_w_allegro","isoDate":"2020-06-15T00:00:00.000Z","itunes":{"author":"Kasia Wróbel","summary":"Projektowanie na styku produktu, biznesu i potrzeb użytkownika. Budzenie ciekawości, czyli czy jesteś w stanie zmienić coś, co sprawia problem tak wielu osobom na raz? Kasia opowiada o \"układaniu klocków\", których użytkownik wie jak użyć.","explicit":"false"}}]},"__N_SSG":true}