{"pageProps":{"posts":[{"title":"How to make context logging in Python less cumbersome","link":"https://blog.allegro.tech/2021/06/python-logging.html","pubDate":"Thu, 17 Jun 2021 00:00:00 +0200","authors":{"author":[{"name":["Łukasz Mach"],"photo":["https://blog.allegro.tech/img/authors/lukasz.mach.jpg"],"url":["https://blog.allegro.tech/authors/lukasz.mach"]}]},"content":"<p>This post is about the reasons behind writing a small (yet practical) library that has just been released as open-source:  <a href=\"https://github.com/allegro/logextractx\">LogExtraCtx</a></p>\n\n<h2 id=\"why-did-i-write-this-library\">Why did I write this library?</h2>\n\n<p>I’m a big fan of logging. I like to log as much extra data as possible, and I’m\nfond of <code class=\"language-plaintext highlighter-rouge\">logging.debug</code> entries.</p>\n\n<p>I’m also a DRY approach believer. I feel strong anxiety when I see repetition in code.\nAnd my mind literally hangs when I need to do <code class=\"language-plaintext highlighter-rouge\">Ctrl-C/V</code>, even if it’s justified by\ncircumstances. In such cases I start to focus on getting rid of copy-pastes instead of\nwriting new code.</p>\n\n<p>Combining all these “passions” is not always easy. It’s hard to log everything without\nrepeating things. Even if it’s possible, it usually leads to inelegant code.</p>\n\n<p>For example, if you want to log that some error occured:</p>\n\n<div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"n\">logger</span><span class=\"p\">.</span><span class=\"n\">error</span><span class=\"p\">(</span><span class=\"s\">'Foo happened: %s'</span><span class=\"p\">,</span> <span class=\"n\">e</span><span class=\"p\">)</span>\n</code></pre></div></div>\n\n<p>Then so far it’s clean and easy.</p>\n\n<p>It would be nice to add some extra details though, like <code class=\"language-plaintext highlighter-rouge\">user</code>\n(then you could search by <code class=\"language-plaintext highlighter-rouge\">user</code> in <a href=\"https://www.elastic.co/kibana\">Kibana</a>, if your logs goes there):</p>\n\n<div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"n\">logger</span><span class=\"p\">.</span><span class=\"n\">error</span><span class=\"p\">(</span><span class=\"s\">'Foo happened: %s'</span><span class=\"p\">,</span> <span class=\"n\">e</span><span class=\"p\">,</span> <span class=\"n\">extra</span><span class=\"o\">=</span><span class=\"p\">{</span><span class=\"s\">'user'</span><span class=\"p\">:</span> <span class=\"n\">user</span><span class=\"p\">,</span>\n                                           <span class=\"s\">'action_type'</span><span class=\"p\">:</span> <span class=\"s\">'bar'</span><span class=\"p\">})</span>\n</code></pre></div></div>\n\n<p>Next, during debugging of a problem, you will notice that it’s not nearly enough and it’s worth\nadding some <code class=\"language-plaintext highlighter-rouge\">logging.debug</code></p>\n\n<div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"n\">logger</span><span class=\"p\">.</span><span class=\"n\">debug</span><span class=\"p\">(</span><span class=\"s\">\"We're going to do SOMETHING in thread\"</span><span class=\"p\">,</span>\n             <span class=\"n\">extra</span><span class=\"o\">=</span><span class=\"p\">{</span><span class=\"s\">'user'</span><span class=\"p\">:</span> <span class=\"n\">user</span><span class=\"p\">,</span>\n                    <span class=\"s\">'action_type'</span><span class=\"p\">:</span> <span class=\"s\">'bar'</span><span class=\"p\">,</span>\n                    <span class=\"s\">'thread_num'</span><span class=\"p\">:</span> <span class=\"n\">thread_num</span><span class=\"p\">})</span>\n</code></pre></div></div>\n\n<p>After that you will notice that your code has <code class=\"language-plaintext highlighter-rouge\">extra=</code> with duplicated <code class=\"language-plaintext highlighter-rouge\">user</code> and <code class=\"language-plaintext highlighter-rouge\">action_type</code>.\nIt’s a Bad Thing! Imagine what would happen if there was another <code class=\"language-plaintext highlighter-rouge\">logger.debug</code>, and another?\nLots of repeated code that should be written only once…</p>\n\n<p><img src=\"https://i.imgflip.com/54peqd.jpg\" alt=\"I see copypastes in code\" /></p>\n\n<p>So in other words, the more details you log, the more cumbersome the code. What can we\ndo?</p>\n\n<h3 id=\"method-1-dont-log-extra\">Method 1: don’t log <code class=\"language-plaintext highlighter-rouge\">extra</code></h3>\n\n<p>It’s not what I like. I <strong>do</strong> want to log them!</p>\n\n<h3 id=\"method-2-store-extra-in-a-variable\">Method 2: store <code class=\"language-plaintext highlighter-rouge\">extra</code> in a variable</h3>\n\n<p>It’s what I used to do sometimes, before <a href=\"https://github.com/allegro/logextractx\">LogExtraCtx</a>. It could go like that:</p>\n\n<div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"n\">extra</span> <span class=\"o\">=</span> <span class=\"p\">{</span><span class=\"s\">'user'</span><span class=\"p\">:</span> <span class=\"n\">user</span><span class=\"p\">,</span> <span class=\"s\">'action_type'</span><span class=\"p\">:</span> <span class=\"s\">'bar'</span><span class=\"p\">}</span>\n\n<span class=\"n\">logger</span><span class=\"p\">.</span><span class=\"n\">debug</span><span class=\"p\">(</span><span class=\"s\">\"We're going to do SOMETHING in thread\"</span><span class=\"p\">,</span>\n             <span class=\"n\">extra</span><span class=\"o\">=</span><span class=\"nb\">dict</span><span class=\"p\">(</span><span class=\"n\">thread_num</span><span class=\"o\">=</span><span class=\"n\">thread_num</span><span class=\"p\">,</span> <span class=\"o\">**</span><span class=\"n\">extra</span><span class=\"p\">))</span>\n\n<span class=\"n\">logger</span><span class=\"p\">.</span><span class=\"n\">error</span><span class=\"p\">(</span><span class=\"s\">'Foo happened: %s'</span><span class=\"p\">,</span> <span class=\"n\">e</span><span class=\"p\">,</span> <span class=\"n\">extra</span><span class=\"o\">=</span><span class=\"n\">extra</span><span class=\"p\">)</span>\n</code></pre></div></div>\n\n<p>Quite tricky and not very elegant.</p>\n\n<h3 id=\"method-3-use-logextractx\">Method 3: use <a href=\"https://github.com/allegro/logextractx\">LogExtraCtx</a></h3>\n\n<p>Before I describe the aforementioned library, let me show you a more realistic example.</p>\n\n<p>Consider the following code:</p>\n\n<div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"n\">logger</span> <span class=\"o\">=</span> <span class=\"n\">logging</span><span class=\"p\">.</span><span class=\"n\">getLogger</span><span class=\"p\">(</span><span class=\"n\">__name__</span><span class=\"p\">)</span>\n\n\n<span class=\"k\">def</span> <span class=\"nf\">send_message</span><span class=\"p\">(</span><span class=\"n\">requester</span><span class=\"p\">:</span> <span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">recipient</span><span class=\"p\">:</span> <span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">text</span><span class=\"p\">:</span> <span class=\"nb\">str</span><span class=\"p\">)</span> <span class=\"o\">-&gt;</span> <span class=\"nb\">bool</span><span class=\"p\">:</span>\n    <span class=\"s\">\"\"\" Function send_message sends message to the specified recipient.  \"\"\"</span>\n    <span class=\"k\">try</span><span class=\"p\">:</span>\n        <span class=\"n\">r</span> <span class=\"o\">=</span> <span class=\"n\">requests</span><span class=\"p\">.</span><span class=\"n\">post</span><span class=\"p\">(</span><span class=\"n\">settings</span><span class=\"p\">.</span><span class=\"n\">MSG_PROVIDER</span><span class=\"p\">,</span> <span class=\"n\">json</span><span class=\"o\">=</span><span class=\"p\">{</span><span class=\"s\">'recipient'</span><span class=\"p\">:</span> <span class=\"n\">recipient</span><span class=\"p\">,</span> <span class=\"s\">'content'</span><span class=\"p\">:</span> <span class=\"n\">text</span><span class=\"p\">},</span>\n                          <span class=\"p\">...</span> <span class=\"o\">&lt;</span> <span class=\"n\">other</span> <span class=\"n\">params</span> <span class=\"o\">&gt;</span> <span class=\"p\">....)</span>\n        <span class=\"n\">r</span><span class=\"p\">.</span><span class=\"n\">raise_for_status</span><span class=\"p\">()</span>\n    <span class=\"k\">except</span> <span class=\"n\">requests</span><span class=\"p\">.</span><span class=\"n\">exceptions</span><span class=\"p\">.</span><span class=\"n\">RequestException</span> <span class=\"k\">as</span> <span class=\"n\">e</span><span class=\"p\">:</span>\n        <span class=\"n\">logger</span><span class=\"p\">.</span><span class=\"n\">error</span><span class=\"p\">(</span><span class=\"s\">'Sending message failed. Response text: \"%s\"'</span><span class=\"p\">,</span> <span class=\"n\">e</span><span class=\"p\">,</span>\n                     <span class=\"n\">extra</span><span class=\"o\">=</span><span class=\"p\">{</span>  <span class=\"c1\"># extra data to be logged and indexed by Kibana\n</span>                         <span class=\"s\">'ACTION_TYPE'</span><span class=\"p\">:</span> <span class=\"s\">'SEND_MSG'</span><span class=\"p\">,</span>\n                         <span class=\"s\">'requester'</span><span class=\"p\">:</span> <span class=\"n\">requester</span><span class=\"p\">,</span>\n                         <span class=\"s\">'recipient'</span><span class=\"p\">:</span> <span class=\"n\">recipient</span><span class=\"p\">,</span>\n                     <span class=\"p\">})</span>\n        <span class=\"k\">return</span> <span class=\"bp\">False</span>\n    <span class=\"n\">logger</span><span class=\"p\">.</span><span class=\"n\">info</span><span class=\"p\">(</span><span class=\"s\">'Sending MSG success.'</span><span class=\"p\">,</span>\n                <span class=\"n\">extra</span><span class=\"o\">=</span><span class=\"p\">{</span>  <span class=\"c1\"># the same extra data to be logged/indexed\n</span>                    <span class=\"s\">'ACTION_TYPE'</span><span class=\"p\">:</span> <span class=\"s\">'SEND_MSG'</span><span class=\"p\">,</span>\n                    <span class=\"s\">'requester'</span><span class=\"p\">:</span> <span class=\"n\">requester</span><span class=\"p\">,</span>\n                    <span class=\"s\">'recipient'</span><span class=\"p\">:</span> <span class=\"n\">recipient</span><span class=\"p\">,</span>\n                <span class=\"p\">})</span>\n    <span class=\"k\">return</span> <span class=\"bp\">True</span>\n</code></pre></div></div>\n\n<p>There are two log entries, both with extra data for easy-finding in Kibana (there is a bit\nof redundancy in the code, so I already feel the desire to DRY it).</p>\n\n<p>Then I need to add additional logging:</p>\n\n<div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"n\">logger</span><span class=\"p\">.</span><span class=\"n\">debug</span><span class=\"p\">(</span><span class=\"s\">\"headers=%r\"</span><span class=\"p\">,</span> <span class=\"n\">r</span><span class=\"p\">.</span><span class=\"n\">result</span><span class=\"p\">.</span><span class=\"n\">headers</span><span class=\"p\">,</span>\n             <span class=\"n\">extra</span><span class=\"o\">=</span><span class=\"p\">{</span><span class=\"s\">'ACTION_TYPE'</span><span class=\"p\">:</span> <span class=\"s\">'SEND_MSG'</span><span class=\"p\">,</span>\n                    <span class=\"s\">'requester'</span><span class=\"p\">:</span> <span class=\"n\">requester</span><span class=\"p\">,</span>\n                    <span class=\"s\">'recipient'</span><span class=\"p\">:</span> <span class=\"n\">recipient</span>\n                    <span class=\"p\">})</span>\n</code></pre></div></div>\n\n<p>And then, I need even more context in every log entry:</p>\n\n<div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"n\">extra</span> <span class=\"o\">=</span> <span class=\"p\">{</span><span class=\"s\">'ACTION_TYPE'</span><span class=\"p\">:</span> <span class=\"s\">'SEND_MSG'</span><span class=\"p\">,</span>\n         <span class=\"s\">'requester'</span><span class=\"p\">:</span> <span class=\"n\">requester</span><span class=\"p\">,</span>\n         <span class=\"s\">'recipient'</span><span class=\"p\">:</span> <span class=\"n\">recipient</span><span class=\"p\">,</span>\n         <span class=\"s\">'user'</span><span class=\"p\">:</span> <span class=\"nb\">str</span><span class=\"p\">(</span><span class=\"n\">request</span><span class=\"p\">.</span><span class=\"n\">user</span><span class=\"p\">),</span>\n         <span class=\"s\">'environment'</span><span class=\"p\">:</span> <span class=\"n\">env_type</span><span class=\"p\">,</span>\n         <span class=\"p\">}</span>\n</code></pre></div></div>\n\n<p>Combined all together, it becomes a big, unreadable blob of code. A very simple piece of logic has been\nspoiled by 3 log entries.</p>\n\n<div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"n\">logger</span> <span class=\"o\">=</span> <span class=\"n\">logging</span><span class=\"p\">.</span><span class=\"n\">getLogger</span><span class=\"p\">(</span><span class=\"n\">__name__</span><span class=\"p\">)</span>\n\n\n<span class=\"k\">def</span> <span class=\"nf\">send_message</span><span class=\"p\">(</span><span class=\"n\">environment</span><span class=\"p\">:</span> <span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">requester</span><span class=\"p\">:</span> <span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">recipient</span><span class=\"p\">:</span> <span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">text</span><span class=\"p\">:</span> <span class=\"nb\">str</span><span class=\"p\">)</span> <span class=\"o\">-&gt;</span> <span class=\"nb\">bool</span><span class=\"p\">:</span>\n    <span class=\"s\">\"\"\" Function send_message sends MSG to the specified recipient.  \"\"\"</span>\n    <span class=\"k\">try</span><span class=\"p\">:</span>\n        <span class=\"n\">r</span> <span class=\"o\">=</span> <span class=\"n\">requests</span><span class=\"p\">.</span><span class=\"n\">post</span><span class=\"p\">(</span><span class=\"n\">settings</span><span class=\"p\">.</span><span class=\"n\">MSG_PROVIDER</span><span class=\"p\">,</span> <span class=\"n\">json</span><span class=\"o\">=</span><span class=\"p\">{</span><span class=\"s\">'recipient'</span><span class=\"p\">:</span> <span class=\"n\">recipient</span><span class=\"p\">,</span> <span class=\"s\">'content'</span><span class=\"p\">:</span> <span class=\"n\">text</span><span class=\"p\">},</span>\n                          <span class=\"p\">...</span> <span class=\"o\">&lt;</span> <span class=\"n\">other</span> <span class=\"n\">params</span> <span class=\"o\">&gt;</span> <span class=\"p\">....)</span>\n        <span class=\"n\">r</span><span class=\"p\">.</span><span class=\"n\">raise_for_status</span><span class=\"p\">()</span>\n    <span class=\"k\">except</span> <span class=\"n\">requests</span><span class=\"p\">.</span><span class=\"n\">exceptions</span><span class=\"p\">.</span><span class=\"n\">RequestException</span> <span class=\"k\">as</span> <span class=\"n\">e</span><span class=\"p\">:</span>\n        <span class=\"n\">logger</span><span class=\"p\">.</span><span class=\"n\">error</span><span class=\"p\">(</span><span class=\"s\">'Sending MSG failed. Response text: \"%s\"'</span><span class=\"p\">,</span> <span class=\"n\">e</span><span class=\"p\">,</span>\n                     <span class=\"n\">extra</span><span class=\"o\">=</span><span class=\"p\">{</span>   <span class=\"c1\"># extra data to be logged and indexed by Kibana\n</span>                         <span class=\"s\">'ACTION_TYPE'</span><span class=\"p\">:</span> <span class=\"s\">'SEND_MSG'</span><span class=\"p\">,</span>\n                         <span class=\"s\">'requester'</span><span class=\"p\">:</span> <span class=\"n\">requester</span><span class=\"p\">,</span>\n                         <span class=\"s\">'recipient'</span><span class=\"p\">:</span> <span class=\"n\">recipient</span><span class=\"p\">,</span>\n                         <span class=\"s\">'user'</span><span class=\"p\">:</span> <span class=\"nb\">str</span><span class=\"p\">(</span><span class=\"n\">request</span><span class=\"p\">.</span><span class=\"n\">user</span><span class=\"p\">),</span>\n                         <span class=\"s\">'environment'</span><span class=\"p\">:</span> <span class=\"n\">env_type</span><span class=\"p\">,</span>\n                     <span class=\"p\">})</span>\n        <span class=\"n\">logger</span><span class=\"p\">.</span><span class=\"n\">debug</span><span class=\"p\">(</span><span class=\"s\">\"headers=%r\"</span><span class=\"p\">,</span> <span class=\"n\">r</span><span class=\"p\">.</span><span class=\"n\">result</span><span class=\"p\">.</span><span class=\"n\">headers</span><span class=\"p\">,</span>\n                     <span class=\"n\">extra</span><span class=\"o\">=</span><span class=\"p\">{</span><span class=\"s\">'ACTION_TYPE'</span><span class=\"p\">:</span> <span class=\"s\">'SEND_MSG'</span><span class=\"p\">,</span>\n                            <span class=\"s\">'requester'</span><span class=\"p\">:</span> <span class=\"n\">requester</span><span class=\"p\">,</span>\n                            <span class=\"s\">'recipient'</span><span class=\"p\">:</span> <span class=\"n\">recipient</span><span class=\"p\">,</span>\n                            <span class=\"s\">'user'</span><span class=\"p\">:</span> <span class=\"nb\">str</span><span class=\"p\">(</span><span class=\"n\">request</span><span class=\"p\">.</span><span class=\"n\">user</span><span class=\"p\">),</span>\n                            <span class=\"s\">'environment'</span><span class=\"p\">:</span> <span class=\"n\">env_type</span><span class=\"p\">,</span>\n                            <span class=\"p\">})</span>\n        <span class=\"k\">return</span> <span class=\"bp\">False</span>\n    <span class=\"n\">logger</span><span class=\"p\">.</span><span class=\"n\">info</span><span class=\"p\">(</span><span class=\"s\">'Sending MSG success.'</span><span class=\"p\">,</span>\n                <span class=\"n\">extra</span><span class=\"o\">=</span><span class=\"p\">{</span>  <span class=\"c1\"># the same extra data to be logged/indexed\n</span>                    <span class=\"s\">'ACTION_TYPE'</span><span class=\"p\">:</span> <span class=\"s\">'SEND_MSG'</span><span class=\"p\">,</span>\n                    <span class=\"s\">'requester'</span><span class=\"p\">:</span> <span class=\"n\">requester</span><span class=\"p\">,</span>\n                    <span class=\"s\">'recipient'</span><span class=\"p\">:</span> <span class=\"n\">recipient</span><span class=\"p\">,</span>\n                    <span class=\"s\">'user'</span><span class=\"p\">:</span> <span class=\"nb\">str</span><span class=\"p\">(</span><span class=\"n\">request</span><span class=\"p\">.</span><span class=\"n\">user</span><span class=\"p\">),</span>\n                    <span class=\"s\">'environment'</span><span class=\"p\">:</span> <span class=\"n\">env_type</span><span class=\"p\">,</span>\n                <span class=\"p\">})</span>\n    <span class=\"k\">return</span> <span class=\"bp\">True</span>\n</code></pre></div></div>\n\n<h4 id=\"my-solution--logextractx\">My solution — <em>LogExtraCtx</em>:</h4>\n\n<p>In order to use it, just replace <code class=\"language-plaintext highlighter-rouge\">logging.getLogger</code> with <code class=\"language-plaintext highlighter-rouge\">getLogger</code> from <code class=\"language-plaintext highlighter-rouge\">logextractx.logger</code>,\nand then create local logger with local context:</p>\n\n<div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"kn\">from</span> <span class=\"nn\">logextractx.logger</span> <span class=\"kn\">import</span> <span class=\"n\">getLogger</span>\n<span class=\"n\">logger</span> <span class=\"o\">=</span> <span class=\"n\">getLogger</span><span class=\"p\">(</span><span class=\"n\">__name__</span><span class=\"p\">)</span>\n<span class=\"p\">[...]</span>\n<span class=\"n\">loclogger</span> <span class=\"o\">=</span> <span class=\"n\">logger</span><span class=\"p\">.</span><span class=\"n\">local</span><span class=\"p\">(</span><span class=\"n\">extra</span><span class=\"o\">=</span><span class=\"p\">{</span><span class=\"s\">'DATA_IN'</span><span class=\"p\">:</span> <span class=\"s\">'CURRENT_CONTEXT'</span><span class=\"p\">})</span>\n</code></pre></div></div>\n\n<p>so the previous example is reduced to the following clean code:</p>\n\n<div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"kn\">from</span> <span class=\"nn\">logextractx.logger</span> <span class=\"kn\">import</span> <span class=\"n\">getLogger</span>\n<span class=\"n\">logger</span> <span class=\"o\">=</span> <span class=\"n\">getLogger</span><span class=\"p\">(</span><span class=\"n\">__name__</span><span class=\"p\">)</span>\n\n<span class=\"k\">def</span> <span class=\"nf\">send_message</span><span class=\"p\">(</span><span class=\"n\">environment</span><span class=\"p\">:</span> <span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">requester</span><span class=\"p\">:</span> <span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">recipient</span><span class=\"p\">:</span> <span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">text</span><span class=\"p\">:</span> <span class=\"nb\">str</span><span class=\"p\">)</span> <span class=\"o\">-&gt;</span> <span class=\"nb\">bool</span><span class=\"p\">:</span>\n    <span class=\"s\">\"\"\" Function send_message sends MSG to the specified recipient.  \"\"\"</span>\n\n    <span class=\"c1\"># extra data to be logged/indexed\n</span>    <span class=\"n\">loclogger</span> <span class=\"o\">=</span> <span class=\"n\">logger</span><span class=\"p\">.</span><span class=\"n\">local</span><span class=\"p\">(</span><span class=\"n\">extra</span><span class=\"o\">=</span><span class=\"p\">{</span><span class=\"s\">'ACTION_TYPE'</span><span class=\"p\">:</span> <span class=\"s\">'SEND_MSG'</span><span class=\"p\">,</span>\n                                    <span class=\"s\">'requester'</span><span class=\"p\">:</span> <span class=\"n\">requester</span><span class=\"p\">,</span>\n                                    <span class=\"s\">'recipient'</span><span class=\"p\">:</span> <span class=\"n\">recipient</span><span class=\"p\">,</span>\n                                    <span class=\"s\">'user'</span><span class=\"p\">:</span> <span class=\"nb\">str</span><span class=\"p\">(</span><span class=\"n\">request</span><span class=\"p\">.</span><span class=\"n\">user</span><span class=\"p\">),</span>\n                                    <span class=\"s\">'environment'</span><span class=\"p\">:</span> <span class=\"n\">env_type</span><span class=\"p\">})</span>\n\n    <span class=\"k\">try</span><span class=\"p\">:</span>\n        <span class=\"n\">r</span> <span class=\"o\">=</span> <span class=\"n\">requests</span><span class=\"p\">.</span><span class=\"n\">post</span><span class=\"p\">(</span><span class=\"n\">settings</span><span class=\"p\">.</span><span class=\"n\">MSG_PROVIDER</span><span class=\"p\">,</span> <span class=\"n\">json</span><span class=\"o\">=</span><span class=\"p\">{</span><span class=\"s\">'recipient'</span><span class=\"p\">:</span> <span class=\"n\">recipient</span><span class=\"p\">,</span> <span class=\"s\">'content'</span><span class=\"p\">:</span> <span class=\"n\">text</span><span class=\"p\">},</span>\n                          <span class=\"p\">...</span> <span class=\"o\">&lt;</span> <span class=\"n\">other</span> <span class=\"n\">params</span> <span class=\"o\">&gt;</span> <span class=\"p\">....)</span>\n        <span class=\"n\">r</span><span class=\"p\">.</span><span class=\"n\">raise_for_status</span><span class=\"p\">()</span>\n    <span class=\"k\">except</span> <span class=\"n\">requests</span><span class=\"p\">.</span><span class=\"n\">exceptions</span><span class=\"p\">.</span><span class=\"n\">RequestException</span> <span class=\"k\">as</span> <span class=\"n\">e</span><span class=\"p\">:</span>\n        <span class=\"n\">loclogger</span><span class=\"p\">.</span><span class=\"n\">error</span><span class=\"p\">(</span><span class=\"s\">'Sending MSG failed. Response text: \"%s\"'</span><span class=\"p\">,</span> <span class=\"n\">e</span><span class=\"p\">)</span>\n        <span class=\"n\">loclogger</span><span class=\"p\">.</span><span class=\"n\">debug</span><span class=\"p\">(</span><span class=\"s\">\"headers=%r\"</span><span class=\"p\">,</span> <span class=\"n\">r</span><span class=\"p\">.</span><span class=\"n\">result</span><span class=\"p\">.</span><span class=\"n\">headers</span><span class=\"p\">)</span>\n        <span class=\"k\">return</span> <span class=\"bp\">False</span>\n    <span class=\"n\">loclogger</span><span class=\"p\">.</span><span class=\"n\">info</span><span class=\"p\">(</span><span class=\"s\">'Sending MSG success.'</span><span class=\"p\">)</span>\n    <span class=\"k\">return</span> <span class=\"bp\">True</span>\n</code></pre></div></div>\n\n<h2 id=\"interesting-and-useful-side-effect\">Interesting and useful “side effect”</h2>\n\n<p>Usually, it’s hard to distinguish log entries from various users. For example when you have an error in\nyour code and you find <code class=\"language-plaintext highlighter-rouge\">IndexError</code>, you cannot be <strong>really sure</strong> to which request it belongs.</p>\n\n<p>Of course, you can guess, based on chronology and many other symptoms,\nbut if you have many concurrent requests, then it’s hard or even impossible to associate <code class=\"language-plaintext highlighter-rouge\">ERROR</code> log\nwith previous <code class=\"language-plaintext highlighter-rouge\">INFO</code> or <code class=\"language-plaintext highlighter-rouge\">DEBUG</code>.</p>\n\n<p>So it’s nice to have some kind of tracking ID (<code class=\"language-plaintext highlighter-rouge\">request-id</code>), that sticks to the request,\nfollows it and is added to every log entry until the end of request processing. It’s also worth having\n<code class=\"language-plaintext highlighter-rouge\">session-id</code> attached to all requests that belong to given HTTP session.</p>\n\n<p>To use it in your <a href=\"https://www.djangoproject.com/\">Django</a> project, you should use the following:</p>\n\n<ul>\n  <li>append <code class=\"language-plaintext highlighter-rouge\">logextractx.middleware.LogCtxDjangoMiddleware</code> to your <code class=\"language-plaintext highlighter-rouge\">MIDDLEWARE</code> in settings:</li>\n</ul>\n\n<div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>\n<span class=\"n\">MIDDLEWARE</span> <span class=\"o\">=</span> <span class=\"p\">[</span>\n    <span class=\"p\">[...]</span>\n     <span class=\"s\">'django.contrib.sessions.middleware.SessionMiddleware'</span><span class=\"p\">,</span>\n    <span class=\"p\">[...]</span>\n    <span class=\"s\">'logextractx.middleware.LogCtxDjangoMiddleware'</span><span class=\"p\">,</span>\n <span class=\"p\">]</span>\n</code></pre></div></div>\n\n<p>And instead of <code class=\"language-plaintext highlighter-rouge\">logextractx.logger</code> use <code class=\"language-plaintext highlighter-rouge\">logextractx.middleware</code>:</p>\n\n<div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>\n<span class=\"kn\">from</span> <span class=\"nn\">logextractx.middleware</span> <span class=\"kn\">import</span> <span class=\"n\">getLogger</span>\n<span class=\"n\">logger</span> <span class=\"o\">=</span> <span class=\"n\">getLogger</span><span class=\"p\">(</span><span class=\"n\">__name__</span><span class=\"p\">)</span>\n<span class=\"p\">[...]</span>\n</code></pre></div></div>\n\n<p>Also, you need to add filter into logging</p>\n\n<div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>\n    <span class=\"s\">'filters'</span><span class=\"p\">:</span> <span class=\"p\">{</span>\n        <span class=\"s\">'RidFilter'</span><span class=\"p\">:</span> <span class=\"p\">{</span>\n            <span class=\"s\">'()'</span><span class=\"p\">:</span> <span class=\"s\">'logextractx.middleware.RidFilter'</span>\n        <span class=\"p\">}</span>\n    <span class=\"p\">}</span>\n</code></pre></div></div>\n\n<p>And that’s all. Now every log entry will contain <code class=\"language-plaintext highlighter-rouge\">request-id</code> and <code class=\"language-plaintext highlighter-rouge\">session-id</code> fields,\nwhat looks so nice in Kibana:</p>\n\n<p><img src=\"/img/articles/2021-06-17-python-logging/kibana-clean.png\" alt=\"kibana-example\" /></p>\n\n<h2 id=\"extra-formatter\">Extra Formatter</h2>\n\n<p>If you use plain logging format, instead of Kibana + JSON formatter, then you may be interested in using\n<code class=\"language-plaintext highlighter-rouge\">logextractx.formatter.ExtraFormatter</code>. Just add following in your formatter definition (DictConfig):</p>\n\n<div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>\n        <span class=\"s\">'formatters'</span><span class=\"p\">:</span> <span class=\"p\">{</span>\n            <span class=\"s\">'simple'</span><span class=\"p\">:</span> <span class=\"p\">{</span>\n                <span class=\"s\">'()'</span><span class=\"p\">:</span> <span class=\"s\">'logextractx.formatter.ExtraFormatter'</span><span class=\"p\">,</span>\n                <span class=\"s\">'fmt'</span><span class=\"p\">:</span> <span class=\"s\">'%(levelname)s %(asctime)s %(name)s: %(message)s [%(extras)s]'</span>\n            <span class=\"p\">}</span>\n        <span class=\"p\">}</span>\n</code></pre></div></div>\n\n<p>And then you will have all <code class=\"language-plaintext highlighter-rouge\">extra</code> in a single log line.</p>\n\n<h2 id=\"conclusion\">Conclusion</h2>\n\n<p>Logging a lot of details is good, but when it leads to breaching the DRY approach, I encourage you\nto use <a href=\"https://github.com/allegro/logextractx\">LogExtraCtx</a>.</p>\n\n<p>Also feel free to contribute — PRs are welcome.</p>\n\n","contentSnippet":"This post is about the reasons behind writing a small (yet practical) library that has just been released as open-source:  LogExtraCtx\nWhy did I write this library?\nI’m a big fan of logging. I like to log as much extra data as possible, and I’m\nfond of logging.debug entries.\nI’m also a DRY approach believer. I feel strong anxiety when I see repetition in code.\nAnd my mind literally hangs when I need to do Ctrl-C/V, even if it’s justified by\ncircumstances. In such cases I start to focus on getting rid of copy-pastes instead of\nwriting new code.\nCombining all these “passions” is not always easy. It’s hard to log everything without\nrepeating things. Even if it’s possible, it usually leads to inelegant code.\nFor example, if you want to log that some error occured:\n\nlogger.error('Foo happened: %s', e)\n\n\nThen so far it’s clean and easy.\nIt would be nice to add some extra details though, like user\n(then you could search by user in Kibana, if your logs goes there):\n\nlogger.error('Foo happened: %s', e, extra={'user': user,\n                                           'action_type': 'bar'})\n\n\nNext, during debugging of a problem, you will notice that it’s not nearly enough and it’s worth\nadding some logging.debug\n\nlogger.debug(\"We're going to do SOMETHING in thread\",\n             extra={'user': user,\n                    'action_type': 'bar',\n                    'thread_num': thread_num})\n\n\nAfter that you will notice that your code has extra= with duplicated user and action_type.\nIt’s a Bad Thing! Imagine what would happen if there was another logger.debug, and another?\nLots of repeated code that should be written only once…\n\nSo in other words, the more details you log, the more cumbersome the code. What can we\ndo?\nMethod 1: don’t log extra\nIt’s not what I like. I do want to log them!\nMethod 2: store extra in a variable\nIt’s what I used to do sometimes, before LogExtraCtx. It could go like that:\n\nextra = {'user': user, 'action_type': 'bar'}\n\nlogger.debug(\"We're going to do SOMETHING in thread\",\n             extra=dict(thread_num=thread_num, **extra))\n\nlogger.error('Foo happened: %s', e, extra=extra)\n\n\nQuite tricky and not very elegant.\nMethod 3: use LogExtraCtx\nBefore I describe the aforementioned library, let me show you a more realistic example.\nConsider the following code:\n\nlogger = logging.getLogger(__name__)\n\n\ndef send_message(requester: str, recipient: str, text: str) -> bool:\n    \"\"\" Function send_message sends message to the specified recipient.  \"\"\"\n    try:\n        r = requests.post(settings.MSG_PROVIDER, json={'recipient': recipient, 'content': text},\n                          ... < other params > ....)\n        r.raise_for_status()\n    except requests.exceptions.RequestException as e:\n        logger.error('Sending message failed. Response text: \"%s\"', e,\n                     extra={  # extra data to be logged and indexed by Kibana\n                         'ACTION_TYPE': 'SEND_MSG',\n                         'requester': requester,\n                         'recipient': recipient,\n                     })\n        return False\n    logger.info('Sending MSG success.',\n                extra={  # the same extra data to be logged/indexed\n                    'ACTION_TYPE': 'SEND_MSG',\n                    'requester': requester,\n                    'recipient': recipient,\n                })\n    return True\n\n\nThere are two log entries, both with extra data for easy-finding in Kibana (there is a bit\nof redundancy in the code, so I already feel the desire to DRY it).\nThen I need to add additional logging:\n\nlogger.debug(\"headers=%r\", r.result.headers,\n             extra={'ACTION_TYPE': 'SEND_MSG',\n                    'requester': requester,\n                    'recipient': recipient\n                    })\n\n\nAnd then, I need even more context in every log entry:\n\nextra = {'ACTION_TYPE': 'SEND_MSG',\n         'requester': requester,\n         'recipient': recipient,\n         'user': str(request.user),\n         'environment': env_type,\n         }\n\n\nCombined all together, it becomes a big, unreadable blob of code. A very simple piece of logic has been\nspoiled by 3 log entries.\n\nlogger = logging.getLogger(__name__)\n\n\ndef send_message(environment: str, requester: str, recipient: str, text: str) -> bool:\n    \"\"\" Function send_message sends MSG to the specified recipient.  \"\"\"\n    try:\n        r = requests.post(settings.MSG_PROVIDER, json={'recipient': recipient, 'content': text},\n                          ... < other params > ....)\n        r.raise_for_status()\n    except requests.exceptions.RequestException as e:\n        logger.error('Sending MSG failed. Response text: \"%s\"', e,\n                     extra={   # extra data to be logged and indexed by Kibana\n                         'ACTION_TYPE': 'SEND_MSG',\n                         'requester': requester,\n                         'recipient': recipient,\n                         'user': str(request.user),\n                         'environment': env_type,\n                     })\n        logger.debug(\"headers=%r\", r.result.headers,\n                     extra={'ACTION_TYPE': 'SEND_MSG',\n                            'requester': requester,\n                            'recipient': recipient,\n                            'user': str(request.user),\n                            'environment': env_type,\n                            })\n        return False\n    logger.info('Sending MSG success.',\n                extra={  # the same extra data to be logged/indexed\n                    'ACTION_TYPE': 'SEND_MSG',\n                    'requester': requester,\n                    'recipient': recipient,\n                    'user': str(request.user),\n                    'environment': env_type,\n                })\n    return True\n\n\nMy solution — LogExtraCtx:\nIn order to use it, just replace logging.getLogger with getLogger from logextractx.logger,\nand then create local logger with local context:\n\nfrom logextractx.logger import getLogger\nlogger = getLogger(__name__)\n[...]\nloclogger = logger.local(extra={'DATA_IN': 'CURRENT_CONTEXT'})\n\n\nso the previous example is reduced to the following clean code:\n\nfrom logextractx.logger import getLogger\nlogger = getLogger(__name__)\n\ndef send_message(environment: str, requester: str, recipient: str, text: str) -> bool:\n    \"\"\" Function send_message sends MSG to the specified recipient.  \"\"\"\n\n    # extra data to be logged/indexed\n    loclogger = logger.local(extra={'ACTION_TYPE': 'SEND_MSG',\n                                    'requester': requester,\n                                    'recipient': recipient,\n                                    'user': str(request.user),\n                                    'environment': env_type})\n\n    try:\n        r = requests.post(settings.MSG_PROVIDER, json={'recipient': recipient, 'content': text},\n                          ... < other params > ....)\n        r.raise_for_status()\n    except requests.exceptions.RequestException as e:\n        loclogger.error('Sending MSG failed. Response text: \"%s\"', e)\n        loclogger.debug(\"headers=%r\", r.result.headers)\n        return False\n    loclogger.info('Sending MSG success.')\n    return True\n\n\nInteresting and useful “side effect”\nUsually, it’s hard to distinguish log entries from various users. For example when you have an error in\nyour code and you find IndexError, you cannot be really sure to which request it belongs.\nOf course, you can guess, based on chronology and many other symptoms,\nbut if you have many concurrent requests, then it’s hard or even impossible to associate ERROR log\nwith previous INFO or DEBUG.\nSo it’s nice to have some kind of tracking ID (request-id), that sticks to the request,\nfollows it and is added to every log entry until the end of request processing. It’s also worth having\nsession-id attached to all requests that belong to given HTTP session.\nTo use it in your Django project, you should use the following:\nappend logextractx.middleware.LogCtxDjangoMiddleware to your MIDDLEWARE in settings:\n\n\nMIDDLEWARE = [\n    [...]\n     'django.contrib.sessions.middleware.SessionMiddleware',\n    [...]\n    'logextractx.middleware.LogCtxDjangoMiddleware',\n ]\n\n\nAnd instead of logextractx.logger use logextractx.middleware:\n\n\nfrom logextractx.middleware import getLogger\nlogger = getLogger(__name__)\n[...]\n\n\nAlso, you need to add filter into logging\n\n\n    'filters': {\n        'RidFilter': {\n            '()': 'logextractx.middleware.RidFilter'\n        }\n    }\n\n\nAnd that’s all. Now every log entry will contain request-id and session-id fields,\nwhat looks so nice in Kibana:\n\nExtra Formatter\nIf you use plain logging format, instead of Kibana + JSON formatter, then you may be interested in using\nlogextractx.formatter.ExtraFormatter. Just add following in your formatter definition (DictConfig):\n\n\n        'formatters': {\n            'simple': {\n                '()': 'logextractx.formatter.ExtraFormatter',\n                'fmt': '%(levelname)s %(asctime)s %(name)s: %(message)s [%(extras)s]'\n            }\n        }\n\n\nAnd then you will have all extra in a single log line.\nConclusion\nLogging a lot of details is good, but when it leads to breaching the DRY approach, I encourage you\nto use LogExtraCtx.\nAlso feel free to contribute — PRs are welcome.","guid":"https://blog.allegro.tech/2021/06/python-logging.html","categories":["tech","python","logging"],"isoDate":"2021-06-16T22:00:00.000Z","thumbnail":"images/post-headers/python.png"},{"title":"Measuring Web Performance","link":"https://blog.allegro.tech/2021/06/measuring-web-performance.html","pubDate":"Tue, 08 Jun 2021 00:00:00 +0200","authors":{"author":[{"name":["Jerzy Jelinek"],"photo":["https://blog.allegro.tech/img/authors/jerzy.jelinek.jpg"],"url":["https://blog.allegro.tech/authors/jerzy.jelinek"]}]},"content":"<p>Some time ago we announced that Allegro passes Core Web Vitals assessment and thanks to that we were awarded in “<a href=\"https://www.thinkwithgoogle.com/intl/en-cee/marketing-strategies/app-and-mobile/why-should-73-of-polish-websites-have-a-closer-look-at-their-mobile-user-experience/\" title=\"Core Web Vitals Hall of Fame\">Core Web Vitals Hall of Fame</a>”.\nIt means that Allegro is in the group of the 27% fastest websites in Polish Internet.</p>\n\n<p>In this series of articles, Webperf team and I want to tell you what our daily work has been like over the years,\nwhat we’ve optimized and what we’ve failed at, and how the perception of web performance has changed at our company.</p>\n\n<div class=\"twitter-tweet twitter-tweet-rendered\" style=\"width: 100%; margin: 10px auto; display: flex; max-width: 550px;\"><iframe id=\"twitter-widget-0\" scrolling=\"no\" allowtransparency=\"true\" allowfullscreen=\"true\" class=\"\" style=\"position: static; visibility: visible; width: 550px; height: 544px; display: block; flex-grow: 1;\" title=\"Twitter Tweet\" src=\"https://platform.twitter.com/embed/Tweet.html?creatorScreenName=allegrotech&amp;dnt=false&amp;embedId=twitter-widget-0&amp;features=eyJ0ZndfZXhwZXJpbWVudHNfY29va2llX2V4cGlyYXRpb24iOnsiYnVja2V0IjoxMjA5NjAwLCJ2ZXJzaW9uIjpudWxsfSwidGZ3X2hvcml6b25fdHdlZXRfZW1iZWRfOTU1NSI6eyJidWNrZXQiOiJodGUiLCJ2ZXJzaW9uIjpudWxsfSwidGZ3X3R3ZWV0X2VtYmVkX2NsaWNrYWJpbGl0eV8xMjEwMiI6eyJidWNrZXQiOiJjb250cm9sIiwidmVyc2lvbiI6bnVsbH19&amp;frame=false&amp;hideCard=false&amp;hideThread=false&amp;id=1331547521139822592&amp;lang=en&amp;origin=https%3A%2F%2Fblog.allegro.tech%2F2021%2F06%2Fmeasuring-web-performance.html&amp;sessionId=1ab4cf28b64fcefdb722376a0651b436e188c01b&amp;theme=light&amp;widgetsVersion=82e1070%3A1619632193066&amp;width=550px\" data-tweet-id=\"1331547521139822592\" frameborder=\"0\"></iframe></div>\n\n<p>Our path to a quite fast page (we’re still hoping for more) was winding, bumpy, more than once ended in a dead end and forced us to rethink our solutions.\nWe want to show that there is no magical <code class=\"language-plaintext highlighter-rouge\">{ perf: true }</code> option and that some things you just have to figure out by trial and error.</p>\n\n<h2 id=\"beginnings\">Beginnings</h2>\n\n<p>It all started with a group of enthusiasts concerned about the poor performance of <a href=\"https://allegro.pl\" title=\"Allegro.pl\">Allegro</a> and the lack of actions to improve it.\nTheir grassroots initiative was appreciated and the core of the technical team (Webperf) was formed.\nThere was one major problem — it is relatively easy to make micro-optimizations in the code of one component,\nhowever, it is much more difficult to push through a major change involving different teams or business areas.\nThe company needed to know how the change would affect not only performance but also the business.\nAt that time there were many success stories from various companies on the internet about how improvements in loading speed had impacted their business results.\nHowever, at Allegro, we had never seen correlation between performance and business. It was our holy grail to be found and as a first step,\nthe decision was made to collect performance measures that could be linked to business data in the future.</p>\n\n<h2 id=\"measurement\">Measurement</h2>\n\n<p>The idea was simple, we wanted to:</p>\n\n<ul>\n  <li>create a library which would use Performance API to create marks on the client side,</li>\n  <li>use an existing mechanism for sending events to the backend,</li>\n  <li>store the information in a database so we could easily operate on it,</li>\n  <li>and finally display basic metrics on a dashboard.</li>\n</ul>\n\n<p>But first things first.</p>\n\n<h3 id=\"metrics-library\">Metrics Library</h3>\n\n<p>The first commits to the library collecting performance measures (called Pinter) took place on June 5, 2017. Since then, it has been actively developed.</p>\n\n<p>We collect two types of measures in Pinter:</p>\n\n<ul>\n  <li>Standard, e.g., Web Vitals,</li>\n  <li>Custom, e.g., Time To Component Interactive.</li>\n</ul>\n\n<h4 id=\"principle-of-operation-of-the-pinter\">Principle of operation of the Pinter</h4>\n\n<p><img src=\"/img/articles/2021-06-08-measuring-web-performance/pinter-diagram.jpg\" alt=\"Pinter principle of operation of Pinter\" title=\"Pinter principle of operation of Pinter\" /></p>\n\n<p>In general, metrics’ changes are tracked using PerformanceObserver from which values are collected, processed into a performance event and sent to the backend.</p>\n\n<p>However, there are several metrics, e.g., Navigation Timing, Resource Timing or Benchmark that are only sent once, after the document has loaded.</p>\n\n<p>Our script, like any other, can affect web performance. This is why the traffic is sampled and the library itself is not served to all users.</p>\n\n<h4 id=\"collected-measures\">Collected measures</h4>\n\n<p>The browser provides a whole bunch of APIs to analyze resources, connections etc. Combined with data from the DOM tree, we have a general picture of what the user experience was like.</p>\n\n<p>Below is a slice of what we are collecting and why:</p>\n\n<ul>\n  <li><strong><a href=\"https://web.dev/learn-web-vitals/\">Web Vitals</a></strong>\n    <ul>\n      <li><strong>First Contentful Paint</strong> — when the first content on the page was rendered.</li>\n      <li><strong>Largest Contentful Paint</strong> — what is the largest image or text block on the page and when it appeared on the screen.</li>\n      <li><strong>Cumulative Layout Shift</strong> — layout stability.</li>\n      <li><strong>First Input Delay</strong> — how quickly the first user interaction is handled.</li>\n    </ul>\n  </li>\n  <li><strong>Custom Marks</strong>\n    <ul>\n      <li><strong>Time To Component Interactive</strong> — when the critical component is fully interactive and can handle all user actions, e.g. after React rehydration.</li>\n    </ul>\n  </li>\n  <li><strong>Navigation</strong>\n    <ul>\n      <li><strong>Type</strong> — what <a href=\"https://developer.mozilla.org/en-US/docs/Web/API/PerformanceNavigationTiming/type\">type of navigation</a> the user was using. Useful when analyzing metrics.</li>\n      <li><strong>Timing</strong> — data from <code class=\"language-plaintext highlighter-rouge\">window.performance.timing</code> about connection, response time, load time etc.</li>\n    </ul>\n  </li>\n  <li><strong>Resources</strong>\n    <ul>\n      <li><strong>Transfer Size</strong> — the total size of scripts and styles transferred over the network.</li>\n      <li><strong>Total Encoded Body Size</strong> — the total size of scripts and styles on the page. Is not distorted by cache.</li>\n      <li><strong>Resource count</strong> — number of assets with breakdown into styles, internal scripts and 3rd party scripts.</li>\n    </ul>\n  </li>\n  <li><strong>Benchmark</strong> — information about the performance of a given device. We want to know if weaker devices perform worse and if our fixes have a positive impact on them.</li>\n</ul>\n\n<h3 id=\"the-backend\">The backend</h3>\n\n<p>All collected performance data is sent to the backend where it is anonymized, aggregated and prepared to be displayed on charts.\nIt is a complex system which allows us to operate only on the necessary portion of data.</p>\n\n<h4 id=\"principle-of-operation-of-the-backend\">Principle of operation of the backend</h4>\n\n<p><img src=\"/img/articles/2021-06-08-measuring-web-performance/backend-diagram.jpg\" alt=\"backend principle of operation\" title=\"backend principle of operation\" /></p>\n\n<p>Initially, all events (including performance ones) are gathered and stored in a single <a href=\"https://hive.apache.org/\">HIVE</a> table.\nWe want to be able to quickly analyze as well as compare historical records, but this amount of data would effectively prevent us from doing so.\nTherefore, we need a whole process to extract the most relevant information. We transform the filtered performance events combined\nwith more general data (page route, device details etc.) to a new, smaller Hive table. Then we index this data in <a href=\"https://druid.apache.org/\">Druid</a>\n(high performance real-time analytics database), which is consumed by <a href=\"/2018/10/turnilo-lets-change-the-way-people-explore-big-data.html\">Turnilo</a> and Grafana.\nOnce the entire process is complete, we are able to filter, split, plot and generally process about 2TB of data in real time as needed.</p>\n\n<h4 id=\"visualizations\">Visualizations</h4>\n\n<p>We use two independent systems to present the data:</p>\n\n<ul>\n  <li><a href=\"https://grafana.com/\">Grafana</a>, which is used for daily monitoring.</li>\n  <li><a href=\"https://github.com/allegro/turnilo\">Turnilo</a>, which is used for analyzing anomalies or testing the impact of A/B experiments.</li>\n</ul>\n\n<h5 id=\"grafana\">Grafana</h5>\n\n<p>Our dashboard gathers the most important metrics which allow us to catch potential performance problems but it is not used for analysis.\nIt is worth noting that we display data only for mobile devices. We do this for a reason: in general those devices\nare not as efficient as desktops and the share of phones in Allegro traffic is growing day by day.\nWe assume that improving performance on mobile devices would have a positive impact on desktops as well.</p>\n\n<p><img src=\"/img/articles/2021-06-08-measuring-web-performance/grafana-screen.png\" alt=\"Grafana screenshot\" title=\"Grafana screenshot\" /></p>\n\n<h5 id=\"turnilo\">Turnilo</h5>\n\n<p>It is a business intelligence, data exploration and visualization web application. Thanks to the wide range of available dimensions\nand metrics we are able to pinpoint found issues to particular pages, device types or even browser versions\nand then check if the applied solution actually worked.</p>\n\n<p><img src=\"/img/articles/2021-06-08-measuring-web-performance/turnilo-screen.png\" alt=\"Turnilo screenshot\" title=\"Turnilo screenshot\" /></p>\n\n<h3 id=\"monitoring\">Monitoring</h3>\n\n<p>Checking measures on the dashboard is our daily routine, but we are only humans and sometimes we can miss certain anomalies\nor we won’t be able to notice a changing trend so we decided to automate our work as much as possible.\nWe have created a range of detectors that notify us on Slack or mail when a predetermined threshold is exceeded.</p>\n\n<p><img src=\"/img/articles/2021-06-08-measuring-web-performance/monitoring-screen.png\" alt=\"Monitoring screenshot\" title=\"Monitoring screenshot\" /></p>\n\n<h2 id=\"summary\">Summary</h2>\n\n<p>Before we started our optimization work, we needed to know:</p>\n\n<ul>\n  <li>What do we want to measure?</li>\n  <li>How will we collect this data?</li>\n  <li>How will we visualize and compare them?</li>\n</ul>\n\n<p>Answers to those questions and implementation of their results allow us to keep track of performance regression from our users. We are able to analyze how the implemented optimizations, A/B tests or content changes affect performance metrics.</p>\n\n<p>In the next article, we will tell you what we were able to optimize, how our metrics changed over the years and what were the failures from which we have learned a lot.</p>\n","contentSnippet":"Some time ago we announced that Allegro passes Core Web Vitals assessment and thanks to that we were awarded in “Core Web Vitals Hall of Fame”.\nIt means that Allegro is in the group of the 27% fastest websites in Polish Internet.\nIn this series of articles, Webperf team and I want to tell you what our daily work has been like over the years,\nwhat we’ve optimized and what we’ve failed at, and how the perception of web performance has changed at our company.\n\nOur path to a quite fast page (we’re still hoping for more) was winding, bumpy, more than once ended in a dead end and forced us to rethink our solutions.\nWe want to show that there is no magical { perf: true } option and that some things you just have to figure out by trial and error.\nBeginnings\nIt all started with a group of enthusiasts concerned about the poor performance of Allegro and the lack of actions to improve it.\nTheir grassroots initiative was appreciated and the core of the technical team (Webperf) was formed.\nThere was one major problem — it is relatively easy to make micro-optimizations in the code of one component,\nhowever, it is much more difficult to push through a major change involving different teams or business areas.\nThe company needed to know how the change would affect not only performance but also the business.\nAt that time there were many success stories from various companies on the internet about how improvements in loading speed had impacted their business results.\nHowever, at Allegro, we had never seen correlation between performance and business. It was our holy grail to be found and as a first step,\nthe decision was made to collect performance measures that could be linked to business data in the future.\nMeasurement\nThe idea was simple, we wanted to:\ncreate a library which would use Performance API to create marks on the client side,\nuse an existing mechanism for sending events to the backend,\nstore the information in a database so we could easily operate on it,\nand finally display basic metrics on a dashboard.\nBut first things first.\nMetrics Library\nThe first commits to the library collecting performance measures (called Pinter) took place on June 5, 2017. Since then, it has been actively developed.\nWe collect two types of measures in Pinter:\nStandard, e.g., Web Vitals,\nCustom, e.g., Time To Component Interactive.\nPrinciple of operation of the Pinter\n\nIn general, metrics’ changes are tracked using PerformanceObserver from which values are collected, processed into a performance event and sent to the backend.\nHowever, there are several metrics, e.g., Navigation Timing, Resource Timing or Benchmark that are only sent once, after the document has loaded.\nOur script, like any other, can affect web performance. This is why the traffic is sampled and the library itself is not served to all users.\nCollected measures\nThe browser provides a whole bunch of APIs to analyze resources, connections etc. Combined with data from the DOM tree, we have a general picture of what the user experience was like.\nBelow is a slice of what we are collecting and why:\nWeb Vitals\n    \nFirst Contentful Paint — when the first content on the page was rendered.\nLargest Contentful Paint — what is the largest image or text block on the page and when it appeared on the screen.\nCumulative Layout Shift — layout stability.\nFirst Input Delay — how quickly the first user interaction is handled.\nCustom Marks\n    \nTime To Component Interactive — when the critical component is fully interactive and can handle all user actions, e.g. after React rehydration.\nNavigation\n    \nType — what type of navigation the user was using. Useful when analyzing metrics.\nTiming — data from window.performance.timing about connection, response time, load time etc.\nResources\n    \nTransfer Size — the total size of scripts and styles transferred over the network.\nTotal Encoded Body Size — the total size of scripts and styles on the page. Is not distorted by cache.\nResource count — number of assets with breakdown into styles, internal scripts and 3rd party scripts.\nBenchmark — information about the performance of a given device. We want to know if weaker devices perform worse and if our fixes have a positive impact on them.\nThe backend\nAll collected performance data is sent to the backend where it is anonymized, aggregated and prepared to be displayed on charts.\nIt is a complex system which allows us to operate only on the necessary portion of data.\nPrinciple of operation of the backend\n\nInitially, all events (including performance ones) are gathered and stored in a single HIVE table.\nWe want to be able to quickly analyze as well as compare historical records, but this amount of data would effectively prevent us from doing so.\nTherefore, we need a whole process to extract the most relevant information. We transform the filtered performance events combined\nwith more general data (page route, device details etc.) to a new, smaller Hive table. Then we index this data in Druid\n(high performance real-time analytics database), which is consumed by Turnilo and Grafana.\nOnce the entire process is complete, we are able to filter, split, plot and generally process about 2TB of data in real time as needed.\nVisualizations\nWe use two independent systems to present the data:\nGrafana, which is used for daily monitoring.\nTurnilo, which is used for analyzing anomalies or testing the impact of A/B experiments.\nGrafana\nOur dashboard gathers the most important metrics which allow us to catch potential performance problems but it is not used for analysis.\nIt is worth noting that we display data only for mobile devices. We do this for a reason: in general those devices\nare not as efficient as desktops and the share of phones in Allegro traffic is growing day by day.\nWe assume that improving performance on mobile devices would have a positive impact on desktops as well.\n\nTurnilo\nIt is a business intelligence, data exploration and visualization web application. Thanks to the wide range of available dimensions\nand metrics we are able to pinpoint found issues to particular pages, device types or even browser versions\nand then check if the applied solution actually worked.\n\nMonitoring\nChecking measures on the dashboard is our daily routine, but we are only humans and sometimes we can miss certain anomalies\nor we won’t be able to notice a changing trend so we decided to automate our work as much as possible.\nWe have created a range of detectors that notify us on Slack or mail when a predetermined threshold is exceeded.\n\nSummary\nBefore we started our optimization work, we needed to know:\nWhat do we want to measure?\nHow will we collect this data?\nHow will we visualize and compare them?\nAnswers to those questions and implementation of their results allow us to keep track of performance regression from our users. We are able to analyze how the implemented optimizations, A/B tests or content changes affect performance metrics.\nIn the next article, we will tell you what we were able to optimize, how our metrics changed over the years and what were the failures from which we have learned a lot.","guid":"https://blog.allegro.tech/2021/06/measuring-web-performance.html","categories":["tech","webperf","frontend","performance","perfmatters","javascript"],"isoDate":"2021-06-07T22:00:00.000Z","thumbnail":"images/post-headers/default.jpg"},{"title":"Domino - financial forecasting in the age of global pandemic","link":"https://blog.allegro.tech/2021/05/domino-financial-forecasting-in-the-age-of-global-pandemic.html","pubDate":"Fri, 21 May 2021 00:00:00 +0200","authors":{"author":[{"name":["Piotr Gabryś"],"photo":["https://blog.allegro.tech/img/authors/piotr.gabrys.jpg"],"url":["https://blog.allegro.tech/authors/piotr.gabrys"]},{"name":["Julia Bluszcz"],"photo":["https://blog.allegro.tech/img/authors/julia.bluszcz.jpg"],"url":["https://blog.allegro.tech/authors/julia.bluszcz"]},{"name":["Klaudia Walewska-Łubian"],"photo":["https://blog.allegro.tech/img/authors/klaudia.walewska-lubian.jpg"],"url":["https://blog.allegro.tech/authors/klaudia.walewska-lubian"]}]},"content":"<p>Accurate forecasting is key for any successful business. It allows one to set realistic financial goals for the next quarters, evaluate impact of business decisions, and prepare adequate resources for what is coming.</p>\n\n<p>Yet, many companies struggle with efficient and accurate revenue forecasting. For most of them the task still rests on the financial department’s shoulders, performing manual analyses in Excel, lacking data science know-how, and relying on a set of arbitrary assumptions concerning the future. This process is often inefficient and prone to error, making it hard to distinguish trend-based patterns from manual overrides. In the end, the forecasts often turn out to be inaccurate, but it is difficult to diagnose the source of divergence - was it the fault of the model itself, wrong business assumptions, or perhaps unusual circumstances.</p>\n\n<p>In order to overcome this problem, we decided to develop an AI tool which would allow us to forecast Allegro’s GMV (Gross Merchandise Value) based on a set of business inputs defined by the financial team. The purpose of the tool was twofold: to create automatic, monthly forecasts, and to enable creating scenarios with different input values. Examples of such inputs would be the costs of online advertising or the number of users enrolled in the loyalty program (Allegro Smart!).</p>\n\n<blockquote>\n  <p><strong>Disclaimer:</strong> analysis and graphs presented in article are based on artificial data</p>\n</blockquote>\n\n<h2 id=\"approach\">Approach</h2>\n\n<p>We named our project domino after Marvel’s universe foreseer-heroine Domino, but the name reflects also how the model is executed.</p>\n\n<p>The model works as a graph of dependencies where each element is either a cause or effect of other steps (in the technical nomenclature known as <a href=\"https://en.wikipedia.org/wiki/Directed_acyclic_graph\">DAG</a>). Let’s go through an artificial example. Below you can find a diagram of the model. In the training phase we need to provide historical data for every node in the graph, i.e. we need to know actual online advertising spending (<a href=\"https://en.wikipedia.org/wiki/Pay-per-click\">PPC</a>), important holidays, and actual number of paid and organic visits to be able to accurately model the relationship between these variables.<img src=\"/img/articles/2021-05-21-domino-financial-forecasting-in-the-age-of-global-pandemic/01-approach.png\" alt=\"diagram\" /></p>\n\n<p>During the prediction phase, we only need actual data for violet nodes. These nodes usually represent our key business assumptions which are reflected in the company’s yearly budget. Next, our goal is to recreate all the succeeding values. Some of these subsequent steps will be just arithmetic operations like adding, summing, or applying logarithms (yellow and red). Other steps, here represented with blue nodes, may be Machine Learning models (e.g. <a href=\"https://facebook.github.io/prophet/\">Facebook’s Prophet</a>). They can learn non-trivial effects like seasonalities, trends, holidays, and the influence of the preceding yellow nodes (i.e. explanatory variables).</p>\n\n<p>You may wonder why we bothered with creation of such a complex graph of dependencies instead of making a single model taking all the business inputs as explanatory variables and returning future GMV values as the output. It’s what we want to model in the end, right? Why are we concerned with reconstructing the values of organic or paid traffic along the way? Well, we found out that following business logic yields much better results than inserting all the inputs into a huge single pot. Not only were the results less accurate in the latter case, but also the impact of explanatory variables on GMV was often not clearly distinguishable or even misleading. Instead, we recreated the business scheme by creating and training specialized models to reconstruct all the intermediate steps (e.g. predicting the number of paid visits) and merging their outputs with arithmetic operations or other models. However, remember that this approach comes at a cost! Using many steps in the modeling process may be both a blessing and a curse, since you need to train and maintain multiple models simultaneously.</p>\n\n<p>One of the biggest advantages of this approach is that it allows us to capture very sophisticated non-linear relationships between inputs and the final target variable (GMV in our case). Following the business logic allows us to verify these non-trivial assumptions at each step.</p>\n\n<p>Another advantage of this approach is that every model can be fitted separately with any model class you want. Having tested a few alternatives, we’ve chosen the Prophet library, but potentially any ML algorithm could be used (e.g. ARIMA, Gradient Boosting Trees, or Artificial Neural Networks).</p>\n\n<p>A disadvantage is that the error in prediction propagates downstream the graph. So, if we make a mistake in a prediction in an early step, it will influence all the models and transformations dependent on it. The issue can be mitigated by creating accurate models at each step of the process.</p>\n\n<p>Another slight disadvantage is that our Domino of models is not intrinsically interpretable (as most of the modern model classes). You have to use some post hoc methods to gather information on how the model does process data.</p>\n\n<h2 id=\"technical-implementation\">Technical implementation</h2>\n\n<p>To develop and iterate over the DAG-type model we had to implement a custom Python framework. It allows for training and running models as well as arithmetic transformations in a predefined order.</p>\n\n<p>The implementation allows us to utilize various model frameworks like Facebook’s Prophet, any Scikit-learn’s regressor, or an Artificial NN. For operational purposes, MAPE (Mean Absolute Percentage Error) can be easily calculated for each model in the graph, as well as for the whole DAG (again, the errors propagate downstream).</p>\n\n<p>Below you can see how the above DAG can be implemented.</p>\n\n<div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"kn\">from</span> <span class=\"nn\">domino.pipeline</span> <span class=\"kn\">import</span> <span class=\"n\">Pipeline</span><span class=\"p\">,</span> <span class=\"n\">Model</span><span class=\"p\">,</span> <span class=\"n\">Transformer</span><span class=\"p\">,</span> <span class=\"n\">Combinator</span>\n<span class=\"kn\">from</span> <span class=\"nn\">fbprophet</span> <span class=\"kn\">import</span> <span class=\"n\">Prophet</span>\n\n<span class=\"n\">dag</span> <span class=\"o\">=</span> <span class=\"n\">Pipeline</span><span class=\"p\">(</span><span class=\"n\">input_list</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"s\">'ppc_cost'</span><span class=\"p\">,</span> <span class=\"s\">'smart_users'</span><span class=\"p\">])</span>\n\n<span class=\"n\">dag</span><span class=\"p\">.</span><span class=\"n\">add_step</span><span class=\"p\">(</span><span class=\"n\">step</span><span class=\"o\">=</span><span class=\"n\">Transformer</span><span class=\"p\">(</span><span class=\"n\">base_var</span><span class=\"o\">=</span><span class=\"s\">'ppc_cost'</span><span class=\"p\">,</span>\n                             <span class=\"n\">target_variable</span><span class=\"o\">=</span><span class=\"s\">'ppc_cost_ln'</span><span class=\"p\">,</span>\n                             <span class=\"n\">operation_name</span><span class=\"o\">=</span><span class=\"s\">'ln'</span><span class=\"p\">),</span>\n             <span class=\"n\">step_name</span><span class=\"o\">=</span><span class=\"s\">'transformer1'</span><span class=\"p\">)</span>\n\n<span class=\"n\">dag</span><span class=\"p\">.</span><span class=\"n\">add_step</span><span class=\"p\">(</span><span class=\"n\">step</span><span class=\"o\">=</span><span class=\"n\">Transformer</span><span class=\"p\">(</span><span class=\"n\">base_var</span><span class=\"o\">=</span><span class=\"s\">'smart_users'</span><span class=\"p\">,</span>\n                             <span class=\"n\">target_variable</span><span class=\"o\">=</span><span class=\"s\">'smart_users_add1'</span><span class=\"p\">,</span>\n                             <span class=\"n\">operation_name</span><span class=\"o\">=</span><span class=\"s\">'add1'</span><span class=\"p\">),</span>\n             <span class=\"n\">step_name</span><span class=\"o\">=</span><span class=\"s\">'transformer2'</span><span class=\"p\">)</span>\n\n<span class=\"n\">dag</span><span class=\"p\">.</span><span class=\"n\">add_step</span><span class=\"p\">(</span><span class=\"n\">step</span><span class=\"o\">=</span><span class=\"n\">Transformer</span><span class=\"p\">(</span><span class=\"n\">base_var</span><span class=\"o\">=</span><span class=\"s\">'smart_users_add1'</span><span class=\"p\">,</span>\n                             <span class=\"n\">target_variable</span><span class=\"o\">=</span><span class=\"s\">'smart_users_add1_ln'</span><span class=\"p\">,</span>\n                             <span class=\"n\">operation_name</span><span class=\"o\">=</span><span class=\"s\">'ln'</span><span class=\"p\">),</span>\n             <span class=\"n\">step_name</span><span class=\"o\">=</span><span class=\"s\">'transformer3'</span><span class=\"p\">)</span>\n\n<span class=\"n\">dag</span><span class=\"p\">.</span><span class=\"n\">add_step</span><span class=\"p\">(</span><span class=\"n\">step</span><span class=\"o\">=</span><span class=\"n\">Model</span><span class=\"p\">(</span><span class=\"n\">model</span><span class=\"o\">=</span><span class=\"n\">Prophet</span><span class=\"p\">(),</span>\n                  <span class=\"n\">target_variable</span><span class=\"o\">=</span><span class=\"s\">'paid_visits'</span><span class=\"p\">,</span>\n                  <span class=\"n\">explanatory_variables</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"s\">'ppc_cost_ln'</span><span class=\"p\">]),</span>\n         <span class=\"n\">step_name</span><span class=\"o\">=</span><span class=\"s\">'model1'</span><span class=\"p\">)</span>\n\n<span class=\"n\">dag</span><span class=\"p\">.</span><span class=\"n\">add_step</span><span class=\"p\">(</span><span class=\"n\">step</span><span class=\"o\">=</span><span class=\"n\">Model</span><span class=\"p\">(</span><span class=\"n\">model</span><span class=\"o\">=</span><span class=\"n\">Prophet</span><span class=\"p\">(),</span>\n                  <span class=\"n\">target_variable</span><span class=\"o\">=</span><span class=\"s\">'non_paid_visits'</span><span class=\"p\">,</span>\n                  <span class=\"n\">explanatory_variables</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"s\">'smart_users_add1_ln'</span><span class=\"p\">]),</span>\n             <span class=\"n\">step_name</span><span class=\"o\">=</span><span class=\"s\">'model2'</span><span class=\"p\">)</span>\n\n<span class=\"n\">dag</span><span class=\"p\">.</span><span class=\"n\">add_step</span><span class=\"p\">(</span><span class=\"n\">step</span><span class=\"o\">=</span><span class=\"n\">Combinator</span><span class=\"p\">(</span><span class=\"n\">base_var1</span><span class=\"o\">=</span><span class=\"s\">'paid_visits'</span><span class=\"p\">,</span>\n                            <span class=\"n\">base_var2</span><span class=\"o\">=</span><span class=\"s\">'non_paid_visits'</span><span class=\"p\">,</span>\n                            <span class=\"n\">target_variable</span><span class=\"o\">=</span><span class=\"s\">'all_visits'</span><span class=\"p\">,</span>\n                            <span class=\"n\">operation_name</span><span class=\"o\">=</span><span class=\"s\">'add'</span><span class=\"p\">),</span>\n             <span class=\"n\">step_name</span><span class=\"o\">=</span><span class=\"s\">'combination'</span><span class=\"p\">)</span>\n</code></pre></div></div>\n\n<p>Now we can use the dag object as a single model.</p>\n\n<div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"n\">dag</span><span class=\"p\">.</span><span class=\"n\">fit</span><span class=\"p\">(</span><span class=\"n\">df_train</span><span class=\"p\">)</span>\n<span class=\"n\">predictions</span> <span class=\"o\">=</span> <span class=\"n\">dag</span><span class=\"p\">.</span><span class=\"n\">predict</span><span class=\"p\">(</span><span class=\"n\">df_test</span><span class=\"p\">)</span>\n</code></pre></div></div>\n\n<p>To understand what’s happening inside the DAG, we implemented two methods of calculating MAPE on every step:</p>\n\n<div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"n\">dag</span><span class=\"p\">.</span><span class=\"n\">calculate_mape_for_models</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"o\">=</span><span class=\"n\">df_test</span><span class=\"p\">)</span>\n<span class=\"n\">dag</span><span class=\"p\">.</span><span class=\"n\">calculate_mape_for_dag</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"o\">=</span><span class=\"n\">df_test</span><span class=\"p\">)</span>\n</code></pre></div></div>\n\n<p>They both return dictionaries of model-MAPE pairs. The calculate_mape_for_model method checks each model separately, and calculate_mape_for_dag takes into account the errors propagating from preceding steps. These are examples of results:</p>\n\n<div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"n\">models_mape</span> <span class=\"o\">=</span> <span class=\"p\">{</span>\n   <span class=\"s\">'transformer1'</span><span class=\"p\">:</span> <span class=\"mf\">0.0</span><span class=\"p\">,</span>\n   <span class=\"s\">'transformer2'</span><span class=\"p\">:</span> <span class=\"mf\">0.0</span><span class=\"p\">,</span>\n   <span class=\"s\">'transformer3'</span><span class=\"p\">:</span> <span class=\"mf\">0.0</span><span class=\"p\">,</span>\n   <span class=\"s\">'model1'</span><span class=\"p\">:</span> <span class=\"mf\">0.1</span><span class=\"p\">,</span>\n   <span class=\"s\">'model2'</span><span class=\"p\">:</span> <span class=\"mf\">0.05</span><span class=\"p\">,</span>\n   <span class=\"s\">'combination'</span><span class=\"p\">:</span> <span class=\"mf\">0.0</span>\n<span class=\"p\">}</span>\n\n<span class=\"n\">dag_mape</span> <span class=\"o\">=</span> <span class=\"p\">{</span>\n   <span class=\"s\">'transformer1'</span><span class=\"p\">:</span> <span class=\"mf\">0.0</span><span class=\"p\">,</span>\n   <span class=\"s\">'transformer2'</span><span class=\"p\">:</span> <span class=\"mf\">0.0</span><span class=\"p\">,</span>\n   <span class=\"s\">'transformer3'</span><span class=\"p\">:</span> <span class=\"mf\">0.0</span><span class=\"p\">,</span>\n   <span class=\"s\">'model1'</span><span class=\"p\">:</span> <span class=\"mf\">0.1</span><span class=\"p\">,</span>\n   <span class=\"s\">'model2'</span><span class=\"p\">:</span> <span class=\"mf\">0.05</span><span class=\"p\">,</span>\n   <span class=\"s\">'combination'</span><span class=\"p\">:</span> <span class=\"mf\">0.09</span>\n<span class=\"p\">}</span>\n</code></pre></div></div>\n\n<p>Note that the combination step has MAPE equal to zero in models_mape and a positive one in dag_mape. That’s because it does not generate any error, as it’s an arithmetic operation, but it can propagate errors from previous steps.</p>\n\n<p>Last but not least, there is an explainability method calculate_variable_impact that helps to evaluate how changes in initial inputs impact the subsequent steps in the graph. For example, we can check what is going to happen if we decrease PPC costs by 10% and increase the number of Smart users by 5%.</p>\n\n<div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"n\">dag</span><span class=\"p\">.</span><span class=\"n\">calculate_variable_impact</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"o\">=</span><span class=\"n\">df_test</span><span class=\"p\">,</span>\n                              <span class=\"n\">variables_list</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"s\">'ppc_cost'</span><span class=\"p\">,</span> <span class=\"s\">'smart_users'</span><span class=\"p\">],</span>\n                              <span class=\"n\">variables_multiplier_list</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"mf\">0.9</span><span class=\"p\">,</span> <span class=\"mf\">1.05</span><span class=\"p\">],</span>\n                              <span class=\"n\">diff_type</span><span class=\"o\">=</span><span class=\"s\">'relative'</span><span class=\"p\">)</span>\n</code></pre></div></div>\n\n<p>The percent change will be calculated on every node, i.e. smart_users_add1_ln, paid_visits, and all_visits. We will be able to evaluate how such changes affect not only the GMV, but also all intermediary KPIs.</p>\n\n<h2 id=\"facebooks-prophet\">Facebook’s Prophet</h2>\n\n<p>Having tested various modelling techniques, we chose the forecasting procedure offered by Facebook’s Prophet library (<a href=\"https://facebook.github.io/prophet/\">https://facebook.github.io/prophet/</a>). It uses a decomposable Bayesian time series model with three main components: seasonalities, trends and errors, hence it works well for our time series that have strong seasonal effects. Moreover, the Prophet model is robust to outliers and shifts in the trend, which proved very useful in some models. Mostly, however, we assumed the trend to be flat. The piecewise linear trend explains some of the variance of the dependent variable which could otherwise be explained by the regressor variables/inputs. Given that the purpose of the tool is to allow testing scenarios with different values of the inputs, we needed our models to estimate the relationship between the explanatory variables and the dependent variable, but accounting for seasonality, holidays and some external events (e.g. COVID) only. The graphs below show an example of the forecast components generated by the Prophet, with linear trend, effect of holidays, weekly and yearly seasonality, as well as effect of explanatory variables.</p>\n\n<p><img src=\"/img/articles/2021-05-21-domino-financial-forecasting-in-the-age-of-global-pandemic/02-chart.png\" alt=\"chart\" />\n<img src=\"/img/articles/2021-05-21-domino-financial-forecasting-in-the-age-of-global-pandemic/03-chart.png\" alt=\"chart\" />\n<img src=\"/img/articles/2021-05-21-domino-financial-forecasting-in-the-age-of-global-pandemic/04-chart.png\" alt=\"chart\" /></p>\n\n<p>Adding our domain knowledge about the analysed time series (e.g. calendar effects) and possibility of tuning the parameters of the model (e.g. the strength of the weekly seasonality effect) makes Prophet a perfect fit for our purpose.</p>\n\n<h2 id=\"modelling-framework\">Modelling framework</h2>\n\n<p>Both the patterns and relationships between the variables change slightly over time, hence it would be naive to expect that once all the models are tuned they will give best forecasts forever. It is therefore necessary to analyze the results every time the forecasts are made (every month) and apply necessary tweaks to the models.</p>\n\n<p>As in every machine learning project, we split our time series into:</p>\n\n<ul>\n  <li>\n    <p>Training dataset: the actual dataset that we use to train the model, i.e. the model learns from this data.</p>\n  </li>\n  <li>\n    <p>Validation dataset: the sample of data used to provide an unbiased evaluation of a model fit on the training dataset while tuning model hyperparameters; in time series, this is a time period following the training dataset time series.</p>\n  </li>\n  <li>\n    <p>Test dataset: the sample of data used to provide an unbiased evaluation of a final model fit on the training dataset. In time series, this is a time period following the validation dataset time series.</p>\n  </li>\n</ul>\n\n<p>The COVID-19 pandemic wreaked havoc on our time series, not only changing the patterns temporarily, but often introducing shifts in the trend. Given that we worked on the tool during summer 2020, we were forced to use quite a non-standard approach to hyperparameter tuning and model testing (e.g. to maximize the length of the training period, so that it includes at least a month of the data showing post-COVID comeback to a new normal).</p>\n\n<p>In the long run, we expect that the process will stabilize and we’ll be able to conduct the following adjustment procedure each month: training all the models using the parameters set in the previous month, testing them on the last 2 months of observed data, evaluating monthly and daily MAPE. When forecast errors in GMV prediction or any intermediate model are too large, we scrutinize the graphs of observed vs forecasted values. It is also helpful to compare the predictions vs observed values for the same period of the previous year. This step allows us to verify whether there are any seasonalities or patterns that were not detected by the tuned model. We can fine-tune the models either manually, or using the automatic hyperparameter optimization framework.</p>\n\n<ul>\n  <li>\n    <p>Hyperparameter tuning using Optuna (<a href=\"https://optuna.org/\">https://optuna.org/</a>), half a year’s worth of data and expanding window approach (see visualisation below). This means that we will fine-tune our models using 6 sets of validation datasets, each consisting of 1, 2, 3, 4, 5 and 6 months. The Optuna framework will suggest parameters that minimize the average of MAPE over these datasets.</p>\n  </li>\n  <li>\n    <p>Testing the tuned models on 2 last months of observed data, measuring MAPE on the forecasted vs observed values of GMV, as well as on all intermediate models.</p>\n  </li>\n  <li>\n    <p>If any of the MAPE is not satisfactory, again scrutinizing the graphs and fine-tuning the models manually.</p>\n  </li>\n</ul>\n\n<p>Once we are satisfied with the results, we always check if the changes made to the models do not result in some explanatory variables having unexpected signs of impact on GMV.</p>\n\n<p>Despite the changes in time series, we are expecting that in the long run fewer and fewer tweaks to the models will be necessary, and less work will be required from the analysts to maintain the tool.</p>\n\n<h2 id=\"user-interface\">User interface</h2>\n\n<p>To make the model easily accessible by business users, an interactive application was prepared. The user has default inputs set for upcoming months. They can change their values and get model predictions by clicking the “RUN SCENARIO” button. The predictions can be seen in daily, weekly and monthly granularities. If the user chooses to, they can export the predictions in CSV format. You can find an anonymised print screen of the tool below.</p>\n\n<p><img src=\"/img/articles/2021-05-21-domino-financial-forecasting-in-the-age-of-global-pandemic/05-domino.png\" alt=\"Domino UI\" /></p>\n\n<h2 id=\"summary\">Summary</h2>\n\n<p>As a result of the project, we developed a solution providing incredible business value. The main features of the tool are:</p>\n\n<ul>\n  <li>\n    <p>Great forecast accuracy - we managed to get below 2% MAPE</p>\n  </li>\n  <li>\n    <p>Stability - the structure of the model remains the same and the inputs have the same impact direction over time</p>\n  </li>\n  <li>\n    <p>Responsiveness - the forecasts change with changes in the business inputs</p>\n  </li>\n  <li>\n    <p>Interpretation - though the model is not intrinsically interpretable, we developed methods to check how well it works</p>\n  </li>\n  <li>\n    <p>Interactive UI - stakeholders can experiment with various business scenarios online</p>\n  </li>\n</ul>\n\n<p>Domino proved its effectiveness in hard and demanding times while giving us a lot of practical knowledge related to modeling of such a complex business metric. And, we already started using these lessons in new upcoming projects.</p>\n","contentSnippet":"Accurate forecasting is key for any successful business. It allows one to set realistic financial goals for the next quarters, evaluate impact of business decisions, and prepare adequate resources for what is coming.\nYet, many companies struggle with efficient and accurate revenue forecasting. For most of them the task still rests on the financial department’s shoulders, performing manual analyses in Excel, lacking data science know-how, and relying on a set of arbitrary assumptions concerning the future. This process is often inefficient and prone to error, making it hard to distinguish trend-based patterns from manual overrides. In the end, the forecasts often turn out to be inaccurate, but it is difficult to diagnose the source of divergence - was it the fault of the model itself, wrong business assumptions, or perhaps unusual circumstances.\nIn order to overcome this problem, we decided to develop an AI tool which would allow us to forecast Allegro’s GMV (Gross Merchandise Value) based on a set of business inputs defined by the financial team. The purpose of the tool was twofold: to create automatic, monthly forecasts, and to enable creating scenarios with different input values. Examples of such inputs would be the costs of online advertising or the number of users enrolled in the loyalty program (Allegro Smart!).\nDisclaimer: analysis and graphs presented in article are based on artificial data\nApproach\nWe named our project domino after Marvel’s universe foreseer-heroine Domino, but the name reflects also how the model is executed.\nThe model works as a graph of dependencies where each element is either a cause or effect of other steps (in the technical nomenclature known as DAG). Let’s go through an artificial example. Below you can find a diagram of the model. In the training phase we need to provide historical data for every node in the graph, i.e. we need to know actual online advertising spending (PPC), important holidays, and actual number of paid and organic visits to be able to accurately model the relationship between these variables.\nDuring the prediction phase, we only need actual data for violet nodes. These nodes usually represent our key business assumptions which are reflected in the company’s yearly budget. Next, our goal is to recreate all the succeeding values. Some of these subsequent steps will be just arithmetic operations like adding, summing, or applying logarithms (yellow and red). Other steps, here represented with blue nodes, may be Machine Learning models (e.g. Facebook’s Prophet). They can learn non-trivial effects like seasonalities, trends, holidays, and the influence of the preceding yellow nodes (i.e. explanatory variables).\nYou may wonder why we bothered with creation of such a complex graph of dependencies instead of making a single model taking all the business inputs as explanatory variables and returning future GMV values as the output. It’s what we want to model in the end, right? Why are we concerned with reconstructing the values of organic or paid traffic along the way? Well, we found out that following business logic yields much better results than inserting all the inputs into a huge single pot. Not only were the results less accurate in the latter case, but also the impact of explanatory variables on GMV was often not clearly distinguishable or even misleading. Instead, we recreated the business scheme by creating and training specialized models to reconstruct all the intermediate steps (e.g. predicting the number of paid visits) and merging their outputs with arithmetic operations or other models. However, remember that this approach comes at a cost! Using many steps in the modeling process may be both a blessing and a curse, since you need to train and maintain multiple models simultaneously.\nOne of the biggest advantages of this approach is that it allows us to capture very sophisticated non-linear relationships between inputs and the final target variable (GMV in our case). Following the business logic allows us to verify these non-trivial assumptions at each step.\nAnother advantage of this approach is that every model can be fitted separately with any model class you want. Having tested a few alternatives, we’ve chosen the Prophet library, but potentially any ML algorithm could be used (e.g. ARIMA, Gradient Boosting Trees, or Artificial Neural Networks).\nA disadvantage is that the error in prediction propagates downstream the graph. So, if we make a mistake in a prediction in an early step, it will influence all the models and transformations dependent on it. The issue can be mitigated by creating accurate models at each step of the process.\nAnother slight disadvantage is that our Domino of models is not intrinsically interpretable (as most of the modern model classes). You have to use some post hoc methods to gather information on how the model does process data.\nTechnical implementation\nTo develop and iterate over the DAG-type model we had to implement a custom Python framework. It allows for training and running models as well as arithmetic transformations in a predefined order.\nThe implementation allows us to utilize various model frameworks like Facebook’s Prophet, any Scikit-learn’s regressor, or an Artificial NN. For operational purposes, MAPE (Mean Absolute Percentage Error) can be easily calculated for each model in the graph, as well as for the whole DAG (again, the errors propagate downstream).\nBelow you can see how the above DAG can be implemented.\n\nfrom domino.pipeline import Pipeline, Model, Transformer, Combinator\nfrom fbprophet import Prophet\n\ndag = Pipeline(input_list=['ppc_cost', 'smart_users'])\n\ndag.add_step(step=Transformer(base_var='ppc_cost',\n                             target_variable='ppc_cost_ln',\n                             operation_name='ln'),\n             step_name='transformer1')\n\ndag.add_step(step=Transformer(base_var='smart_users',\n                             target_variable='smart_users_add1',\n                             operation_name='add1'),\n             step_name='transformer2')\n\ndag.add_step(step=Transformer(base_var='smart_users_add1',\n                             target_variable='smart_users_add1_ln',\n                             operation_name='ln'),\n             step_name='transformer3')\n\ndag.add_step(step=Model(model=Prophet(),\n                  target_variable='paid_visits',\n                  explanatory_variables=['ppc_cost_ln']),\n         step_name='model1')\n\ndag.add_step(step=Model(model=Prophet(),\n                  target_variable='non_paid_visits',\n                  explanatory_variables=['smart_users_add1_ln']),\n             step_name='model2')\n\ndag.add_step(step=Combinator(base_var1='paid_visits',\n                            base_var2='non_paid_visits',\n                            target_variable='all_visits',\n                            operation_name='add'),\n             step_name='combination')\n\n\nNow we can use the dag object as a single model.\n\ndag.fit(df_train)\npredictions = dag.predict(df_test)\n\n\nTo understand what’s happening inside the DAG, we implemented two methods of calculating MAPE on every step:\n\ndag.calculate_mape_for_models(x=df_test)\ndag.calculate_mape_for_dag(x=df_test)\n\n\nThey both return dictionaries of model-MAPE pairs. The calculate_mape_for_model method checks each model separately, and calculate_mape_for_dag takes into account the errors propagating from preceding steps. These are examples of results:\n\nmodels_mape = {\n   'transformer1': 0.0,\n   'transformer2': 0.0,\n   'transformer3': 0.0,\n   'model1': 0.1,\n   'model2': 0.05,\n   'combination': 0.0\n}\n\ndag_mape = {\n   'transformer1': 0.0,\n   'transformer2': 0.0,\n   'transformer3': 0.0,\n   'model1': 0.1,\n   'model2': 0.05,\n   'combination': 0.09\n}\n\n\nNote that the combination step has MAPE equal to zero in models_mape and a positive one in dag_mape. That’s because it does not generate any error, as it’s an arithmetic operation, but it can propagate errors from previous steps.\nLast but not least, there is an explainability method calculate_variable_impact that helps to evaluate how changes in initial inputs impact the subsequent steps in the graph. For example, we can check what is going to happen if we decrease PPC costs by 10% and increase the number of Smart users by 5%.\n\ndag.calculate_variable_impact(x=df_test,\n                              variables_list=['ppc_cost', 'smart_users'],\n                              variables_multiplier_list=[0.9, 1.05],\n                              diff_type='relative')\n\n\nThe percent change will be calculated on every node, i.e. smart_users_add1_ln, paid_visits, and all_visits. We will be able to evaluate how such changes affect not only the GMV, but also all intermediary KPIs.\nFacebook’s Prophet\nHaving tested various modelling techniques, we chose the forecasting procedure offered by Facebook’s Prophet library (https://facebook.github.io/prophet/). It uses a decomposable Bayesian time series model with three main components: seasonalities, trends and errors, hence it works well for our time series that have strong seasonal effects. Moreover, the Prophet model is robust to outliers and shifts in the trend, which proved very useful in some models. Mostly, however, we assumed the trend to be flat. The piecewise linear trend explains some of the variance of the dependent variable which could otherwise be explained by the regressor variables/inputs. Given that the purpose of the tool is to allow testing scenarios with different values of the inputs, we needed our models to estimate the relationship between the explanatory variables and the dependent variable, but accounting for seasonality, holidays and some external events (e.g. COVID) only. The graphs below show an example of the forecast components generated by the Prophet, with linear trend, effect of holidays, weekly and yearly seasonality, as well as effect of explanatory variables.\n\n\n\nAdding our domain knowledge about the analysed time series (e.g. calendar effects) and possibility of tuning the parameters of the model (e.g. the strength of the weekly seasonality effect) makes Prophet a perfect fit for our purpose.\nModelling framework\nBoth the patterns and relationships between the variables change slightly over time, hence it would be naive to expect that once all the models are tuned they will give best forecasts forever. It is therefore necessary to analyze the results every time the forecasts are made (every month) and apply necessary tweaks to the models.\nAs in every machine learning project, we split our time series into:\nTraining dataset: the actual dataset that we use to train the model, i.e. the model learns from this data.\nValidation dataset: the sample of data used to provide an unbiased evaluation of a model fit on the training dataset while tuning model hyperparameters; in time series, this is a time period following the training dataset time series.\nTest dataset: the sample of data used to provide an unbiased evaluation of a final model fit on the training dataset. In time series, this is a time period following the validation dataset time series.\nThe COVID-19 pandemic wreaked havoc on our time series, not only changing the patterns temporarily, but often introducing shifts in the trend. Given that we worked on the tool during summer 2020, we were forced to use quite a non-standard approach to hyperparameter tuning and model testing (e.g. to maximize the length of the training period, so that it includes at least a month of the data showing post-COVID comeback to a new normal).\nIn the long run, we expect that the process will stabilize and we’ll be able to conduct the following adjustment procedure each month: training all the models using the parameters set in the previous month, testing them on the last 2 months of observed data, evaluating monthly and daily MAPE. When forecast errors in GMV prediction or any intermediate model are too large, we scrutinize the graphs of observed vs forecasted values. It is also helpful to compare the predictions vs observed values for the same period of the previous year. This step allows us to verify whether there are any seasonalities or patterns that were not detected by the tuned model. We can fine-tune the models either manually, or using the automatic hyperparameter optimization framework.\nHyperparameter tuning using Optuna (https://optuna.org/), half a year’s worth of data and expanding window approach (see visualisation below). This means that we will fine-tune our models using 6 sets of validation datasets, each consisting of 1, 2, 3, 4, 5 and 6 months. The Optuna framework will suggest parameters that minimize the average of MAPE over these datasets.\nTesting the tuned models on 2 last months of observed data, measuring MAPE on the forecasted vs observed values of GMV, as well as on all intermediate models.\nIf any of the MAPE is not satisfactory, again scrutinizing the graphs and fine-tuning the models manually.\nOnce we are satisfied with the results, we always check if the changes made to the models do not result in some explanatory variables having unexpected signs of impact on GMV.\nDespite the changes in time series, we are expecting that in the long run fewer and fewer tweaks to the models will be necessary, and less work will be required from the analysts to maintain the tool.\nUser interface\nTo make the model easily accessible by business users, an interactive application was prepared. The user has default inputs set for upcoming months. They can change their values and get model predictions by clicking the “RUN SCENARIO” button. The predictions can be seen in daily, weekly and monthly granularities. If the user chooses to, they can export the predictions in CSV format. You can find an anonymised print screen of the tool below.\n\nSummary\nAs a result of the project, we developed a solution providing incredible business value. The main features of the tool are:\nGreat forecast accuracy - we managed to get below 2% MAPE\nStability - the structure of the model remains the same and the inputs have the same impact direction over time\nResponsiveness - the forecasts change with changes in the business inputs\nInterpretation - though the model is not intrinsically interpretable, we developed methods to check how well it works\nInteractive UI - stakeholders can experiment with various business scenarios online\nDomino proved its effectiveness in hard and demanding times while giving us a lot of practical knowledge related to modeling of such a complex business metric. And, we already started using these lessons in new upcoming projects.","guid":"https://blog.allegro.tech/2021/05/domino-financial-forecasting-in-the-age-of-global-pandemic.html","categories":["tech"],"isoDate":"2021-05-20T22:00:00.000Z","thumbnail":"images/post-headers/default.jpg"},{"title":"My first days at Allegro","link":"https://blog.allegro.tech/2021/05/my-first-days-at-allegro.html","pubDate":"Tue, 11 May 2021 00:00:00 +0200","authors":{"author":[{"name":["Anna Stanisławska"],"photo":["https://blog.allegro.tech/img/authors/anna.stanislawska.jpg"],"url":["https://blog.allegro.tech/authors/anna.stanislawska"]}]},"content":"<p>The beginnings in a new job can be really tough, especially in such uncertain times as the pandemic. Remote onboarding, Zoom meetings, inability to talk\nface-to-face — it’s a big test, especially if you’re switching industries. My story is quite similar: in July 2020 I started a three-month internship at Allegro\nto train for product management. When the internship ended, I was offered to stay permanently as Junior Product Manager. I’d like to describe my beginnings at\nAllegro. But first — a few words about me.</p>\n\n<h2 id=\"a-few-words-to-begin-with\">A few words to begin with</h2>\n\n<p>I have never imagined that my career path would move towards product management. As far as I remember, I was interested in science, especially chemistry. When I\nwas in high school, I imagined myself working in a lab. However, after a while I understood that my expectations didn’t harmonize with my idea of working in a\nlab. I needed more creativity, independent outlook and impact on projects I would be working on. Later I became interested in marketing and economy. I wondered\nhow to combine all these scientific fields.</p>\n\n<p>So, I decided to study Commodity Science. It is the interdisciplinary field of study that gives a broad perspective of product design, production, marketing,\nquality assurance and management. And that’s all to meet human needs. I decided to take up product management specialization — and I knew that this was the path\nthat I would like to take. And before I graduated, I had the possibility to start working as a product manager in the publishing industry.</p>\n\n<h2 id=\"how-it-started\">How it started</h2>\n\n<p>I came across an internship offer by accident. I was not looking for a new job. I had often used <a href=\"https://allegro.tech\">Allegro</a> to do some shopping, I knew the\nwebsite well. The internship offer was attractive, so I decided to send my application. I decided to take that step despite the fact that the offer indicated a\nthree-month period and I already had a job! After some time I received my internship task. I focused on the biggest trends in the e-commerce industry and I\ncreated a project of a minimal viable product that could be implemented at Allegro using statistics and KPIs. I defined exact goals and real benefits from the\nbuyers’, sellers’ and Allegro’s perspective. Next recruitment stage included two interviews — with my future superiors and an HR business partner. After a few\ndays I got a phone call with the offer — I did it, I got the internship. I decided to accept the offer despite having a permanent job. I knew that the\ninternship could give me a lot of experience in an industry that was unknown to me and which was growing so fast. Allegro is an e-commerce leader in Poland with\nhigh recognizability — and when you want to learn something new, do it with the best.</p>\n\n<h2 id=\"first-steps-into-the-unknown\">First steps into the unknown</h2>\n\n<p>In the beginning, I had two days of remote onboarding. Other interns and I got to know the company structure, we got lots of necessary information on how to\nmove in the corporate world. I learned details of the work in other departments and where to look for the answers in specific situations. Even though the\nonboarding was not held on-premise at headquarters in Poznań as was usual before the pandemic, I had the opportunity to meet other interns and the atmosphere\nwas friendly and supportive. And I felt that despite the fact that we were working in different departments, we were on the same team.</p>\n\n<p>After onboarding, I started working with the team. I was introduced to the secrets of checkout and post-purchase. The biggest challenge to start with? Getting\nto know the team and tasks while sitting at the desk at home. But fortunately everyone was very supportive and they assisted with every issue that I struggled\nwith. Especially my internship mentor — I owe her a lot — gave me tons of very valuable knowledge and I’m thankful that I was able to develop under her\nsupervision. And it was obvious for everyone that I, as the intern, needed more time to recognize all the processes in checkout and post-purchase. These are the\nfinal steps of the purchasing process that combines a lot of Allegro’s parts and at first sight it may seem complicated. But despite being an intern, I got some\nindependence, with support of my superior when I needed it. And after three months I was ready to become a full-time employee. Now I am involved in a big\npost-purchase project (I started working on this project at the beginning of the internship). I cooperate with the development team and the Team Leader, with a\nUX Designer, a UX Researcher and an Analyst. We constantly analyse users’ needs and we implement new functions so that the purchasing process is as easy, fast\nand smooth as possible and that the user can always find what they are looking for.</p>\n\n<p>However, the internship period wasn’t completely hassle-free. Allegro is a very big company that is sometimes overwhelming. At the very beginning, especially\nwhen I had a lot of common tasks with different teams, I felt a little lost when I had to find the proper Product Manager or Team Leader to determine the\ndetails, schedule work and when my knowledge was incomplete. Moreover, the beginnings were a little stressful because my product decisions have a big impact on\nthe shopping experience and our solutions are used by millions of users. I had a lot of stage fright before my first implementation but from month to month I\nthink I became more confident. After a few months it wasn’t a problem to move around anymore, it just had to take some time. Since I became a full-time employee\nI still have the opportunity to receive support from a mentor. It’s a person who is always there to help, answer all questions about the product, process,\nplanning etc. In my case it’s a person from another department so I can get a fresh perspective on my doubts.</p>\n\n<h2 id=\"were-on-the-same-team\">We’re on the same team</h2>\n\n<p>As I mentioned, first days in a new job can be tough, but being at Allegro you don’t need to worry. Allegro is an employee-friendly zone and you can count on\nthe support of more experienced people. Despite the fact that we are working from home, I met great people whom I get along with and I feel good in their\ncompany. Good working relationships and a friendly environment are essential for new employees. We are playing towards one goal — the best shopping experience.</p>\n","contentSnippet":"The beginnings in a new job can be really tough, especially in such uncertain times as the pandemic. Remote onboarding, Zoom meetings, inability to talk\nface-to-face — it’s a big test, especially if you’re switching industries. My story is quite similar: in July 2020 I started a three-month internship at Allegro\nto train for product management. When the internship ended, I was offered to stay permanently as Junior Product Manager. I’d like to describe my beginnings at\nAllegro. But first — a few words about me.\nA few words to begin with\nI have never imagined that my career path would move towards product management. As far as I remember, I was interested in science, especially chemistry. When I\nwas in high school, I imagined myself working in a lab. However, after a while I understood that my expectations didn’t harmonize with my idea of working in a\nlab. I needed more creativity, independent outlook and impact on projects I would be working on. Later I became interested in marketing and economy. I wondered\nhow to combine all these scientific fields.\nSo, I decided to study Commodity Science. It is the interdisciplinary field of study that gives a broad perspective of product design, production, marketing,\nquality assurance and management. And that’s all to meet human needs. I decided to take up product management specialization — and I knew that this was the path\nthat I would like to take. And before I graduated, I had the possibility to start working as a product manager in the publishing industry.\nHow it started\nI came across an internship offer by accident. I was not looking for a new job. I had often used Allegro to do some shopping, I knew the\nwebsite well. The internship offer was attractive, so I decided to send my application. I decided to take that step despite the fact that the offer indicated a\nthree-month period and I already had a job! After some time I received my internship task. I focused on the biggest trends in the e-commerce industry and I\ncreated a project of a minimal viable product that could be implemented at Allegro using statistics and KPIs. I defined exact goals and real benefits from the\nbuyers’, sellers’ and Allegro’s perspective. Next recruitment stage included two interviews — with my future superiors and an HR business partner. After a few\ndays I got a phone call with the offer — I did it, I got the internship. I decided to accept the offer despite having a permanent job. I knew that the\ninternship could give me a lot of experience in an industry that was unknown to me and which was growing so fast. Allegro is an e-commerce leader in Poland with\nhigh recognizability — and when you want to learn something new, do it with the best.\nFirst steps into the unknown\nIn the beginning, I had two days of remote onboarding. Other interns and I got to know the company structure, we got lots of necessary information on how to\nmove in the corporate world. I learned details of the work in other departments and where to look for the answers in specific situations. Even though the\nonboarding was not held on-premise at headquarters in Poznań as was usual before the pandemic, I had the opportunity to meet other interns and the atmosphere\nwas friendly and supportive. And I felt that despite the fact that we were working in different departments, we were on the same team.\nAfter onboarding, I started working with the team. I was introduced to the secrets of checkout and post-purchase. The biggest challenge to start with? Getting\nto know the team and tasks while sitting at the desk at home. But fortunately everyone was very supportive and they assisted with every issue that I struggled\nwith. Especially my internship mentor — I owe her a lot — gave me tons of very valuable knowledge and I’m thankful that I was able to develop under her\nsupervision. And it was obvious for everyone that I, as the intern, needed more time to recognize all the processes in checkout and post-purchase. These are the\nfinal steps of the purchasing process that combines a lot of Allegro’s parts and at first sight it may seem complicated. But despite being an intern, I got some\nindependence, with support of my superior when I needed it. And after three months I was ready to become a full-time employee. Now I am involved in a big\npost-purchase project (I started working on this project at the beginning of the internship). I cooperate with the development team and the Team Leader, with a\nUX Designer, a UX Researcher and an Analyst. We constantly analyse users’ needs and we implement new functions so that the purchasing process is as easy, fast\nand smooth as possible and that the user can always find what they are looking for.\nHowever, the internship period wasn’t completely hassle-free. Allegro is a very big company that is sometimes overwhelming. At the very beginning, especially\nwhen I had a lot of common tasks with different teams, I felt a little lost when I had to find the proper Product Manager or Team Leader to determine the\ndetails, schedule work and when my knowledge was incomplete. Moreover, the beginnings were a little stressful because my product decisions have a big impact on\nthe shopping experience and our solutions are used by millions of users. I had a lot of stage fright before my first implementation but from month to month I\nthink I became more confident. After a few months it wasn’t a problem to move around anymore, it just had to take some time. Since I became a full-time employee\nI still have the opportunity to receive support from a mentor. It’s a person who is always there to help, answer all questions about the product, process,\nplanning etc. In my case it’s a person from another department so I can get a fresh perspective on my doubts.\nWe’re on the same team\nAs I mentioned, first days in a new job can be tough, but being at Allegro you don’t need to worry. Allegro is an employee-friendly zone and you can count on\nthe support of more experienced people. Despite the fact that we are working from home, I met great people whom I get along with and I feel good in their\ncompany. Good working relationships and a friendly environment are essential for new employees. We are playing towards one goal — the best shopping experience.","guid":"https://blog.allegro.tech/2021/05/my-first-days-at-allegro.html","categories":["tech"],"isoDate":"2021-05-10T22:00:00.000Z","thumbnail":"images/post-headers/default.jpg"}],"jobs":[{"id":"743999757285363","name":"Infrastructure Administrator (Azure)","uuid":"6acee10a-2630-4069-a07b-a3f2ffe8d597","refNumber":"REF2720P","company":{"identifier":"Allegro","name":"Allegro"},"releasedDate":"2021-06-23T14:48:50.000Z","location":{"city":"Warszawa, Poznań","country":"pl","remote":false},"industry":{"id":"internet","label":"Internet"},"department":{"id":"2572787","label":"IT - Technical Platform"},"function":{"id":"information_technology","label":"Information Technology"},"typeOfEmployment":{"label":"Full-time"},"experienceLevel":{"id":"mid_senior_level","label":"Mid-Senior Level"},"customField":[{"fieldId":"58c1575ee4b01d4b19ddf797","fieldLabel":"Kraków","valueId":"bcab9d4f-8624-4d85-8e3d-dbf12239b972","valueLabel":"Nie"},{"fieldId":"58c158e9e4b0614667d5973e","fieldLabel":"Więcej lokalizacji","valueId":"3a186e8d-a82c-4955-af81-fcef9a63928a","valueLabel":"Tak"},{"fieldId":"58c1576ee4b0614667d59732","fieldLabel":"Toruń","valueId":"a6765624-e047-4a26-9481-9621086d8b96","valueLabel":"Nie"},{"fieldId":"58c156e6e4b01d4b19ddf793","fieldLabel":"Warszawa","valueId":"6a428533-1586-4372-89ec-65fb45366363","valueLabel":"Tak"},{"fieldId":"58c15608e4b01d4b19ddf790","fieldLabel":"Proces rekrutacji","valueId":"c807eec2-8a53-4b55-b7c5-c03180f2059b","valueLabel":"IT Allegro"},{"fieldId":"COUNTRY","fieldLabel":"Country","valueId":"pl","valueLabel":"Poland"},{"fieldId":"58c15788e4b0614667d59733","fieldLabel":"Błonie","valueId":"25f6cb8c-81b3-434a-93ec-6dc851d5808d","valueLabel":"Nie"},{"fieldId":"58c15798e4b01d4b19ddf79b","fieldLabel":"Wrocław","valueId":"31873284-1e97-427d-8918-6ce504344351","valueLabel":"Nie"},{"fieldId":"58c13159e4b01d4b19ddf729","fieldLabel":"Department","valueId":"2572787","valueLabel":"IT - Technical Platform"},{"fieldId":"58c13159e4b01d4b19ddf728","fieldLabel":"Brands","valueId":"bbfbf1d0-5f28-419f-8570-2aea8b200e1b","valueLabel":"eBilet Polska sp. z o.o."},{"fieldId":"58c156b9e4b01d4b19ddf792","fieldLabel":"Poznań","valueId":"ac20917b-cb6a-4280-aff3-4fae2532a33e","valueLabel":"Tak"}],"ref":"https://api.smartrecruiters.com/v1/companies/allegro/postings/743999757285363","creator":{"name":"Katarzyna Faber"},"language":{"code":"pl","label":"Polish","labelNative":"polski"}},{"id":"743999757177457","name":"Research Engineer - Machine Learning","uuid":"bfd50ecc-c2a0-41f1-ab31-444f2c6c3370","refNumber":"REF1694Q","company":{"identifier":"Allegro","name":"Allegro"},"releasedDate":"2021-06-23T11:24:43.000Z","location":{"city":"Warszawa, Kraków, Poznań, Toruń, Wrocław","country":"pl","remote":false},"industry":{"id":"internet","label":"Internet"},"department":{"id":"2572821","label":"IT - Machine Learning"},"function":{"id":"information_technology","label":"Information Technology"},"typeOfEmployment":{"label":"Full-time"},"experienceLevel":{"id":"mid_senior_level","label":"Mid-Senior Level"},"customField":[{"fieldId":"58c1575ee4b01d4b19ddf797","fieldLabel":"Kraków","valueId":"2cfcfcc1-da44-43f7-8eaa-3907faf2797c","valueLabel":"Tak"},{"fieldId":"58c158e9e4b0614667d5973e","fieldLabel":"Więcej lokalizacji","valueId":"3a186e8d-a82c-4955-af81-fcef9a63928a","valueLabel":"Tak"},{"fieldId":"58c1576ee4b0614667d59732","fieldLabel":"Toruń","valueId":"987e8884-bad1-4a8f-b149-99e4184cc221","valueLabel":"Tak"},{"fieldId":"58c156e6e4b01d4b19ddf793","fieldLabel":"Warszawa","valueId":"6a428533-1586-4372-89ec-65fb45366363","valueLabel":"Tak"},{"fieldId":"58c15608e4b01d4b19ddf790","fieldLabel":"Proces rekrutacji","valueId":"c807eec2-8a53-4b55-b7c5-c03180f2059b","valueLabel":"IT Allegro"},{"fieldId":"COUNTRY","fieldLabel":"Country","valueId":"pl","valueLabel":"Poland"},{"fieldId":"58c15788e4b0614667d59733","fieldLabel":"Błonie","valueId":"25f6cb8c-81b3-434a-93ec-6dc851d5808d","valueLabel":"Nie"},{"fieldId":"58c15798e4b01d4b19ddf79b","fieldLabel":"Wrocław","valueId":"31873284-1e97-427d-8918-6ce504344351","valueLabel":"Nie"},{"fieldId":"58c13159e4b01d4b19ddf729","fieldLabel":"Department","valueId":"2572821","valueLabel":"IT - Machine Learning"},{"fieldId":"58c13159e4b01d4b19ddf728","fieldLabel":"Brands","valueId":"4ccb4fab-6c3f-4ed0-9140-8533fe17447f","valueLabel":"Allegro.pl sp. z o.o."},{"fieldId":"58c156b9e4b01d4b19ddf792","fieldLabel":"Poznań","valueId":"ac20917b-cb6a-4280-aff3-4fae2532a33e","valueLabel":"Tak"},{"fieldId":"5cdab2c84cedfd0006e7758c","fieldLabel":"Key words","valueLabel":"data scientist, NLP, ML, uczenie maszynowe, sieci neuronowe, modele lasów drzew decyzyjnych, modele Bayes’owskie, information retrieval, język naturalny, szeregi czasowe, dane, python, statystyka"}],"ref":"https://api.smartrecruiters.com/v1/companies/allegro/postings/743999757177457","creator":{"name":"Maciej Matwiejczyk"},"language":{"code":"pl","label":"Polish","labelNative":"polski"}},{"id":"743999757164488","name":"Front-End Software Engineer - Marketing & Advertising & Allegro Biznes","uuid":"2be2504b-028c-4243-8ad3-d3cb9ed1b277","refNumber":"REF2760S","company":{"identifier":"Allegro","name":"Allegro"},"releasedDate":"2021-06-23T09:56:49.000Z","location":{"city":"Kraków, Warszawa","country":"pl","remote":false},"industry":{"id":"internet","label":"Internet"},"department":{"id":"2572770","label":"IT - Software Development"},"function":{"id":"information_technology","label":"Information Technology"},"typeOfEmployment":{"label":"Full-time"},"experienceLevel":{"id":"mid_senior_level","label":"Mid-Senior Level"},"customField":[{"fieldId":"58c15608e4b01d4b19ddf790","fieldLabel":"Proces rekrutacji","valueId":"c807eec2-8a53-4b55-b7c5-c03180f2059b","valueLabel":"IT Allegro"},{"fieldId":"58c1575ee4b01d4b19ddf797","fieldLabel":"Kraków","valueId":"2cfcfcc1-da44-43f7-8eaa-3907faf2797c","valueLabel":"Tak"},{"fieldId":"COUNTRY","fieldLabel":"Country","valueId":"pl","valueLabel":"Poland"},{"fieldId":"58c13159e4b01d4b19ddf729","fieldLabel":"Department","valueId":"2572770","valueLabel":"IT - Software Development"},{"fieldId":"58c158e9e4b0614667d5973e","fieldLabel":"Więcej lokalizacji","valueId":"3a186e8d-a82c-4955-af81-fcef9a63928a","valueLabel":"Tak"},{"fieldId":"58c13159e4b01d4b19ddf728","fieldLabel":"Brands","valueId":"4ccb4fab-6c3f-4ed0-9140-8533fe17447f","valueLabel":"Allegro.pl sp. z o.o."},{"fieldId":"58c156e6e4b01d4b19ddf793","fieldLabel":"Warszawa","valueId":"6a428533-1586-4372-89ec-65fb45366363","valueLabel":"Tak"}],"ref":"https://api.smartrecruiters.com/v1/companies/allegro/postings/743999757164488","language":{"code":"pl","label":"Polish","labelNative":"polski"}},{"id":"743999755698336","name":"Front-End Software Engineer - Delivery Experience","uuid":"e4eae041-3793-487d-ada7-36b14225c188","refNumber":"REF2805D","company":{"identifier":"Allegro","name":"Allegro"},"releasedDate":"2021-06-18T09:37:17.000Z","location":{"city":"Warszawa, Poznań","country":"pl","remote":false},"industry":{"id":"internet","label":"Internet"},"department":{"id":"2572770","label":"IT - Software Development"},"function":{"id":"engineering","label":"Engineering"},"typeOfEmployment":{"label":"Full-time"},"experienceLevel":{"id":"mid_senior_level","label":"Mid-Senior Level"},"customField":[{"fieldId":"58c15608e4b01d4b19ddf790","fieldLabel":"Proces rekrutacji","valueId":"c807eec2-8a53-4b55-b7c5-c03180f2059b","valueLabel":"IT Allegro"},{"fieldId":"COUNTRY","fieldLabel":"Country","valueId":"pl","valueLabel":"Poland"},{"fieldId":"58c13159e4b01d4b19ddf729","fieldLabel":"Department","valueId":"2572770","valueLabel":"IT - Software Development"},{"fieldId":"58c13159e4b01d4b19ddf728","fieldLabel":"Brands","valueId":"4ccb4fab-6c3f-4ed0-9140-8533fe17447f","valueLabel":"Allegro.pl sp. z o.o."}],"ref":"https://api.smartrecruiters.com/v1/companies/allegro/postings/743999755698336","creator":{"name":"Paulina Partyka"},"language":{"code":"pl","label":"Polish","labelNative":"polski"}},{"id":"743999755689061","name":"Team Leader - DAX","uuid":"a41d4d31-2b49-4422-85ab-87107fc48c2b","refNumber":"REF2754A","company":{"identifier":"Allegro","name":"Allegro"},"releasedDate":"2021-06-18T09:18:27.000Z","location":{"city":"Warszawa, Poznań","country":"pl","remote":false},"industry":{"id":"internet","label":"Internet"},"department":{"id":"2572770","label":"IT - Software Development"},"function":{"id":"engineering","label":"Engineering"},"typeOfEmployment":{"label":"Full-time"},"experienceLevel":{"id":"mid_senior_level","label":"Mid-Senior Level"},"customField":[{"fieldId":"58c15608e4b01d4b19ddf790","fieldLabel":"Proces rekrutacji","valueId":"c807eec2-8a53-4b55-b7c5-c03180f2059b","valueLabel":"IT Allegro"},{"fieldId":"COUNTRY","fieldLabel":"Country","valueId":"pl","valueLabel":"Poland"},{"fieldId":"58c13159e4b01d4b19ddf729","fieldLabel":"Department","valueId":"2572770","valueLabel":"IT - Software Development"},{"fieldId":"58c13159e4b01d4b19ddf728","fieldLabel":"Brands","valueId":"4ccb4fab-6c3f-4ed0-9140-8533fe17447f","valueLabel":"Allegro.pl sp. z o.o."}],"ref":"https://api.smartrecruiters.com/v1/companies/allegro/postings/743999755689061","creator":{"name":"Paulina Partyka"},"language":{"code":"pl","label":"Polish","labelNative":"polski"}}],"events":[{"created":1623957759000,"duration":7200000,"id":"278903176","name":"Allegro Tech Live #20: Wydajność Backendu","date_in_series_pattern":false,"status":"upcoming","time":1624982400000,"local_date":"2021-06-29","local_time":"18:00","updated":1624266413000,"utc_offset":7200000,"waitlist_count":0,"yes_rsvp_count":82,"venue":{"id":26906060,"name":"Online event","repinned":false,"country":"","localized_country_name":""},"is_online_event":true,"group":{"created":1425052059000,"name":"allegro Tech","id":18465254,"join_mode":"open","lat":52.2599983215332,"lon":21.020000457763672,"urlname":"allegrotech","who":"Techs","localized_location":"Warsaw, Poland","state":"","country":"pl","region":"en_US","timezone":"Europe/Warsaw"},"link":"https://www.meetup.com/allegrotech/events/278903176/","description":"Allegro Tech Live w 100% zdalna odsłona naszych stacjonarnych meetupów Allegro Tech Talks. Zazwyczaj spotykaliśmy się w naszych biurach, ale tym razem to my zagościmy…","how_to_find_us":"https://www.youtube.com/channel/UC66wC6RBjFk6CuVz7wdjJ5g","visibility":"public","member_pay_fee":false},{"created":1621842668000,"duration":100800000,"id":"278374635","name":"UX Research Confetti","date_in_series_pattern":false,"status":"upcoming","time":1624456800000,"local_date":"2021-06-23","local_time":"16:00","updated":1624270599000,"utc_offset":7200000,"waitlist_count":0,"yes_rsvp_count":58,"venue":{"id":26906060,"name":"Online event","repinned":false,"country":"","localized_country_name":""},"is_online_event":true,"group":{"created":1425052059000,"name":"allegro Tech","id":18465254,"join_mode":"open","lat":52.2599983215332,"lon":21.020000457763672,"urlname":"allegrotech","who":"Techs","localized_location":"Warsaw, Poland","state":"","country":"pl","region":"en_US","timezone":"Europe/Warsaw"},"link":"https://www.meetup.com/allegrotech/events/278374635/","description":"🎉 Niech rozsypie się confetti wiedzy o badaniach UX! 🎉 Szukaliśmy konferencji badawczej UX w Polsce i nie znaleźliśmy… Dlatego łączymy siły z ekspertami z…","visibility":"public","member_pay_fee":false},{"created":1622474681000,"duration":5400000,"id":"278528964","name":"Allegro Tech Live Odcinek: #19   Co to znaczy być liderem i jak nim zostać?","date_in_series_pattern":false,"status":"past","time":1623340800000,"local_date":"2021-06-10","local_time":"18:00","updated":1623349290000,"utc_offset":7200000,"waitlist_count":0,"yes_rsvp_count":52,"venue":{"id":26906060,"name":"Online event","repinned":false,"country":"","localized_country_name":""},"is_online_event":true,"group":{"created":1425052059000,"name":"allegro Tech","id":18465254,"join_mode":"open","lat":52.2599983215332,"lon":21.020000457763672,"urlname":"allegrotech","who":"Techs","localized_location":"Warsaw, Poland","state":"","country":"pl","region":"en_US","timezone":"Europe/Warsaw"},"link":"https://www.meetup.com/allegrotech/events/278528964/","description":"Allegro Tech Live to w 100% zdalna odsłona naszych stacjonarnych meetupów Allegro Tech Talks. Zazwyczaj spotykaliśmy się w naszych biurach, ale tym razem to my…","how_to_find_us":" https://www.youtube.com/watch?v=8sLX0ExSq7E","visibility":"public","member_pay_fee":false},{"created":1619620661000,"duration":5400000,"id":"277852879","name":"Allegro Tech Live #18 PM w Allegro, jak do nas dołączyć i czerpać radość z pracy","date_in_series_pattern":false,"status":"past","time":1620921600000,"local_date":"2021-05-13","local_time":"18:00","updated":1620932668000,"utc_offset":7200000,"waitlist_count":0,"yes_rsvp_count":46,"venue":{"id":26906060,"name":"Online event","repinned":false,"country":"","localized_country_name":""},"is_online_event":true,"group":{"created":1425052059000,"name":"allegro Tech","id":18465254,"join_mode":"open","lat":52.2599983215332,"lon":21.020000457763672,"urlname":"allegrotech","who":"Techs","localized_location":"Warsaw, Poland","state":"","country":"pl","region":"en_US","timezone":"Europe/Warsaw"},"link":"https://www.meetup.com/allegrotech/events/277852879/","description":"Allegro Tech Live to w 100% zdalna odsłona naszych stacjonarnych meetupów Allegro Tech Talks. Zazwyczaj spotykaliśmy się w naszych biurach, ale tym razem to my…","how_to_find_us":"https://youtu.be/WNOQJxPKweM","visibility":"public","member_pay_fee":false}],"podcasts":[{"creator":{"name":["Piotr Betkier"]},"title":"Rola architekta w Allegro","link":"https://podcast.allegro.tech/rola_architekta_w_allegro","pubDate":"Wed, 16 Jun 2021 00:00:00 GMT","author":{"name":["Piotr Betkier"]},"enclosure":{"url":"https://www.buzzsprout.com/887914/8712218.mp3","type":"audio/mpeg"},"content":"Od kodowania do tworzenia strategii technicznej... Jak wygląda rola architekta w Allegro? Ile takich osób pracuje w naszej firmie i dlaczego ta rola jest tak różnorodna? Czym jest Andamio i jak rozwijamy naszą platformę – o tym wszystkim opowie Piotr Betkier – Inżynier, Architekt Platformy Technicznej w Allegro oraz twórca piosenek o IT :)","contentSnippet":"Od kodowania do tworzenia strategii technicznej... Jak wygląda rola architekta w Allegro? Ile takich osób pracuje w naszej firmie i dlaczego ta rola jest tak różnorodna? Czym jest Andamio i jak rozwijamy naszą platformę – o tym wszystkim opowie Piotr Betkier – Inżynier, Architekt Platformy Technicznej w Allegro oraz twórca piosenek o IT :)","guid":"https://podcast.allegro.tech/rola_architekta_w_allegro","isoDate":"2021-06-16T00:00:00.000Z","itunes":{"author":"Piotr Betkier","summary":"Od kodowania do tworzenia strategii technicznej... Jak wygląda rola architekta w Allegro? Ile takich osób pracuje w naszej firmie i dlaczego ta rola jest tak różnorodna? Czym jest Andamio i jak rozwijamy naszą platformę – o tym wszystkim opowie Piotr Betkier – Inżynier, Architekt Platformy Technicznej w Allegro oraz twórca piosenek o IT :)","explicit":"false"}},{"creator":{"name":["Piotr Michoński"]},"title":"Infrastruktura Allegro","link":"https://podcast.allegro.tech/infrastruktura_Allegro","pubDate":"Tue, 01 Jun 2021 00:00:00 GMT","author":{"name":["Piotr Michoński"]},"enclosure":{"url":"https://www.buzzsprout.com/887914/8623783-sezon-ii-11-infrastruktura-allegro-piotr-michonski.mp3","type":"audio/mpeg"},"content":"Jak jest zbudowane środowisko uruchomienia aplikacji Allegro? Jak działają serwerownie firmy i ile ich potrzeba, a które elementy Allegro działają w chmurze publicznej? Jak przebiegała transformacja w Allegro i co zmieniało się przez lata? Jak wzrost biznesu wpływa na wielkość infrastruktury i jak infrastruktura Allegro odczuła przyjście pandemii? O tym, a także o rozwoju liderów technologii w Allegro oraz o historii powstania dżingla do naszych podcastów, opowie Piotr Michoński - menadżer Zespołów tworzących infrastrukturę Allegro.","contentSnippet":"Jak jest zbudowane środowisko uruchomienia aplikacji Allegro? Jak działają serwerownie firmy i ile ich potrzeba, a które elementy Allegro działają w chmurze publicznej? Jak przebiegała transformacja w Allegro i co zmieniało się przez lata? Jak wzrost biznesu wpływa na wielkość infrastruktury i jak infrastruktura Allegro odczuła przyjście pandemii? O tym, a także o rozwoju liderów technologii w Allegro oraz o historii powstania dżingla do naszych podcastów, opowie Piotr Michoński - menadżer Zespołów tworzących infrastrukturę Allegro.","guid":"https://podcast.allegro.tech/infrastruktura_Allegro","isoDate":"2021-06-01T00:00:00.000Z","itunes":{"author":"Piotr Michoński","summary":"Jak jest zbudowane środowisko uruchomienia aplikacji Allegro? Jak działają serwerownie firmy i ile ich potrzeba, a które elementy Allegro działają w chmurze publicznej? Jak przebiegała transformacja w Allegro i co zmieniało się przez lata? Jak wzrost biznesu wpływa na wielkość infrastruktury i jak infrastruktura Allegro odczuła przyjście pandemii? O tym, a także o rozwoju liderów technologii w Allegro oraz o historii powstania dżingla do naszych podcastów, opowie Piotr Michoński - menadżer Zespołów tworzących infrastrukturę Allegro.","explicit":"false"}},{"creator":{"name":["Dariusz Eliasz"]},"title":"Praca architekta ekosystemu big data w Allegro","link":"https://podcast.allegro.tech/praca_architekta_ekosystemu_big_data_w_Allegro","pubDate":"Thu, 20 May 2021 00:00:00 GMT","author":{"name":["Dariusz Eliasz"]},"enclosure":{"url":"https://www.buzzsprout.com/887914/8554742-sezon-ii-10-przetwarzanie-danych-w-allegro-dariusz-eliasz.mp3","type":"audio/mpeg"},"content":"Jak wygląda praca architekta ekosystemu big data w Allegro? Jakie zadania realizuje nasz zespół odpowiedzialny za narzędzia i infrastrukturę dla przetwarzania danych? Kiedy możemy mówić o dużych danych i ile petabajtów przetwarza Allegro? Skąd pochodzą dane Allegro i dlaczego jest ich tak dużo oraz z jakiego powodu dopiero teraz przenosimy się do chmury? O tym wszystkim opowie zdobywca statuetki Allegro Tech Hero - Dariusz Eliasz – Team Manager & Platform Architect w Allegro.","contentSnippet":"Jak wygląda praca architekta ekosystemu big data w Allegro? Jakie zadania realizuje nasz zespół odpowiedzialny za narzędzia i infrastrukturę dla przetwarzania danych? Kiedy możemy mówić o dużych danych i ile petabajtów przetwarza Allegro? Skąd pochodzą dane Allegro i dlaczego jest ich tak dużo oraz z jakiego powodu dopiero teraz przenosimy się do chmury? O tym wszystkim opowie zdobywca statuetki Allegro Tech Hero - Dariusz Eliasz – Team Manager & Platform Architect w Allegro.","guid":"https://podcast.allegro.tech/praca_architekta_ekosystemu_big_data_w_Allegro","isoDate":"2021-05-20T00:00:00.000Z","itunes":{"author":"Dariusz Eliasz","summary":"Jak wygląda praca architekta ekosystemu big data w Allegro? Jakie zadania realizuje nasz zespół odpowiedzialny za narzędzia i infrastrukturę dla przetwarzania danych? Kiedy możemy mówić o dużych danych i ile petabajtów przetwarza Allegro? Skąd pochodzą dane Allegro i dlaczego jest ich tak dużo oraz z jakiego powodu dopiero teraz przenosimy się do chmury? O tym wszystkim opowie zdobywca statuetki Allegro Tech Hero - Dariusz Eliasz – Team Manager & Platform Architect w Allegro.","explicit":"false"}},{"creator":{"name":["Bartosz Gałek"]},"title":"Od inżyniera do lidera w Allegro","link":"https://podcast.allegro.tech/od_inzyniera_do_lidera_w_allegro","pubDate":"Thu, 06 May 2021 00:00:00 GMT","author":{"name":["Bartosz Gałek"]},"enclosure":{"url":"https://www.buzzsprout.com/887914/8455586-sezon-ii-9-od-inzyniera-do-lidera-w-allegro-bartosz-galek.mp3","type":"audio/mpeg"},"content":"Czym jest Opbox i jakie wyzwania przed nim stoją? Jak w Allegro angażujemy się w rozwój kultury Open Source? Ile mamy projektów na GitHubie i jak świętujemy Hacktoberfest? W jaki sposób można rozwinąć się od inżyniera do lidera? Na te pytania w najnowszym Allegro Tech Podcast odpowie Bartek Gałek, Team Leader w Allegro.","contentSnippet":"Czym jest Opbox i jakie wyzwania przed nim stoją? Jak w Allegro angażujemy się w rozwój kultury Open Source? Ile mamy projektów na GitHubie i jak świętujemy Hacktoberfest? W jaki sposób można rozwinąć się od inżyniera do lidera? Na te pytania w najnowszym Allegro Tech Podcast odpowie Bartek Gałek, Team Leader w Allegro.","guid":"https://podcast.allegro.tech/od_inzyniera_do_lidera_w_allegro","isoDate":"2021-05-06T00:00:00.000Z","itunes":{"author":"Bartosz Gałek","summary":"Czym jest Opbox i jakie wyzwania przed nim stoją? Jak w Allegro angażujemy się w rozwój kultury Open Source? Ile mamy projektów na GitHubie i jak świętujemy Hacktoberfest? W jaki sposób można rozwinąć się od inżyniera do lidera? Na te pytania w najnowszym Allegro Tech Podcast odpowie Bartek Gałek, Team Leader w Allegro.","explicit":"false"}}]},"__N_SSG":true}