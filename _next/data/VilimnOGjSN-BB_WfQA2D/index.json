{"pageProps":{"posts":[{"title":"Domino - financial forecasting in the age of global pandemic","link":"https://blog.allegro.tech/2021/05/domino-financial-forecasting-in-the-age-of-global-pandemic.html","pubDate":"Fri, 21 May 2021 00:00:00 +0200","authors":{"author":[{"name":["Piotr Gabryś"],"photo":["https://blog.allegro.tech/img/authors/piotr.gabrys.jpg"],"url":["https://blog.allegro.tech/authors/piotr.gabrys"]},{"name":["Julia Bluszcz"],"photo":["https://blog.allegro.tech/img/authors/julia.bluszcz.jpg"],"url":["https://blog.allegro.tech/authors/julia.bluszcz"]},{"name":["Klaudia Walewska-Łubian"],"photo":["https://blog.allegro.tech/img/authors/klaudia.walewska-lubian.jpg"],"url":["https://blog.allegro.tech/authors/klaudia.walewska-lubian"]}]},"content":"<p>Accurate forecasting is key for any successful business. It allows one to set realistic financial goals for the next quarters, evaluate impact of business decisions, and prepare adequate resources for what is coming.</p>\n\n<p>Yet, many companies struggle with efficient and accurate revenue forecasting. For most of them the task still rests on the financial department’s shoulders, performing manual analyses in Excel, lacking data science know-how, and relying on a set of arbitrary assumptions concerning the future. This process is often inefficient and prone to error, making it hard to distinguish trend-based patterns from manual overrides. In the end, the forecasts often turn out to be inaccurate, but it is difficult to diagnose the source of divergence - was it the fault of the model itself, wrong business assumptions, or perhaps unusual circumstances.</p>\n\n<p>In order to overcome this problem, we decided to develop an AI tool which would allow us to forecast Allegro’s GMV (Gross Merchandise Value) based on a set of business inputs defined by the financial team. The purpose of the tool was twofold: to create automatic, monthly forecasts, and to enable creating scenarios with different input values. Examples of such inputs would be the costs of online advertising or the number of users enrolled in the loyalty program (Allegro Smart!).</p>\n\n<blockquote>\n  <p><strong>Disclaimer:</strong> analysis and graphs presented in article are based on artificial data</p>\n</blockquote>\n\n<h2 id=\"approach\">Approach</h2>\n\n<p>We named our project domino after Marvel’s universe foreseer-heroine Domino, but the name reflects also how the model is executed.</p>\n\n<p>The model works as a graph of dependencies where each element is either a cause or effect of other steps (in the technical nomenclature known as <a href=\"https://en.wikipedia.org/wiki/Directed_acyclic_graph\">DAG</a>). Let’s go through an artificial example. Below you can find a diagram of the model. In the training phase we need to provide historical data for every node in the graph, i.e. we need to know actual online advertising spending (<a href=\"https://en.wikipedia.org/wiki/Pay-per-click\">PPC</a>), important holidays, and actual number of paid and organic visits to be able to accurately model the relationship between these variables.<img src=\"/img/articles/2021-05-21-domino-financial-forecasting-in-the-age-of-global-pandemic/01-approach.png\" alt=\"diagram\" /></p>\n\n<p>During the prediction phase, we only need actual data for violet nodes. These nodes usually represent our key business assumptions which are reflected in the company’s yearly budget. Next, our goal is to recreate all the succeeding values. Some of these subsequent steps will be just arithmetic operations like adding, summing, or applying logarithms (yellow and red). Other steps, here represented with blue nodes, may be Machine Learning models (e.g. <a href=\"https://facebook.github.io/prophet/\">Facebook’s Prophet</a>). They can learn non-trivial effects like seasonalities, trends, holidays, and the influence of the preceding yellow nodes (i.e. explanatory variables).</p>\n\n<p>You may wonder why we bothered with creation of such a complex graph of dependencies instead of making a single model taking all the business inputs as explanatory variables and returning future GMV values as the output. It’s what we want to model in the end, right? Why are we concerned with reconstructing the values of organic or paid traffic along the way? Well, we found out that following business logic yields much better results than inserting all the inputs into a huge single pot. Not only were the results less accurate in the latter case, but also the impact of explanatory variables on GMV was often not clearly distinguishable or even misleading. Instead, we recreated the business scheme by creating and training specialized models to reconstruct all the intermediate steps (e.g. predicting the number of paid visits) and merging their outputs with arithmetic operations or other models. However, remember that this approach comes at a cost! Using many steps in the modeling process may be both a blessing and a curse, since you need to train and maintain multiple models simultaneously.</p>\n\n<p>One of the biggest advantages of this approach is that it allows us to capture very sophisticated non-linear relationships between inputs and the final target variable (GMV in our case). Following the business logic allows us to verify these non-trivial assumptions at each step.</p>\n\n<p>Another advantage of this approach is that every model can be fitted separately with any model class you want. Having tested a few alternatives, we’ve chosen the Prophet library, but potentially any ML algorithm could be used (e.g. ARIMA, Gradient Boosting Trees, or Artificial Neural Networks).</p>\n\n<p>A disadvantage is that the error in prediction propagates downstream the graph. So, if we make a mistake in a prediction in an early step, it will influence all the models and transformations dependent on it. The issue can be mitigated by creating accurate models at each step of the process.</p>\n\n<p>Another slight disadvantage is that our Domino of models is not intrinsically interpretable (as most of the modern model classes). You have to use some post hoc methods to gather information on how the model does process data.</p>\n\n<h2 id=\"technical-implementation\">Technical implementation</h2>\n\n<p>To develop and iterate over the DAG-type model we had to implement a custom Python framework. It allows for training and running models as well as arithmetic transformations in a predefined order.</p>\n\n<p>The implementation allows us to utilize various model frameworks like Facebook’s Prophet, any Scikit-learn’s regressor, or an Artificial NN. For operational purposes, MAPE (Mean Absolute Percentage Error) can be easily calculated for each model in the graph, as well as for the whole DAG (again, the errors propagate downstream).</p>\n\n<p>Below you can see how the above DAG can be implemented.</p>\n\n<div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"kn\">from</span> <span class=\"nn\">domino.pipeline</span> <span class=\"kn\">import</span> <span class=\"n\">Pipeline</span><span class=\"p\">,</span> <span class=\"n\">Model</span><span class=\"p\">,</span> <span class=\"n\">Transformer</span><span class=\"p\">,</span> <span class=\"n\">Combinator</span>\n<span class=\"kn\">from</span> <span class=\"nn\">fbprophet</span> <span class=\"kn\">import</span> <span class=\"n\">Prophet</span>\n\n<span class=\"n\">dag</span> <span class=\"o\">=</span> <span class=\"n\">Pipeline</span><span class=\"p\">(</span><span class=\"n\">input_list</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"s\">'ppc_cost'</span><span class=\"p\">,</span> <span class=\"s\">'smart_users'</span><span class=\"p\">])</span>\n\n<span class=\"n\">dag</span><span class=\"p\">.</span><span class=\"n\">add_step</span><span class=\"p\">(</span><span class=\"n\">step</span><span class=\"o\">=</span><span class=\"n\">Transformer</span><span class=\"p\">(</span><span class=\"n\">base_var</span><span class=\"o\">=</span><span class=\"s\">'ppc_cost'</span><span class=\"p\">,</span>\n                             <span class=\"n\">target_variable</span><span class=\"o\">=</span><span class=\"s\">'ppc_cost_ln'</span><span class=\"p\">,</span>\n                             <span class=\"n\">operation_name</span><span class=\"o\">=</span><span class=\"s\">'ln'</span><span class=\"p\">),</span>\n             <span class=\"n\">step_name</span><span class=\"o\">=</span><span class=\"s\">'transformer1'</span><span class=\"p\">)</span>\n\n<span class=\"n\">dag</span><span class=\"p\">.</span><span class=\"n\">add_step</span><span class=\"p\">(</span><span class=\"n\">step</span><span class=\"o\">=</span><span class=\"n\">Transformer</span><span class=\"p\">(</span><span class=\"n\">base_var</span><span class=\"o\">=</span><span class=\"s\">'smart_users'</span><span class=\"p\">,</span>\n                             <span class=\"n\">target_variable</span><span class=\"o\">=</span><span class=\"s\">'smart_users_add1'</span><span class=\"p\">,</span>\n                             <span class=\"n\">operation_name</span><span class=\"o\">=</span><span class=\"s\">'add1'</span><span class=\"p\">),</span>\n             <span class=\"n\">step_name</span><span class=\"o\">=</span><span class=\"s\">'transformer2'</span><span class=\"p\">)</span>\n\n<span class=\"n\">dag</span><span class=\"p\">.</span><span class=\"n\">add_step</span><span class=\"p\">(</span><span class=\"n\">step</span><span class=\"o\">=</span><span class=\"n\">Transformer</span><span class=\"p\">(</span><span class=\"n\">base_var</span><span class=\"o\">=</span><span class=\"s\">'smart_users_add1'</span><span class=\"p\">,</span>\n                             <span class=\"n\">target_variable</span><span class=\"o\">=</span><span class=\"s\">'smart_users_add1_ln'</span><span class=\"p\">,</span>\n                             <span class=\"n\">operation_name</span><span class=\"o\">=</span><span class=\"s\">'ln'</span><span class=\"p\">),</span>\n             <span class=\"n\">step_name</span><span class=\"o\">=</span><span class=\"s\">'transformer3'</span><span class=\"p\">)</span>\n\n<span class=\"n\">dag</span><span class=\"p\">.</span><span class=\"n\">add_step</span><span class=\"p\">(</span><span class=\"n\">step</span><span class=\"o\">=</span><span class=\"n\">Model</span><span class=\"p\">(</span><span class=\"n\">model</span><span class=\"o\">=</span><span class=\"n\">Prophet</span><span class=\"p\">(),</span>\n                  <span class=\"n\">target_variable</span><span class=\"o\">=</span><span class=\"s\">'paid_visits'</span><span class=\"p\">,</span>\n                  <span class=\"n\">explanatory_variables</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"s\">'ppc_cost_ln'</span><span class=\"p\">]),</span>\n         <span class=\"n\">step_name</span><span class=\"o\">=</span><span class=\"s\">'model1'</span><span class=\"p\">)</span>\n\n<span class=\"n\">dag</span><span class=\"p\">.</span><span class=\"n\">add_step</span><span class=\"p\">(</span><span class=\"n\">step</span><span class=\"o\">=</span><span class=\"n\">Model</span><span class=\"p\">(</span><span class=\"n\">model</span><span class=\"o\">=</span><span class=\"n\">Prophet</span><span class=\"p\">(),</span>\n                  <span class=\"n\">target_variable</span><span class=\"o\">=</span><span class=\"s\">'non_paid_visits'</span><span class=\"p\">,</span>\n                  <span class=\"n\">explanatory_variables</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"s\">'smart_users_add1_ln'</span><span class=\"p\">]),</span>\n             <span class=\"n\">step_name</span><span class=\"o\">=</span><span class=\"s\">'model2'</span><span class=\"p\">)</span>\n\n<span class=\"n\">dag</span><span class=\"p\">.</span><span class=\"n\">add_step</span><span class=\"p\">(</span><span class=\"n\">step</span><span class=\"o\">=</span><span class=\"n\">Combinator</span><span class=\"p\">(</span><span class=\"n\">base_var1</span><span class=\"o\">=</span><span class=\"s\">'paid_visits'</span><span class=\"p\">,</span>\n                            <span class=\"n\">base_var2</span><span class=\"o\">=</span><span class=\"s\">'non_paid_visits'</span><span class=\"p\">,</span>\n                            <span class=\"n\">target_variable</span><span class=\"o\">=</span><span class=\"s\">'all_visits'</span><span class=\"p\">,</span>\n                            <span class=\"n\">operation_name</span><span class=\"o\">=</span><span class=\"s\">'add'</span><span class=\"p\">),</span>\n             <span class=\"n\">step_name</span><span class=\"o\">=</span><span class=\"s\">'combination'</span><span class=\"p\">)</span>\n</code></pre></div></div>\n\n<p>Now we can use the dag object as a single model.</p>\n\n<div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"n\">dag</span><span class=\"p\">.</span><span class=\"n\">fit</span><span class=\"p\">(</span><span class=\"n\">df_train</span><span class=\"p\">)</span>\n<span class=\"n\">predictions</span> <span class=\"o\">=</span> <span class=\"n\">dag</span><span class=\"p\">.</span><span class=\"n\">predict</span><span class=\"p\">(</span><span class=\"n\">df_test</span><span class=\"p\">)</span>\n</code></pre></div></div>\n\n<p>To understand what’s happening inside the DAG, we implemented two methods of calculating MAPE on every step:</p>\n\n<div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"n\">dag</span><span class=\"p\">.</span><span class=\"n\">calculate_mape_for_models</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"o\">=</span><span class=\"n\">df_test</span><span class=\"p\">)</span>\n<span class=\"n\">dag</span><span class=\"p\">.</span><span class=\"n\">calculate_mape_for_dag</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"o\">=</span><span class=\"n\">df_test</span><span class=\"p\">)</span>\n</code></pre></div></div>\n\n<p>They both return dictionaries of model-MAPE pairs. The calculate_mape_for_model method checks each model separately, and calculate_mape_for_dag takes into account the errors propagating from preceding steps. These are examples of results:</p>\n\n<div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"n\">models_mape</span> <span class=\"o\">=</span> <span class=\"p\">{</span>\n   <span class=\"s\">'transformer1'</span><span class=\"p\">:</span> <span class=\"mf\">0.0</span><span class=\"p\">,</span>\n   <span class=\"s\">'transformer2'</span><span class=\"p\">:</span> <span class=\"mf\">0.0</span><span class=\"p\">,</span>\n   <span class=\"s\">'transformer3'</span><span class=\"p\">:</span> <span class=\"mf\">0.0</span><span class=\"p\">,</span>\n   <span class=\"s\">'model1'</span><span class=\"p\">:</span> <span class=\"mf\">0.1</span><span class=\"p\">,</span>\n   <span class=\"s\">'model2'</span><span class=\"p\">:</span> <span class=\"mf\">0.05</span><span class=\"p\">,</span>\n   <span class=\"s\">'combination'</span><span class=\"p\">:</span> <span class=\"mf\">0.0</span>\n<span class=\"p\">}</span>\n\n<span class=\"n\">dag_mape</span> <span class=\"o\">=</span> <span class=\"p\">{</span>\n   <span class=\"s\">'transformer1'</span><span class=\"p\">:</span> <span class=\"mf\">0.0</span><span class=\"p\">,</span>\n   <span class=\"s\">'transformer2'</span><span class=\"p\">:</span> <span class=\"mf\">0.0</span><span class=\"p\">,</span>\n   <span class=\"s\">'transformer3'</span><span class=\"p\">:</span> <span class=\"mf\">0.0</span><span class=\"p\">,</span>\n   <span class=\"s\">'model1'</span><span class=\"p\">:</span> <span class=\"mf\">0.1</span><span class=\"p\">,</span>\n   <span class=\"s\">'model2'</span><span class=\"p\">:</span> <span class=\"mf\">0.05</span><span class=\"p\">,</span>\n   <span class=\"s\">'combination'</span><span class=\"p\">:</span> <span class=\"mf\">0.09</span>\n<span class=\"p\">}</span>\n</code></pre></div></div>\n\n<p>Note that the combination step has MAPE equal to zero in models_mape and a positive one in dag_mape. That’s because it does not generate any error, as it’s an arithmetic operation, but it can propagate errors from previous steps.</p>\n\n<p>Last but not least, there is an explainability method calculate_variable_impact that helps to evaluate how changes in initial inputs impact the subsequent steps in the graph. For example, we can check what is going to happen if we decrease PPC costs by 10% and increase the number of Smart users by 5%.</p>\n\n<div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"n\">dag</span><span class=\"p\">.</span><span class=\"n\">calculate_variable_impact</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"o\">=</span><span class=\"n\">df_test</span><span class=\"p\">,</span>\n                              <span class=\"n\">variables_list</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"s\">'ppc_cost'</span><span class=\"p\">,</span> <span class=\"s\">'smart_users'</span><span class=\"p\">],</span>\n                              <span class=\"n\">variables_multiplier_list</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"mf\">0.9</span><span class=\"p\">,</span> <span class=\"mf\">1.05</span><span class=\"p\">],</span>\n                              <span class=\"n\">diff_type</span><span class=\"o\">=</span><span class=\"s\">'relative'</span><span class=\"p\">)</span>\n</code></pre></div></div>\n\n<p>The percent change will be calculated on every node, i.e. smart_users_add1_ln, paid_visits, and all_visits. We will be able to evaluate how such changes affect not only the GMV, but also all intermediary KPIs.</p>\n\n<h2 id=\"facebooks-prophet\">Facebook’s Prophet</h2>\n\n<p>Having tested various modelling techniques, we chose the forecasting procedure offered by Facebook’s Prophet library (<a href=\"https://facebook.github.io/prophet/\">https://facebook.github.io/prophet/</a>). It uses a decomposable Bayesian time series model with three main components: seasonalities, trends and errors, hence it works well for our time series that have strong seasonal effects. Moreover, the Prophet model is robust to outliers and shifts in the trend, which proved very useful in some models. Mostly, however, we assumed the trend to be flat. The piecewise linear trend explains some of the variance of the dependent variable which could otherwise be explained by the regressor variables/inputs. Given that the purpose of the tool is to allow testing scenarios with different values of the inputs, we needed our models to estimate the relationship between the explanatory variables and the dependent variable, but accounting for seasonality, holidays and some external events (e.g. COVID) only. The graphs below show an example of the forecast components generated by the Prophet, with linear trend, effect of holidays, weekly and yearly seasonality, as well as effect of explanatory variables.</p>\n\n<p><img src=\"/img/articles/2021-05-21-domino-financial-forecasting-in-the-age-of-global-pandemic/02-chart.png\" alt=\"chart\" />\n<img src=\"/img/articles/2021-05-21-domino-financial-forecasting-in-the-age-of-global-pandemic/03-chart.png\" alt=\"chart\" />\n<img src=\"/img/articles/2021-05-21-domino-financial-forecasting-in-the-age-of-global-pandemic/04-chart.png\" alt=\"chart\" /></p>\n\n<p>Adding our domain knowledge about the analysed time series (e.g. calendar effects) and possibility of tuning the parameters of the model (e.g. the strength of the weekly seasonality effect) makes Prophet a perfect fit for our purpose.</p>\n\n<h2 id=\"modelling-framework\">Modelling framework</h2>\n\n<p>Both the patterns and relationships between the variables change slightly over time, hence it would be naive to expect that once all the models are tuned they will give best forecasts forever. It is therefore necessary to analyze the results every time the forecasts are made (every month) and apply necessary tweaks to the models.</p>\n\n<p>As in every machine learning project, we split our time series into:</p>\n\n<ul>\n  <li>\n    <p>Training dataset: the actual dataset that we use to train the model, i.e. the model learns from this data.</p>\n  </li>\n  <li>\n    <p>Validation dataset: the sample of data used to provide an unbiased evaluation of a model fit on the training dataset while tuning model hyperparameters; in time series, this is a time period following the training dataset time series.</p>\n  </li>\n  <li>\n    <p>Test dataset: the sample of data used to provide an unbiased evaluation of a final model fit on the training dataset. In time series, this is a time period following the validation dataset time series.</p>\n  </li>\n</ul>\n\n<p>The COVID-19 pandemic wreaked havoc on our time series, not only changing the patterns temporarily, but often introducing shifts in the trend. Given that we worked on the tool during summer 2020, we were forced to use quite a non-standard approach to hyperparameter tuning and model testing (e.g. to maximize the length of the training period, so that it includes at least a month of the data showing post-COVID comeback to a new normal).</p>\n\n<p>In the long run, we expect that the process will stabilize and we’ll be able to conduct the following adjustment procedure each month: training all the models using the parameters set in the previous month, testing them on the last 2 months of observed data, evaluating monthly and daily MAPE. When forecast errors in GMV prediction or any intermediate model are too large, we scrutinize the graphs of observed vs forecasted values. It is also helpful to compare the predictions vs observed values for the same period of the previous year. This step allows us to verify whether there are any seasonalities or patterns that were not detected by the tuned model. We can fine-tune the models either manually, or using the automatic hyperparameter optimization framework.</p>\n\n<ul>\n  <li>\n    <p>Hyperparameter tuning using Optuna (<a href=\"https://optuna.org/\">https://optuna.org/</a>), half a year’s worth of data and expanding window approach (see visualisation below). This means that we will fine-tune our models using 6 sets of validation datasets, each consisting of 1, 2, 3, 4, 5 and 6 months. The Optuna framework will suggest parameters that minimize the average of MAPE over these datasets.</p>\n  </li>\n  <li>\n    <p>Testing the tuned models on 2 last months of observed data, measuring MAPE on the forecasted vs observed values of GMV, as well as on all intermediate models.</p>\n  </li>\n  <li>\n    <p>If any of the MAPE is not satisfactory, again scrutinizing the graphs and fine-tuning the models manually.</p>\n  </li>\n</ul>\n\n<p>Once we are satisfied with the results, we always check if the changes made to the models do not result in some explanatory variables having unexpected signs of impact on GMV.</p>\n\n<p>Despite the changes in time series, we are expecting that in the long run fewer and fewer tweaks to the models will be necessary, and less work will be required from the analysts to maintain the tool.</p>\n\n<h2 id=\"user-interface\">User interface</h2>\n\n<p>To make the model easily accessible by business users, an interactive application was prepared. The user has default inputs set for upcoming months. They can change their values and get model predictions by clicking the “RUN SCENARIO” button. The predictions can be seen in daily, weekly and monthly granularities. If the user chooses to, they can export the predictions in CSV format. You can find an anonymised print screen of the tool below.</p>\n\n<p><img src=\"/img/articles/2021-05-21-domino-financial-forecasting-in-the-age-of-global-pandemic/05-domino.png\" alt=\"Domino UI\" /></p>\n\n<h2 id=\"summary\">Summary</h2>\n\n<p>As a result of the project, we developed a solution providing incredible business value. The main features of the tool are:</p>\n\n<ul>\n  <li>\n    <p>Great forecast accuracy - we managed to get below 2% MAPE</p>\n  </li>\n  <li>\n    <p>Stability - the structure of the model remains the same and the inputs have the same impact direction over time</p>\n  </li>\n  <li>\n    <p>Responsiveness - the forecasts change with changes in the business inputs</p>\n  </li>\n  <li>\n    <p>Interpretation - though the model is not intrinsically interpretable, we developed methods to check how well it works</p>\n  </li>\n  <li>\n    <p>Interactive UI - stakeholders can experiment with various business scenarios online</p>\n  </li>\n</ul>\n\n<p>Domino proved its effectiveness in hard and demanding times while giving us a lot of practical knowledge related to modeling of such a complex business metric. And, we already started using these lessons in new upcoming projects.</p>\n","contentSnippet":"Accurate forecasting is key for any successful business. It allows one to set realistic financial goals for the next quarters, evaluate impact of business decisions, and prepare adequate resources for what is coming.\nYet, many companies struggle with efficient and accurate revenue forecasting. For most of them the task still rests on the financial department’s shoulders, performing manual analyses in Excel, lacking data science know-how, and relying on a set of arbitrary assumptions concerning the future. This process is often inefficient and prone to error, making it hard to distinguish trend-based patterns from manual overrides. In the end, the forecasts often turn out to be inaccurate, but it is difficult to diagnose the source of divergence - was it the fault of the model itself, wrong business assumptions, or perhaps unusual circumstances.\nIn order to overcome this problem, we decided to develop an AI tool which would allow us to forecast Allegro’s GMV (Gross Merchandise Value) based on a set of business inputs defined by the financial team. The purpose of the tool was twofold: to create automatic, monthly forecasts, and to enable creating scenarios with different input values. Examples of such inputs would be the costs of online advertising or the number of users enrolled in the loyalty program (Allegro Smart!).\nDisclaimer: analysis and graphs presented in article are based on artificial data\nApproach\nWe named our project domino after Marvel’s universe foreseer-heroine Domino, but the name reflects also how the model is executed.\nThe model works as a graph of dependencies where each element is either a cause or effect of other steps (in the technical nomenclature known as DAG). Let’s go through an artificial example. Below you can find a diagram of the model. In the training phase we need to provide historical data for every node in the graph, i.e. we need to know actual online advertising spending (PPC), important holidays, and actual number of paid and organic visits to be able to accurately model the relationship between these variables.\nDuring the prediction phase, we only need actual data for violet nodes. These nodes usually represent our key business assumptions which are reflected in the company’s yearly budget. Next, our goal is to recreate all the succeeding values. Some of these subsequent steps will be just arithmetic operations like adding, summing, or applying logarithms (yellow and red). Other steps, here represented with blue nodes, may be Machine Learning models (e.g. Facebook’s Prophet). They can learn non-trivial effects like seasonalities, trends, holidays, and the influence of the preceding yellow nodes (i.e. explanatory variables).\nYou may wonder why we bothered with creation of such a complex graph of dependencies instead of making a single model taking all the business inputs as explanatory variables and returning future GMV values as the output. It’s what we want to model in the end, right? Why are we concerned with reconstructing the values of organic or paid traffic along the way? Well, we found out that following business logic yields much better results than inserting all the inputs into a huge single pot. Not only were the results less accurate in the latter case, but also the impact of explanatory variables on GMV was often not clearly distinguishable or even misleading. Instead, we recreated the business scheme by creating and training specialized models to reconstruct all the intermediate steps (e.g. predicting the number of paid visits) and merging their outputs with arithmetic operations or other models. However, remember that this approach comes at a cost! Using many steps in the modeling process may be both a blessing and a curse, since you need to train and maintain multiple models simultaneously.\nOne of the biggest advantages of this approach is that it allows us to capture very sophisticated non-linear relationships between inputs and the final target variable (GMV in our case). Following the business logic allows us to verify these non-trivial assumptions at each step.\nAnother advantage of this approach is that every model can be fitted separately with any model class you want. Having tested a few alternatives, we’ve chosen the Prophet library, but potentially any ML algorithm could be used (e.g. ARIMA, Gradient Boosting Trees, or Artificial Neural Networks).\nA disadvantage is that the error in prediction propagates downstream the graph. So, if we make a mistake in a prediction in an early step, it will influence all the models and transformations dependent on it. The issue can be mitigated by creating accurate models at each step of the process.\nAnother slight disadvantage is that our Domino of models is not intrinsically interpretable (as most of the modern model classes). You have to use some post hoc methods to gather information on how the model does process data.\nTechnical implementation\nTo develop and iterate over the DAG-type model we had to implement a custom Python framework. It allows for training and running models as well as arithmetic transformations in a predefined order.\nThe implementation allows us to utilize various model frameworks like Facebook’s Prophet, any Scikit-learn’s regressor, or an Artificial NN. For operational purposes, MAPE (Mean Absolute Percentage Error) can be easily calculated for each model in the graph, as well as for the whole DAG (again, the errors propagate downstream).\nBelow you can see how the above DAG can be implemented.\n\nfrom domino.pipeline import Pipeline, Model, Transformer, Combinator\nfrom fbprophet import Prophet\n\ndag = Pipeline(input_list=['ppc_cost', 'smart_users'])\n\ndag.add_step(step=Transformer(base_var='ppc_cost',\n                             target_variable='ppc_cost_ln',\n                             operation_name='ln'),\n             step_name='transformer1')\n\ndag.add_step(step=Transformer(base_var='smart_users',\n                             target_variable='smart_users_add1',\n                             operation_name='add1'),\n             step_name='transformer2')\n\ndag.add_step(step=Transformer(base_var='smart_users_add1',\n                             target_variable='smart_users_add1_ln',\n                             operation_name='ln'),\n             step_name='transformer3')\n\ndag.add_step(step=Model(model=Prophet(),\n                  target_variable='paid_visits',\n                  explanatory_variables=['ppc_cost_ln']),\n         step_name='model1')\n\ndag.add_step(step=Model(model=Prophet(),\n                  target_variable='non_paid_visits',\n                  explanatory_variables=['smart_users_add1_ln']),\n             step_name='model2')\n\ndag.add_step(step=Combinator(base_var1='paid_visits',\n                            base_var2='non_paid_visits',\n                            target_variable='all_visits',\n                            operation_name='add'),\n             step_name='combination')\n\n\nNow we can use the dag object as a single model.\n\ndag.fit(df_train)\npredictions = dag.predict(df_test)\n\n\nTo understand what’s happening inside the DAG, we implemented two methods of calculating MAPE on every step:\n\ndag.calculate_mape_for_models(x=df_test)\ndag.calculate_mape_for_dag(x=df_test)\n\n\nThey both return dictionaries of model-MAPE pairs. The calculate_mape_for_model method checks each model separately, and calculate_mape_for_dag takes into account the errors propagating from preceding steps. These are examples of results:\n\nmodels_mape = {\n   'transformer1': 0.0,\n   'transformer2': 0.0,\n   'transformer3': 0.0,\n   'model1': 0.1,\n   'model2': 0.05,\n   'combination': 0.0\n}\n\ndag_mape = {\n   'transformer1': 0.0,\n   'transformer2': 0.0,\n   'transformer3': 0.0,\n   'model1': 0.1,\n   'model2': 0.05,\n   'combination': 0.09\n}\n\n\nNote that the combination step has MAPE equal to zero in models_mape and a positive one in dag_mape. That’s because it does not generate any error, as it’s an arithmetic operation, but it can propagate errors from previous steps.\nLast but not least, there is an explainability method calculate_variable_impact that helps to evaluate how changes in initial inputs impact the subsequent steps in the graph. For example, we can check what is going to happen if we decrease PPC costs by 10% and increase the number of Smart users by 5%.\n\ndag.calculate_variable_impact(x=df_test,\n                              variables_list=['ppc_cost', 'smart_users'],\n                              variables_multiplier_list=[0.9, 1.05],\n                              diff_type='relative')\n\n\nThe percent change will be calculated on every node, i.e. smart_users_add1_ln, paid_visits, and all_visits. We will be able to evaluate how such changes affect not only the GMV, but also all intermediary KPIs.\nFacebook’s Prophet\nHaving tested various modelling techniques, we chose the forecasting procedure offered by Facebook’s Prophet library (https://facebook.github.io/prophet/). It uses a decomposable Bayesian time series model with three main components: seasonalities, trends and errors, hence it works well for our time series that have strong seasonal effects. Moreover, the Prophet model is robust to outliers and shifts in the trend, which proved very useful in some models. Mostly, however, we assumed the trend to be flat. The piecewise linear trend explains some of the variance of the dependent variable which could otherwise be explained by the regressor variables/inputs. Given that the purpose of the tool is to allow testing scenarios with different values of the inputs, we needed our models to estimate the relationship between the explanatory variables and the dependent variable, but accounting for seasonality, holidays and some external events (e.g. COVID) only. The graphs below show an example of the forecast components generated by the Prophet, with linear trend, effect of holidays, weekly and yearly seasonality, as well as effect of explanatory variables.\n\n\n\nAdding our domain knowledge about the analysed time series (e.g. calendar effects) and possibility of tuning the parameters of the model (e.g. the strength of the weekly seasonality effect) makes Prophet a perfect fit for our purpose.\nModelling framework\nBoth the patterns and relationships between the variables change slightly over time, hence it would be naive to expect that once all the models are tuned they will give best forecasts forever. It is therefore necessary to analyze the results every time the forecasts are made (every month) and apply necessary tweaks to the models.\nAs in every machine learning project, we split our time series into:\nTraining dataset: the actual dataset that we use to train the model, i.e. the model learns from this data.\nValidation dataset: the sample of data used to provide an unbiased evaluation of a model fit on the training dataset while tuning model hyperparameters; in time series, this is a time period following the training dataset time series.\nTest dataset: the sample of data used to provide an unbiased evaluation of a final model fit on the training dataset. In time series, this is a time period following the validation dataset time series.\nThe COVID-19 pandemic wreaked havoc on our time series, not only changing the patterns temporarily, but often introducing shifts in the trend. Given that we worked on the tool during summer 2020, we were forced to use quite a non-standard approach to hyperparameter tuning and model testing (e.g. to maximize the length of the training period, so that it includes at least a month of the data showing post-COVID comeback to a new normal).\nIn the long run, we expect that the process will stabilize and we’ll be able to conduct the following adjustment procedure each month: training all the models using the parameters set in the previous month, testing them on the last 2 months of observed data, evaluating monthly and daily MAPE. When forecast errors in GMV prediction or any intermediate model are too large, we scrutinize the graphs of observed vs forecasted values. It is also helpful to compare the predictions vs observed values for the same period of the previous year. This step allows us to verify whether there are any seasonalities or patterns that were not detected by the tuned model. We can fine-tune the models either manually, or using the automatic hyperparameter optimization framework.\nHyperparameter tuning using Optuna (https://optuna.org/), half a year’s worth of data and expanding window approach (see visualisation below). This means that we will fine-tune our models using 6 sets of validation datasets, each consisting of 1, 2, 3, 4, 5 and 6 months. The Optuna framework will suggest parameters that minimize the average of MAPE over these datasets.\nTesting the tuned models on 2 last months of observed data, measuring MAPE on the forecasted vs observed values of GMV, as well as on all intermediate models.\nIf any of the MAPE is not satisfactory, again scrutinizing the graphs and fine-tuning the models manually.\nOnce we are satisfied with the results, we always check if the changes made to the models do not result in some explanatory variables having unexpected signs of impact on GMV.\nDespite the changes in time series, we are expecting that in the long run fewer and fewer tweaks to the models will be necessary, and less work will be required from the analysts to maintain the tool.\nUser interface\nTo make the model easily accessible by business users, an interactive application was prepared. The user has default inputs set for upcoming months. They can change their values and get model predictions by clicking the “RUN SCENARIO” button. The predictions can be seen in daily, weekly and monthly granularities. If the user chooses to, they can export the predictions in CSV format. You can find an anonymised print screen of the tool below.\n\nSummary\nAs a result of the project, we developed a solution providing incredible business value. The main features of the tool are:\nGreat forecast accuracy - we managed to get below 2% MAPE\nStability - the structure of the model remains the same and the inputs have the same impact direction over time\nResponsiveness - the forecasts change with changes in the business inputs\nInterpretation - though the model is not intrinsically interpretable, we developed methods to check how well it works\nInteractive UI - stakeholders can experiment with various business scenarios online\nDomino proved its effectiveness in hard and demanding times while giving us a lot of practical knowledge related to modeling of such a complex business metric. And, we already started using these lessons in new upcoming projects.","guid":"https://blog.allegro.tech/2021/05/domino-financial-forecasting-in-the-age-of-global-pandemic.html","categories":["tech"],"isoDate":"2021-05-20T22:00:00.000Z","thumbnail":"images/post-headers/default.jpg"},{"title":"My first days at Allegro","link":"https://blog.allegro.tech/2021/05/my-first-days-at-allegro.html","pubDate":"Tue, 11 May 2021 00:00:00 +0200","authors":{"author":[{"name":["Anna Stanisławska"],"photo":["https://blog.allegro.tech/img/authors/anna.stanislawska.jpg"],"url":["https://blog.allegro.tech/authors/anna.stanislawska"]}]},"content":"<p>The beginnings in a new job can be really tough, especially in such uncertain times as the pandemic. Remote onboarding, Zoom meetings, inability to talk\nface-to-face — it’s a big test, especially if you’re switching industries. My story is quite similar: in July 2020 I started a three-month internship at Allegro\nto train for product management. When the internship ended, I was offered to stay permanently as Junior Product Manager. I’d like to describe my beginnings at\nAllegro. But first — a few words about me.</p>\n\n<h2 id=\"a-few-words-to-begin-with\">A few words to begin with</h2>\n\n<p>I have never imagined that my career path would move towards product management. As far as I remember, I was interested in science, especially chemistry. When I\nwas in high school, I imagined myself working in a lab. However, after a while I understood that my expectations didn’t harmonize with my idea of working in a\nlab. I needed more creativity, independent outlook and impact on projects I would be working on. Later I became interested in marketing and economy. I wondered\nhow to combine all these scientific fields.</p>\n\n<p>So, I decided to study Commodity Science. It is the interdisciplinary field of study that gives a broad perspective of product design, production, marketing,\nquality assurance and management. And that’s all to meet human needs. I decided to take up product management specialization — and I knew that this was the path\nthat I would like to take. And before I graduated, I had the possibility to start working as a product manager in the publishing industry.</p>\n\n<h2 id=\"how-it-started\">How it started</h2>\n\n<p>I came across an internship offer by accident. I was not looking for a new job. I had often used <a href=\"https://allegro.tech\">Allegro</a> to do some shopping, I knew the\nwebsite well. The internship offer was attractive, so I decided to send my application. I decided to take that step despite the fact that the offer indicated a\nthree-month period and I already had a job! After some time I received my internship task. I focused on the biggest trends in the e-commerce industry and I\ncreated a project of a minimal viable product that could be implemented at Allegro using statistics and KPIs. I defined exact goals and real benefits from the\nbuyers’, sellers’ and Allegro’s perspective. Next recruitment stage included two interviews — with my future superiors and an HR business partner. After a few\ndays I got a phone call with the offer — I did it, I got the internship. I decided to accept the offer despite having a permanent job. I knew that the\ninternship could give me a lot of experience in an industry that was unknown to me and which was growing so fast. Allegro is an e-commerce leader in Poland with\nhigh recognizability — and when you want to learn something new, do it with the best.</p>\n\n<h2 id=\"first-steps-into-the-unknown\">First steps into the unknown</h2>\n\n<p>In the beginning, I had two days of remote onboarding. Other interns and I got to know the company structure, we got lots of necessary information on how to\nmove in the corporate world. I learned details of the work in other departments and where to look for the answers in specific situations. Even though the\nonboarding was not held on-premise at headquarters in Poznań as was usual before the pandemic, I had the opportunity to meet other interns and the atmosphere\nwas friendly and supportive. And I felt that despite the fact that we were working in different departments, we were on the same team.</p>\n\n<p>After onboarding, I started working with the team. I was introduced to the secrets of checkout and post-purchase. The biggest challenge to start with? Getting\nto know the team and tasks while sitting at the desk at home. But fortunately everyone was very supportive and they assisted with every issue that I struggled\nwith. Especially my internship mentor — I owe her a lot — gave me tons of very valuable knowledge and I’m thankful that I was able to develop under her\nsupervision. And it was obvious for everyone that I, as the intern, needed more time to recognize all the processes in checkout and post-purchase. These are the\nfinal steps of the purchasing process that combines a lot of Allegro’s parts and at first sight it may seem complicated. But despite being an intern, I got some\nindependence, with support of my superior when I needed it. And after three months I was ready to become a full-time employee. Now I am involved in a big\npost-purchase project (I started working on this project at the beginning of the internship). I cooperate with the development team and the Team Leader, with a\nUX Designer, a UX Researcher and an Analyst. We constantly analyse users’ needs and we implement new functions so that the purchasing process is as easy, fast\nand smooth as possible and that the user can always find what they are looking for.</p>\n\n<p>However, the internship period wasn’t completely hassle-free. Allegro is a very big company that is sometimes overwhelming. At the very beginning, especially\nwhen I had a lot of common tasks with different teams, I felt a little lost when I had to find the proper Product Manager or Team Leader to determine the\ndetails, schedule work and when my knowledge was incomplete. Moreover, the beginnings were a little stressful because my product decisions have a big impact on\nthe shopping experience and our solutions are used by millions of users. I had a lot of stage fright before my first implementation but from month to month I\nthink I became more confident. After a few months it wasn’t a problem to move around anymore, it just had to take some time. Since I became a full-time employee\nI still have the opportunity to receive support from a mentor. It’s a person who is always there to help, answer all questions about the product, process,\nplanning etc. In my case it’s a person from another department so I can get a fresh perspective on my doubts.</p>\n\n<h2 id=\"were-on-the-same-team\">We’re on the same team</h2>\n\n<p>As I mentioned, first days in a new job can be tough, but being at Allegro you don’t need to worry. Allegro is an employee-friendly zone and you can count on\nthe support of more experienced people. Despite the fact that we are working from home, I met great people whom I get along with and I feel good in their\ncompany. Good working relationships and a friendly environment are essential for new employees. We are playing towards one goal — the best shopping experience.</p>\n","contentSnippet":"The beginnings in a new job can be really tough, especially in such uncertain times as the pandemic. Remote onboarding, Zoom meetings, inability to talk\nface-to-face — it’s a big test, especially if you’re switching industries. My story is quite similar: in July 2020 I started a three-month internship at Allegro\nto train for product management. When the internship ended, I was offered to stay permanently as Junior Product Manager. I’d like to describe my beginnings at\nAllegro. But first — a few words about me.\nA few words to begin with\nI have never imagined that my career path would move towards product management. As far as I remember, I was interested in science, especially chemistry. When I\nwas in high school, I imagined myself working in a lab. However, after a while I understood that my expectations didn’t harmonize with my idea of working in a\nlab. I needed more creativity, independent outlook and impact on projects I would be working on. Later I became interested in marketing and economy. I wondered\nhow to combine all these scientific fields.\nSo, I decided to study Commodity Science. It is the interdisciplinary field of study that gives a broad perspective of product design, production, marketing,\nquality assurance and management. And that’s all to meet human needs. I decided to take up product management specialization — and I knew that this was the path\nthat I would like to take. And before I graduated, I had the possibility to start working as a product manager in the publishing industry.\nHow it started\nI came across an internship offer by accident. I was not looking for a new job. I had often used Allegro to do some shopping, I knew the\nwebsite well. The internship offer was attractive, so I decided to send my application. I decided to take that step despite the fact that the offer indicated a\nthree-month period and I already had a job! After some time I received my internship task. I focused on the biggest trends in the e-commerce industry and I\ncreated a project of a minimal viable product that could be implemented at Allegro using statistics and KPIs. I defined exact goals and real benefits from the\nbuyers’, sellers’ and Allegro’s perspective. Next recruitment stage included two interviews — with my future superiors and an HR business partner. After a few\ndays I got a phone call with the offer — I did it, I got the internship. I decided to accept the offer despite having a permanent job. I knew that the\ninternship could give me a lot of experience in an industry that was unknown to me and which was growing so fast. Allegro is an e-commerce leader in Poland with\nhigh recognizability — and when you want to learn something new, do it with the best.\nFirst steps into the unknown\nIn the beginning, I had two days of remote onboarding. Other interns and I got to know the company structure, we got lots of necessary information on how to\nmove in the corporate world. I learned details of the work in other departments and where to look for the answers in specific situations. Even though the\nonboarding was not held on-premise at headquarters in Poznań as was usual before the pandemic, I had the opportunity to meet other interns and the atmosphere\nwas friendly and supportive. And I felt that despite the fact that we were working in different departments, we were on the same team.\nAfter onboarding, I started working with the team. I was introduced to the secrets of checkout and post-purchase. The biggest challenge to start with? Getting\nto know the team and tasks while sitting at the desk at home. But fortunately everyone was very supportive and they assisted with every issue that I struggled\nwith. Especially my internship mentor — I owe her a lot — gave me tons of very valuable knowledge and I’m thankful that I was able to develop under her\nsupervision. And it was obvious for everyone that I, as the intern, needed more time to recognize all the processes in checkout and post-purchase. These are the\nfinal steps of the purchasing process that combines a lot of Allegro’s parts and at first sight it may seem complicated. But despite being an intern, I got some\nindependence, with support of my superior when I needed it. And after three months I was ready to become a full-time employee. Now I am involved in a big\npost-purchase project (I started working on this project at the beginning of the internship). I cooperate with the development team and the Team Leader, with a\nUX Designer, a UX Researcher and an Analyst. We constantly analyse users’ needs and we implement new functions so that the purchasing process is as easy, fast\nand smooth as possible and that the user can always find what they are looking for.\nHowever, the internship period wasn’t completely hassle-free. Allegro is a very big company that is sometimes overwhelming. At the very beginning, especially\nwhen I had a lot of common tasks with different teams, I felt a little lost when I had to find the proper Product Manager or Team Leader to determine the\ndetails, schedule work and when my knowledge was incomplete. Moreover, the beginnings were a little stressful because my product decisions have a big impact on\nthe shopping experience and our solutions are used by millions of users. I had a lot of stage fright before my first implementation but from month to month I\nthink I became more confident. After a few months it wasn’t a problem to move around anymore, it just had to take some time. Since I became a full-time employee\nI still have the opportunity to receive support from a mentor. It’s a person who is always there to help, answer all questions about the product, process,\nplanning etc. In my case it’s a person from another department so I can get a fresh perspective on my doubts.\nWe’re on the same team\nAs I mentioned, first days in a new job can be tough, but being at Allegro you don’t need to worry. Allegro is an employee-friendly zone and you can count on\nthe support of more experienced people. Despite the fact that we are working from home, I met great people whom I get along with and I feel good in their\ncompany. Good working relationships and a friendly environment are essential for new employees. We are playing towards one goal — the best shopping experience.","guid":"https://blog.allegro.tech/2021/05/my-first-days-at-allegro.html","categories":["tech"],"isoDate":"2021-05-10T22:00:00.000Z","thumbnail":"images/post-headers/default.jpg"},{"title":"Kotlin - a language for everyone and for everything. Even scripts.","link":"https://blog.allegro.tech/2021/04/kotlin-scripting.html","pubDate":"Tue, 13 Apr 2021 00:00:00 +0200","authors":{"author":[{"name":["Weronika Orczyk"],"photo":["https://blog.allegro.tech/img/authors/weronika.orczyk.jpg"],"url":["https://blog.allegro.tech/authors/weronika.orczyk"]}]},"content":"<p>According to Wikipedia there are approximately 700 computer languages available. Seven hundred.\nThis is an unbelievable number and it’s the reason why programmers face the problem of\nchoosing a programming language to work with, which frameworks to use and which tech stack to learn.\nAll of them have pros and cons, but when looking for an all-purpose language you should take Kotlin\ninto consideration and ask yourself the question: “Do I really need another programming language?”.</p>\n\n<p><img src=\"/img/articles/2021-04-13-scripting.jpg\" alt=\"scripting\" /></p>\n\n<p>Kotlin is an open source, statically typed and powerful language. It takes inspiration from many programming languages like Java,\nScala, C#, Groovy, Python and it attempts to combine the best features from each. The support for multiplatform programming\nis one of Kotlin’s key benefits. It is able to compile to many platforms including Android, JVM, JavaScript and native.</p>\n\n<p>Kotlin is great for building applications, but what if you need to write a script? The first thing that comes to mind is to use\na classic, traditional shell script or Python, what gives you the ability to move files, copy, delete and more by running\na program from command-line. But if Kotlin is your cup of tea, it can be also used as a scripting language!</p>\n\n<p>When talking about shell scripts, I have to mention their syntax, which can be unclear and enigmatic. It is really hard to write\na clear piece of code in a complex script. Another drawback is long execution time. Shell scripts are simply slow due to almost\nevery executed shell command’s need to launch a new process. Additionally, there are compatibility problems between platforms.\nThe creator of Perl, Larry Wall, famously wrote that “It is easier to port a shell than a shell script.”</p>\n\n<p>I’ve mentioned slow execution of shell scripts due to creation of a new process every command. It’s true,\nbut on the other hand spinning up a new JVM to execute a script and a compilation process is also long. The execution time usually\ndepends on a machine and distribution of JVM. It can be a bit frustrating when Java version is inappropriate for an executed script\nor when some machines use different operating system, what can lead to compatibility issues.</p>\n\n<p>So how about Python? It provides lots of useful libraries covering almost any needs and is available on most systems,\nessentially eliminating the problem of compatibility.</p>\n\n<p>While scripts written in Python are more readable than shell scripts, they aren’t type-safe in the way that Kotlin\nis. Kotlin is statically typed, what means that types are verified during compilation, and strongly typed, i.e.\nthe type of a variable cannot change after its declaration. In contrast to a dynamically typed language, including Python,\nit helps prevent all kinds of incompatible type errors. It brings the opportunity to create scripts that help automate\nmundane tasks, but doing it in a much safer way.</p>\n\n<p>Let’s move to the code and see Kotlin scripts in practice.</p>\n\n<h3 id=\"install-kotlin-for-scripting\">Install Kotlin for scripting</h3>\n<p>In order to allow scripting, you need to make Kotlin available for the entire system. Open a terminal and check if Kotlin is\ninstalled:</p>\n<div class=\"language-plaintext highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>kotlinc -version\n</code></pre></div></div>\n<p>If the command was not found, the easiest way to install Kotlin on UNIX-based systems such as OS X,\nLinux, Cygwin, FreeBSD, and Solaris is SDKMAN!. Install Kotlin with the command:</p>\n<div class=\"language-plaintext highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>sdk install kotlin\n</code></pre></div></div>\n\n<p>Alternatively, on OS X you can install the compiler via Homebrew:</p>\n<div class=\"language-plaintext highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>brew install kotlin\n</code></pre></div></div>\n<p>Check the Kotlin version again.</p>\n\n<h3 id=\"first-script\">First script</h3>\n\n<p><a href=\"https://github.com/Kotlin/KEEP/blob/master/proposals/scripting-support.md\">Kotlin Evolution and Enhancement Process (KEEP) document</a>\nwas created to explain Kotlin scripting. It presents use cases of scripting with examples and contains the general proposal,\nbut it is still a draft. According to this file, a Kotlin script is a simple Kotlin snippet. Each command in the file is\nexecuted as if it was in the main function. To make a file executable, the file has to be named <code class=\"language-plaintext highlighter-rouge\">*.kts</code>.</p>\n\n<p>Create the first script and name the file <code class=\"language-plaintext highlighter-rouge\">first-script.kts</code>.</p>\n<div class=\"language-plaintext highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>touch first-script.kts\n</code></pre></div></div>\n<p>Edit file and add some code:</p>\n<div class=\"language-plaintext highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>println(\"Hello, it’s your first script called with args:\")\nargs.forEach {\n    println(\"* $it\")\n}\n</code></pre></div></div>\n<p>And run it with example arguments:</p>\n<div class=\"language-plaintext highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>kotlinc -script first-script.kts test 'test with space'\n</code></pre></div></div>\n<p>You should see the result:</p>\n<div class=\"language-plaintext highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>Hello, it’s your first script called with args:\n* test\n* test with space\n</code></pre></div></div>\n<p>Really simple! In addition, scripts can use Java or Kotlin stlib to have access to more powerful libraries.\nOf course, functions and classes can be used to make the script more readable and easy to maintain.</p>\n\n<div class=\"language-plaintext highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>import java.io.File\n\nfun printDirs() {\n    File(\".\").listFiles { file -&gt; file.isDirectory }?.forEach {\n        println(it)\n    }\n}\n\nprintDirs()\n</code></pre></div></div>\n\n<h3 id=\"kotlin-main-kts\">Kotlin-main-kts</h3>\n<p>Kotlin-main-kts is a solution provided by the Kotlin team. The 1.3.70 version brought a set of improvements that\nprovide a better experience of using Kotlin scripts with IntelliJ IDEA and Kotlin command-line tools.\nMore details can be found in <a href=\"https://blog.jetbrains.com/kotlin/2020/03/kotlin-1-3-70-released/\">a release blog post</a>.\nAll you need to use kotlin-main-kts is to create a file named <code class=\"language-plaintext highlighter-rouge\">*.main.kts</code> and execute script.</p>\n\n<p>Moreover, from 1.3.70 on, it is easier to run the program. The command is:</p>\n\n<div class=\"language-plaintext highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>kotlin myscript.main.kts\n</code></pre></div></div>\n\n<p>Using kotlin-main-kts has a lot of advantages. <code class=\"language-plaintext highlighter-rouge\">*.main.kts</code> scripts are supported out of the box in IntelliJ,\neven outside the source directories. It gives you highlighting and navigation, autocompletes code\nand resolves dynamic dependencies. What’s more, third party libraries can be used in scripts by\nincluding them as dependencies using directives or annotations specified with gradle-style with group ID,\nartifact ID and version. For example, if you write a script which copies data from a CSV file and inserts the data\ninto a database, you need to use a connector. You can achieve this by <code class=\"language-plaintext highlighter-rouge\">@file:DependsOn</code> annotation. The example below shows\nhow to connect to a MongoDB database. A database <em>URL</em> is required as an argument e.g. <code class=\"language-plaintext highlighter-rouge\">mongodb://localhost:27017/test-db</code></p>\n\n<div class=\"language-plaintext highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>@file:Repository(\"https://jcenter.bintray.com\")\n@file:DependsOn(\"org.mongodb:mongo-java-driver:3.12.8\")\n\nimport com.mongodb.client.MongoClients\n\nfun createConnection(url: String) =\n    MongoClients.create(url).use {\n        println(\"Connected to MongoDB!\")\n    }\n\nval (mongoUrl) = args\ncreateConnection(mongoUrl)\n</code></pre></div></div>\n\n<p>What’s interesting, it is also possible to execute a script exactly in the same way as an executable file - for example by\ncommand <code class=\"language-plaintext highlighter-rouge\">./kotlin-script.main.kts</code>. The first line in a file makes difference. It is required to add <code class=\"language-plaintext highlighter-rouge\">#!/usr/bin/env kotlin</code>\nat the beginning of script to run it in such way.</p>\n\n<div class=\"language-plaintext highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>#!/usr/bin/env kotlin\nprintln(\"Hello, world!\")\n</code></pre></div></div>\n\n<h3 id=\"kscript\">Kscript</h3>\n\n<p>Also, I have to mention <a href=\"https://github.com/holgerbrandl/kscript\">Kscript</a>. It is one of the ways to support scripting in\nKotlin. Actually, it is a wrapper around kotlinc which adds some features. After Kscript installation you can easily run\nscript by command:</p>\n<div class=\"language-plaintext highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>kscript first-script.kts\n</code></pre></div></div>\n\n<p>This tool was created in 2016. In the beginning, Kotlin wasn’t feature-rich enough to be a viable alternative to the\nshell. <a href=\"https://www.youtube.com/watch?v=cOJPKhlRa8c\">The great talk on Kotlin Conf by Holger Brandl</a> shows the\nhistory of its creation and explains that initially the tool was created for data science purposes.</p>\n\n<p>The creators listed such advantages as:</p>\n<ul>\n  <li>compiled script caching (using md5 checksums)</li>\n  <li>dependency declarations using gradle-style resource locators and automatic dependency resolution</li>\n  <li>possibility to deploy scripts as standalone binaries</li>\n  <li>inlined usage - you don’t need to create script file</li>\n</ul>\n\n<p>In my opinion, the cache mechanism is the biggest advantage of using Kscript tool. Whenever you run a script with kotlinc\nit is recompiled, which makes it slow. Each time the script is started, Kscripts checks if code has been modified since the last\ncompilation (by hashing the content of the script). If file wasn’t changed, cached version is immediately executed. The\ntable below shows time of execution of the same script by <code class=\"language-plaintext highlighter-rouge\">kotlinc</code> command and Kscript. As we can see, the cache has a\npositive effect on execution time.</p>\n\n<table>\n  <thead>\n    <tr>\n      <th><code class=\"language-plaintext highlighter-rouge\">time kotlinc -script script.kts</code></th>\n      <th><code class=\"language-plaintext highlighter-rouge\">time kscript script.kts</code></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>2.930s</td>\n      <td>3.972s</td>\n    </tr>\n    <tr>\n      <td>2.991s</td>\n      <td>0.868s</td>\n    </tr>\n    <tr>\n      <td>2.968s</td>\n      <td>0.912s</td>\n    </tr>\n  </tbody>\n</table>\n\n<p>Kotlin v1.4 brought a much improved scripting integration. But until Kotlin scripting interpreter isn’t rich and\nversatile, Kscript will be supported and developed.</p>\n\n<h3 id=\"conclusion\">Conclusion</h3>\n<p>Kotlin makes scripting really easy. Powerful scripts can be created with the autocompletion and the strong typing,\nmaking them really readable and highly maintainable.</p>\n\n<p>Happy scripting!</p>\n","contentSnippet":"According to Wikipedia there are approximately 700 computer languages available. Seven hundred.\nThis is an unbelievable number and it’s the reason why programmers face the problem of\nchoosing a programming language to work with, which frameworks to use and which tech stack to learn.\nAll of them have pros and cons, but when looking for an all-purpose language you should take Kotlin\ninto consideration and ask yourself the question: “Do I really need another programming language?”.\n\nKotlin is an open source, statically typed and powerful language. It takes inspiration from many programming languages like Java,\nScala, C#, Groovy, Python and it attempts to combine the best features from each. The support for multiplatform programming\nis one of Kotlin’s key benefits. It is able to compile to many platforms including Android, JVM, JavaScript and native.\nKotlin is great for building applications, but what if you need to write a script? The first thing that comes to mind is to use\na classic, traditional shell script or Python, what gives you the ability to move files, copy, delete and more by running\na program from command-line. But if Kotlin is your cup of tea, it can be also used as a scripting language!\nWhen talking about shell scripts, I have to mention their syntax, which can be unclear and enigmatic. It is really hard to write\na clear piece of code in a complex script. Another drawback is long execution time. Shell scripts are simply slow due to almost\nevery executed shell command’s need to launch a new process. Additionally, there are compatibility problems between platforms.\nThe creator of Perl, Larry Wall, famously wrote that “It is easier to port a shell than a shell script.”\nI’ve mentioned slow execution of shell scripts due to creation of a new process every command. It’s true,\nbut on the other hand spinning up a new JVM to execute a script and a compilation process is also long. The execution time usually\ndepends on a machine and distribution of JVM. It can be a bit frustrating when Java version is inappropriate for an executed script\nor when some machines use different operating system, what can lead to compatibility issues.\nSo how about Python? It provides lots of useful libraries covering almost any needs and is available on most systems,\nessentially eliminating the problem of compatibility.\nWhile scripts written in Python are more readable than shell scripts, they aren’t type-safe in the way that Kotlin\nis. Kotlin is statically typed, what means that types are verified during compilation, and strongly typed, i.e.\nthe type of a variable cannot change after its declaration. In contrast to a dynamically typed language, including Python,\nit helps prevent all kinds of incompatible type errors. It brings the opportunity to create scripts that help automate\nmundane tasks, but doing it in a much safer way.\nLet’s move to the code and see Kotlin scripts in practice.\nInstall Kotlin for scripting\nIn order to allow scripting, you need to make Kotlin available for the entire system. Open a terminal and check if Kotlin is\ninstalled:\n\nkotlinc -version\n\n\nIf the command was not found, the easiest way to install Kotlin on UNIX-based systems such as OS X,\nLinux, Cygwin, FreeBSD, and Solaris is SDKMAN!. Install Kotlin with the command:\n\nsdk install kotlin\n\n\nAlternatively, on OS X you can install the compiler via Homebrew:\n\nbrew install kotlin\n\n\nCheck the Kotlin version again.\nFirst script\nKotlin Evolution and Enhancement Process (KEEP) document\nwas created to explain Kotlin scripting. It presents use cases of scripting with examples and contains the general proposal,\nbut it is still a draft. According to this file, a Kotlin script is a simple Kotlin snippet. Each command in the file is\nexecuted as if it was in the main function. To make a file executable, the file has to be named *.kts.\nCreate the first script and name the file first-script.kts.\n\ntouch first-script.kts\n\n\nEdit file and add some code:\n\nprintln(\"Hello, it’s your first script called with args:\")\nargs.forEach {\n    println(\"* $it\")\n}\n\n\nAnd run it with example arguments:\n\nkotlinc -script first-script.kts test 'test with space'\n\n\nYou should see the result:\n\nHello, it’s your first script called with args:\n* test\n* test with space\n\n\nReally simple! In addition, scripts can use Java or Kotlin stlib to have access to more powerful libraries.\nOf course, functions and classes can be used to make the script more readable and easy to maintain.\n\nimport java.io.File\n\nfun printDirs() {\n    File(\".\").listFiles { file -> file.isDirectory }?.forEach {\n        println(it)\n    }\n}\n\nprintDirs()\n\n\nKotlin-main-kts\nKotlin-main-kts is a solution provided by the Kotlin team. The 1.3.70 version brought a set of improvements that\nprovide a better experience of using Kotlin scripts with IntelliJ IDEA and Kotlin command-line tools.\nMore details can be found in a release blog post.\nAll you need to use kotlin-main-kts is to create a file named *.main.kts and execute script.\nMoreover, from 1.3.70 on, it is easier to run the program. The command is:\n\nkotlin myscript.main.kts\n\n\nUsing kotlin-main-kts has a lot of advantages. *.main.kts scripts are supported out of the box in IntelliJ,\neven outside the source directories. It gives you highlighting and navigation, autocompletes code\nand resolves dynamic dependencies. What’s more, third party libraries can be used in scripts by\nincluding them as dependencies using directives or annotations specified with gradle-style with group ID,\nartifact ID and version. For example, if you write a script which copies data from a CSV file and inserts the data\ninto a database, you need to use a connector. You can achieve this by @file:DependsOn annotation. The example below shows\nhow to connect to a MongoDB database. A database URL is required as an argument e.g. mongodb://localhost:27017/test-db\n\n@file:Repository(\"https://jcenter.bintray.com\")\n@file:DependsOn(\"org.mongodb:mongo-java-driver:3.12.8\")\n\nimport com.mongodb.client.MongoClients\n\nfun createConnection(url: String) =\n    MongoClients.create(url).use {\n        println(\"Connected to MongoDB!\")\n    }\n\nval (mongoUrl) = args\ncreateConnection(mongoUrl)\n\n\nWhat’s interesting, it is also possible to execute a script exactly in the same way as an executable file - for example by\ncommand ./kotlin-script.main.kts. The first line in a file makes difference. It is required to add #!/usr/bin/env kotlin\nat the beginning of script to run it in such way.\n\n#!/usr/bin/env kotlin\nprintln(\"Hello, world!\")\n\n\nKscript\nAlso, I have to mention Kscript. It is one of the ways to support scripting in\nKotlin. Actually, it is a wrapper around kotlinc which adds some features. After Kscript installation you can easily run\nscript by command:\n\nkscript first-script.kts\n\n\nThis tool was created in 2016. In the beginning, Kotlin wasn’t feature-rich enough to be a viable alternative to the\nshell. The great talk on Kotlin Conf by Holger Brandl shows the\nhistory of its creation and explains that initially the tool was created for data science purposes.\nThe creators listed such advantages as:\ncompiled script caching (using md5 checksums)\ndependency declarations using gradle-style resource locators and automatic dependency resolution\npossibility to deploy scripts as standalone binaries\ninlined usage - you don’t need to create script file\nIn my opinion, the cache mechanism is the biggest advantage of using Kscript tool. Whenever you run a script with kotlinc\nit is recompiled, which makes it slow. Each time the script is started, Kscripts checks if code has been modified since the last\ncompilation (by hashing the content of the script). If file wasn’t changed, cached version is immediately executed. The\ntable below shows time of execution of the same script by kotlinc command and Kscript. As we can see, the cache has a\npositive effect on execution time.\ntime kotlinc -script script.kts\n      time kscript script.kts\n    \n2.930s\n      3.972s\n    \n2.991s\n      0.868s\n    \n2.968s\n      0.912s\n    \nKotlin v1.4 brought a much improved scripting integration. But until Kotlin scripting interpreter isn’t rich and\nversatile, Kscript will be supported and developed.\nConclusion\nKotlin makes scripting really easy. Powerful scripts can be created with the autocompletion and the strong typing,\nmaking them really readable and highly maintainable.\nHappy scripting!","guid":"https://blog.allegro.tech/2021/04/kotlin-scripting.html","categories":["tech","kotlin","script"],"isoDate":"2021-04-12T22:00:00.000Z","thumbnail":"images/post-headers/default.jpg"},{"title":"Digging into Allegro, or how I started my testing career","link":"https://blog.allegro.tech/2021/03/digging-into-allegro-how-i-started-testing-career.html","pubDate":"Tue, 23 Mar 2021 00:00:00 +0100","authors":{"author":[{"name":["Aleksandra Kulesz"],"photo":["https://blog.allegro.tech/img/authors/aleksandra.kulesz.jpg"],"url":["https://blog.allegro.tech/authors/aleksandra.kulesz"]}]},"content":"<p>Maybe it’s another “How I’ve changed my worklife” story, but I hope it is an interesting one. It seems quite important to tell: before the beginning of my\nAllegro adventure, I was working as… an archaeologist. It was both physical work on excavations and research work at my PhD studies. And the only thing that\nconnects my past work and the current one is: attention to detail.</p>\n\n<p>I had been thinking about changing my career path for some time. I’ve got some IT-experienced friends and they convinced me to try my luck in that field. In the\nbeginning I learned some front-end technologies on my own. Later I systematized and developed my knowledge at a programming bootcamp. And started looking for a\njob. I sent my application to Allegro for a front-end developer offer. Unfortunately, my lack of experience (and probably knowledge, too) caused me not to get a\njob offer. However, someone from the HR squad encouraged me to not give up and try one more time with the <a href=\"https://allegro.pl/praca/staze\">Summer eXperience internship program</a>.</p>\n\n<h2 id=\"it-begins\">It begins!</h2>\n\n<p>I did it! I was invited to begin cooperation (as Test Engineer Intern) with Allegro’s teams responsible for developing listings (search result pages). First of\nJuly was the day when I and my 52 intern-colleagues started work. After a two-day long onboarding (a very intensive time filled with tons of useful pieces of\ninformation!), I met my teammates for the first time. During summertime anti-covid restrictions were not so strict and I was able to visit the office a few\ntimes. We discussed my responsibilities and they guided me through the listings domain.</p>\n\n<p>In the beginning of my internship I had to test manually our web and mobile-web solutions. For a person who never had anything to do with a complex\nmicroservices system it was quite a challenging experience. I needed to understand and learn how services work and communicate with each other. The next step\nwas to become familiar with test environments. Phoenix (development test environments) and Sandbox are very useful, but at the same time they are very fussy.\nYou have to be quite patient to work with those tools. And when you are a QA Engineer you work with them all the time! But it turned out to be just a warm up. I\nquickly learned how to test our mobile applications (both for Android and iOS) and write and maintain automatic tests in the <a href=\"https://www.cypress.io/\">Cypress framework</a>.\nI was introduced to chaos and efficiency testing. I also wrote my first automatic tests in <a href=\"https://developer.android.com/training/testing/espresso\">Espresso</a>\nand TeaBiscuit (our internal tool). Every day brought something exciting and new, and still does.</p>\n\n<h2 id=\"what-do-i-like\">What do I like?</h2>\n\n<p>My favourite side of Allegro is knowledge sharing. Regarding common problems with technology or domain knowledge, I can always ask my supportive teammates, but\nthere is much more. We can take part in many interesting courses and workshops organised by people truly dedicated to their subjects. Among those are not only\ntech-courses, but you can also take part in workshops which develop your soft or leadership skills. In my opinion, the most important thing is talking to other\ntesters in our company. We have got a slack channel, a hot field of debates, sharing experiences and solving problems. There is also a monthly testers’ meeting.\nDuring these events we can present our tools, and share our feelings about them. After a short lecture we discuss a lot and it is really inspiring. Last but not\nleast are our conferences and hackathons. Allegro organizes amazing, huge meetings (in the past year, they were, of course, organised online). Subjects are very\ndiversified and broaden horizons.</p>\n\n<p>Another amazing issue is the attitude. The whole team is proud of their work and tries to cooperate agreeably in order to be effective and efficient. Even as an\nIntern you do not feel as a second-class employee, you are treated as seriously as your coworker with ten years of experience. I never met a person who was rude\nto me, or was angry with my lack of knowledge. Everybody knows that there are newbies in the company, and they are forgiving. When you achieve a goal your\ncolleagues are happy for you.</p>\n\n<h2 id=\"any-downsides\">Any downsides?</h2>\n\n<p>I was thinking long and hard about downsides or disadvantages of working at Allegro. Beside problems with test environments I mentioned above, I have one little\nissue. Because the company is so big, there are so many teams and services it is sometimes difficult to find the source of needed information. From time to time\nit takes much longer than you assumed. It might be a little bit frustrating. It also causes that many teams develop similar/alternative tools at the same time.\nSix months later, we realize that it could have been achieved faster and with less effort.Therefore, communication seems to be the most important thing, and\nsometimes it can be a bit neglected.</p>\n\n<p>There is another minor difficulty. There are small pieces of code, small features that do not have an owner. When you find a bug there or would like to make\nsome changes on it, it is not obvious to whom you should communicate your actions.</p>\n\n<h2 id=\"lets-wrap-up\">Let’s wrap up!</h2>\n\n<p>I like working with my teammates. I believe that I have developed a lot since I started work here. I am really happy to be part of this company, because Allegro\nis a great opportunity for everyone who wants to learn. It is good to be here! / Dobrze tu być!</p>\n","contentSnippet":"Maybe it’s another “How I’ve changed my worklife” story, but I hope it is an interesting one. It seems quite important to tell: before the beginning of my\nAllegro adventure, I was working as… an archaeologist. It was both physical work on excavations and research work at my PhD studies. And the only thing that\nconnects my past work and the current one is: attention to detail.\nI had been thinking about changing my career path for some time. I’ve got some IT-experienced friends and they convinced me to try my luck in that field. In the\nbeginning I learned some front-end technologies on my own. Later I systematized and developed my knowledge at a programming bootcamp. And started looking for a\njob. I sent my application to Allegro for a front-end developer offer. Unfortunately, my lack of experience (and probably knowledge, too) caused me not to get a\njob offer. However, someone from the HR squad encouraged me to not give up and try one more time with the Summer eXperience internship program.\nIt begins!\nI did it! I was invited to begin cooperation (as Test Engineer Intern) with Allegro’s teams responsible for developing listings (search result pages). First of\nJuly was the day when I and my 52 intern-colleagues started work. After a two-day long onboarding (a very intensive time filled with tons of useful pieces of\ninformation!), I met my teammates for the first time. During summertime anti-covid restrictions were not so strict and I was able to visit the office a few\ntimes. We discussed my responsibilities and they guided me through the listings domain.\nIn the beginning of my internship I had to test manually our web and mobile-web solutions. For a person who never had anything to do with a complex\nmicroservices system it was quite a challenging experience. I needed to understand and learn how services work and communicate with each other. The next step\nwas to become familiar with test environments. Phoenix (development test environments) and Sandbox are very useful, but at the same time they are very fussy.\nYou have to be quite patient to work with those tools. And when you are a QA Engineer you work with them all the time! But it turned out to be just a warm up. I\nquickly learned how to test our mobile applications (both for Android and iOS) and write and maintain automatic tests in the Cypress framework.\nI was introduced to chaos and efficiency testing. I also wrote my first automatic tests in Espresso\nand TeaBiscuit (our internal tool). Every day brought something exciting and new, and still does.\nWhat do I like?\nMy favourite side of Allegro is knowledge sharing. Regarding common problems with technology or domain knowledge, I can always ask my supportive teammates, but\nthere is much more. We can take part in many interesting courses and workshops organised by people truly dedicated to their subjects. Among those are not only\ntech-courses, but you can also take part in workshops which develop your soft or leadership skills. In my opinion, the most important thing is talking to other\ntesters in our company. We have got a slack channel, a hot field of debates, sharing experiences and solving problems. There is also a monthly testers’ meeting.\nDuring these events we can present our tools, and share our feelings about them. After a short lecture we discuss a lot and it is really inspiring. Last but not\nleast are our conferences and hackathons. Allegro organizes amazing, huge meetings (in the past year, they were, of course, organised online). Subjects are very\ndiversified and broaden horizons.\nAnother amazing issue is the attitude. The whole team is proud of their work and tries to cooperate agreeably in order to be effective and efficient. Even as an\nIntern you do not feel as a second-class employee, you are treated as seriously as your coworker with ten years of experience. I never met a person who was rude\nto me, or was angry with my lack of knowledge. Everybody knows that there are newbies in the company, and they are forgiving. When you achieve a goal your\ncolleagues are happy for you.\nAny downsides?\nI was thinking long and hard about downsides or disadvantages of working at Allegro. Beside problems with test environments I mentioned above, I have one little\nissue. Because the company is so big, there are so many teams and services it is sometimes difficult to find the source of needed information. From time to time\nit takes much longer than you assumed. It might be a little bit frustrating. It also causes that many teams develop similar/alternative tools at the same time.\nSix months later, we realize that it could have been achieved faster and with less effort.Therefore, communication seems to be the most important thing, and\nsometimes it can be a bit neglected.\nThere is another minor difficulty. There are small pieces of code, small features that do not have an owner. When you find a bug there or would like to make\nsome changes on it, it is not obvious to whom you should communicate your actions.\nLet’s wrap up!\nI like working with my teammates. I believe that I have developed a lot since I started work here. I am really happy to be part of this company, because Allegro\nis a great opportunity for everyone who wants to learn. It is good to be here! / Dobrze tu być!","guid":"https://blog.allegro.tech/2021/03/digging-into-allegro-how-i-started-testing-career.html","categories":["tech","testing","intern","career change"],"isoDate":"2021-03-22T23:00:00.000Z","thumbnail":"images/post-headers/testing.png"}],"jobs":[{"id":"743999751619727","name":"Senior Data Analyst / Team Leader","uuid":"1c7bd66c-64f8-4298-b99b-a1a234705d4a","refNumber":"REF2776P","company":{"identifier":"Allegro","name":"Allegro"},"releasedDate":"2021-06-04T12:17:31.000Z","location":{"city":"Poznań","country":"pl","remote":false},"industry":{"id":"internet","label":"Internet"},"department":{"id":"2215787","label":"Analytics"},"function":{"id":"analyst","label":"Analyst"},"typeOfEmployment":{"label":"Full-time"},"experienceLevel":{"id":"mid_senior_level","label":"Mid-Senior Level"},"customField":[{"fieldId":"58c15608e4b01d4b19ddf790","fieldLabel":"Proces rekrutacji","valueId":"c807eec2-8a53-4b55-b7c5-c03180f2059b","valueLabel":"IT Allegro"},{"fieldId":"COUNTRY","fieldLabel":"Country","valueId":"pl","valueLabel":"Poland"},{"fieldId":"58c13159e4b01d4b19ddf729","fieldLabel":"Department","valueId":"2215787","valueLabel":"Analytics"},{"fieldId":"58c13159e4b01d4b19ddf728","fieldLabel":"Brands","valueId":"4ccb4fab-6c3f-4ed0-9140-8533fe17447f","valueLabel":"Allegro.pl sp. z o.o."}],"ref":"https://api.smartrecruiters.com/v1/companies/allegro/postings/743999751619727","creator":{"name":"Ada Latańska"},"language":{"code":"pl","label":"Polish","labelNative":"polski"}},{"id":"743999751588430","name":"Senior Software Engineer (Kotlin/Java) - Site Reliability Engineering","uuid":"20f5619b-3b8f-4ad1-91ca-0f267f71e1ba","refNumber":"REF2691L","company":{"identifier":"Allegro","name":"Allegro"},"releasedDate":"2021-06-04T08:33:32.000Z","location":{"city":"Poznań, Warszawa","country":"pl","remote":false},"industry":{"id":"internet","label":"Internet"},"department":{"id":"1013462","label":"Technology - Product Development"},"function":{"id":"information_technology","label":"Information Technology"},"typeOfEmployment":{"label":"Full-time"},"experienceLevel":{"id":"mid_senior_level","label":"Mid-Senior Level"},"customField":[{"fieldId":"58c15608e4b01d4b19ddf790","fieldLabel":"Proces rekrutacji","valueId":"c807eec2-8a53-4b55-b7c5-c03180f2059b","valueLabel":"IT Allegro"},{"fieldId":"COUNTRY","fieldLabel":"Country","valueId":"pl","valueLabel":"Poland"},{"fieldId":"58c13159e4b01d4b19ddf729","fieldLabel":"Department","valueId":"1013462","valueLabel":"Technology - Product Development"},{"fieldId":"58c158e9e4b0614667d5973e","fieldLabel":"Więcej lokalizacji","valueId":"fa9e2c82-b44f-40df-a699-4dea59cb0c13","valueLabel":"Nie"},{"fieldId":"58c13159e4b01d4b19ddf728","fieldLabel":"Brands","valueId":"4ccb4fab-6c3f-4ed0-9140-8533fe17447f","valueLabel":"Allegro.pl sp. z o.o."}],"ref":"https://api.smartrecruiters.com/v1/companies/allegro/postings/743999751588430","creator":{"name":"Adriana Gesiarz"},"language":{"code":"pl","label":"Polish","labelNative":"polski"}},{"id":"743999750962284","name":"Systems Administrator","uuid":"be6bd7a3-9e1a-4b8e-9c87-71731f54f610","refNumber":"REF2841X","company":{"identifier":"Allegro","name":"Allegro"},"releasedDate":"2021-06-01T14:06:34.000Z","location":{"city":"Poznań, Warszawa, Kraków, Toruń, Wrocław","country":"pl","remote":false},"industry":{"id":"internet","label":"Internet"},"department":{"id":"1013275","label":"IT Business Services"},"function":{"id":"information_technology","label":"Information Technology"},"typeOfEmployment":{"label":"Full-time"},"experienceLevel":{"id":"mid_senior_level","label":"Mid-Senior Level"},"customField":[{"fieldId":"58c1575ee4b01d4b19ddf797","fieldLabel":"Kraków","valueId":"2cfcfcc1-da44-43f7-8eaa-3907faf2797c","valueLabel":"Tak"},{"fieldId":"58c158e9e4b0614667d5973e","fieldLabel":"Więcej lokalizacji","valueId":"3a186e8d-a82c-4955-af81-fcef9a63928a","valueLabel":"Tak"},{"fieldId":"58c1576ee4b0614667d59732","fieldLabel":"Toruń","valueId":"987e8884-bad1-4a8f-b149-99e4184cc221","valueLabel":"Tak"},{"fieldId":"58c156e6e4b01d4b19ddf793","fieldLabel":"Warszawa","valueId":"6a428533-1586-4372-89ec-65fb45366363","valueLabel":"Tak"},{"fieldId":"58c15608e4b01d4b19ddf790","fieldLabel":"Proces rekrutacji","valueId":"c807eec2-8a53-4b55-b7c5-c03180f2059b","valueLabel":"IT Allegro"},{"fieldId":"COUNTRY","fieldLabel":"Country","valueId":"pl","valueLabel":"Poland"},{"fieldId":"58c15798e4b01d4b19ddf79b","fieldLabel":"Wrocław","valueId":"64818201-cdba-422e-8e8c-8ec633b0d327","valueLabel":"Tak"},{"fieldId":"58c13159e4b01d4b19ddf729","fieldLabel":"Department","valueId":"1013275","valueLabel":"IT Business Services"},{"fieldId":"58c13159e4b01d4b19ddf728","fieldLabel":"Brands","valueId":"4ccb4fab-6c3f-4ed0-9140-8533fe17447f","valueLabel":"Allegro.pl sp. z o.o."},{"fieldId":"58c156b9e4b01d4b19ddf792","fieldLabel":"Poznań","valueId":"ac20917b-cb6a-4280-aff3-4fae2532a33e","valueLabel":"Tak"}],"ref":"https://api.smartrecruiters.com/v1/companies/allegro/postings/743999750962284","creator":{"name":"Paulina Tynecka"},"language":{"code":"pl","label":"Polish","labelNative":"polski"}},{"id":"743999750906121","name":"Big Data Engineer - Merchant Experience","uuid":"3ec7b3ab-f28f-4281-8b61-487b2192235c","refNumber":"REF2778Y","company":{"identifier":"Allegro","name":"Allegro"},"releasedDate":"2021-06-01T08:47:26.000Z","location":{"city":"Kraków, Warszawa","region":"Lesser Poland Voivodeship","country":"pl","remote":false},"industry":{"id":"internet","label":"Internet"},"department":{"id":"2216093","label":"Development"},"function":{"id":"engineering","label":"Engineering"},"typeOfEmployment":{"label":"Full-time"},"experienceLevel":{"id":"associate","label":"Associate"},"customField":[{"fieldId":"58c15608e4b01d4b19ddf790","fieldLabel":"Proces rekrutacji","valueId":"c807eec2-8a53-4b55-b7c5-c03180f2059b","valueLabel":"IT Allegro"},{"fieldId":"58c1575ee4b01d4b19ddf797","fieldLabel":"Kraków","valueId":"2cfcfcc1-da44-43f7-8eaa-3907faf2797c","valueLabel":"Tak"},{"fieldId":"COUNTRY","fieldLabel":"Country","valueId":"pl","valueLabel":"Poland"},{"fieldId":"58c13159e4b01d4b19ddf729","fieldLabel":"Department","valueId":"2216093","valueLabel":"Development"},{"fieldId":"58c158e9e4b0614667d5973e","fieldLabel":"Więcej lokalizacji","valueId":"3a186e8d-a82c-4955-af81-fcef9a63928a","valueLabel":"Tak"},{"fieldId":"58c13159e4b01d4b19ddf728","fieldLabel":"Brands","valueId":"4ccb4fab-6c3f-4ed0-9140-8533fe17447f","valueLabel":"Allegro.pl sp. z o.o."},{"fieldId":"58c156e6e4b01d4b19ddf793","fieldLabel":"Warszawa","valueId":"6a428533-1586-4372-89ec-65fb45366363","valueLabel":"Tak"},{"fieldId":"5cdab2c84cedfd0006e7758c","fieldLabel":"Key words","valueLabel":"bigdata data"}],"ref":"https://api.smartrecruiters.com/v1/companies/allegro/postings/743999750906121","language":{"code":"pl","label":"Polish","labelNative":"polski"}},{"id":"743999750882690","name":"Integration Engineer","uuid":"ded7aa9b-2cbd-4b40-b653-f8a269d5d088","refNumber":"REF2138N","company":{"identifier":"Allegro","name":"Allegro"},"releasedDate":"2021-06-01T07:22:19.000Z","location":{"city":"Warszawa, Poznań","country":"pl","remote":false},"industry":{"id":"internet","label":"Internet"},"department":{"id":"1013462","label":"Technology - Product Development"},"function":{"id":"information_technology","label":"Information Technology"},"typeOfEmployment":{"label":"Full-time"},"experienceLevel":{"id":"entry_level","label":"Entry Level"},"customField":[{"fieldId":"58c15608e4b01d4b19ddf790","fieldLabel":"Proces rekrutacji","valueId":"c807eec2-8a53-4b55-b7c5-c03180f2059b","valueLabel":"IT Allegro"},{"fieldId":"COUNTRY","fieldLabel":"Country","valueId":"pl","valueLabel":"Poland"},{"fieldId":"58c13159e4b01d4b19ddf729","fieldLabel":"Department","valueId":"1013462","valueLabel":"Technology - Product Development"},{"fieldId":"58c158e9e4b0614667d5973e","fieldLabel":"Więcej lokalizacji","valueId":"3a186e8d-a82c-4955-af81-fcef9a63928a","valueLabel":"Tak"},{"fieldId":"58c13159e4b01d4b19ddf728","fieldLabel":"Brands","valueId":"4ccb4fab-6c3f-4ed0-9140-8533fe17447f","valueLabel":"Allegro.pl sp. z o.o."},{"fieldId":"58c156b9e4b01d4b19ddf792","fieldLabel":"Poznań","valueId":"ac20917b-cb6a-4280-aff3-4fae2532a33e","valueLabel":"Tak"},{"fieldId":"58c156e6e4b01d4b19ddf793","fieldLabel":"Warszawa","valueId":"6a428533-1586-4372-89ec-65fb45366363","valueLabel":"Tak"}],"ref":"https://api.smartrecruiters.com/v1/companies/allegro/postings/743999750882690","creator":{"name":"Klaudia Sipurzyńska"},"language":{"code":"pl","label":"Polish","labelNative":"polski"}}],"events":[{"created":1621842668000,"duration":100800000,"id":"278374635","name":"UX Research Confetti","date_in_series_pattern":false,"status":"upcoming","time":1624456800000,"local_date":"2021-06-23","local_time":"16:00","updated":1621951173000,"utc_offset":7200000,"waitlist_count":0,"yes_rsvp_count":31,"venue":{"id":26906060,"name":"Online event","repinned":false,"country":"","localized_country_name":""},"is_online_event":true,"group":{"created":1425052059000,"name":"allegro Tech","id":18465254,"join_mode":"open","lat":52.2599983215332,"lon":21.020000457763672,"urlname":"allegrotech","who":"Techs","localized_location":"Warsaw, Poland","state":"","country":"pl","region":"en_US","timezone":"Europe/Warsaw"},"link":"https://www.meetup.com/allegrotech/events/278374635/","description":"🎉 Niech rozsypie się confetti wiedzy o badaniach UX! 🎉 Szukaliśmy konferencji badawczej UX w Polsce i nie znaleźliśmy… Dlatego łączymy siły z ekspertami z…","visibility":"public","member_pay_fee":false},{"created":1622474681000,"duration":5400000,"id":"278528964","name":"Allegro Tech Live Odcinek: #19   Co to znaczy być liderem i jak nim zostać?","date_in_series_pattern":false,"status":"upcoming","time":1623340800000,"local_date":"2021-06-10","local_time":"18:00","updated":1622474681000,"utc_offset":7200000,"waitlist_count":0,"yes_rsvp_count":19,"venue":{"id":26906060,"name":"Online event","repinned":false,"country":"","localized_country_name":""},"is_online_event":true,"group":{"created":1425052059000,"name":"allegro Tech","id":18465254,"join_mode":"open","lat":52.2599983215332,"lon":21.020000457763672,"urlname":"allegrotech","who":"Techs","localized_location":"Warsaw, Poland","state":"","country":"pl","region":"en_US","timezone":"Europe/Warsaw"},"link":"https://www.meetup.com/allegrotech/events/278528964/","description":"Allegro Tech Live to w 100% zdalna odsłona naszych stacjonarnych meetupów Allegro Tech Talks. Zazwyczaj spotykaliśmy się w naszych biurach, ale tym razem to my…","visibility":"public","member_pay_fee":false},{"created":1619620661000,"duration":5400000,"id":"277852879","name":"Allegro Tech Live #18 PM w Allegro, jak do nas dołączyć i czerpać radość z pracy","date_in_series_pattern":false,"status":"past","time":1620921600000,"local_date":"2021-05-13","local_time":"18:00","updated":1620932668000,"utc_offset":7200000,"waitlist_count":0,"yes_rsvp_count":46,"venue":{"id":26906060,"name":"Online event","repinned":false,"country":"","localized_country_name":""},"is_online_event":true,"group":{"created":1425052059000,"name":"allegro Tech","id":18465254,"join_mode":"open","lat":52.2599983215332,"lon":21.020000457763672,"urlname":"allegrotech","who":"Techs","localized_location":"Warsaw, Poland","state":"","country":"pl","region":"en_US","timezone":"Europe/Warsaw"},"link":"https://www.meetup.com/allegrotech/events/277852879/","description":"Allegro Tech Live to w 100% zdalna odsłona naszych stacjonarnych meetupów Allegro Tech Talks. Zazwyczaj spotykaliśmy się w naszych biurach, ale tym razem to my…","how_to_find_us":"https://youtu.be/WNOQJxPKweM","visibility":"public","member_pay_fee":false},{"created":1618821709000,"duration":7200000,"id":"277660388","name":"Allegro Tech Labs #8 Online: Praktyczne prognozowanie w Allegro ","date_in_series_pattern":false,"status":"past","time":1620316800000,"local_date":"2021-05-06","local_time":"18:00","updated":1620327325000,"utc_offset":7200000,"waitlist_count":0,"yes_rsvp_count":45,"venue":{"id":26906060,"name":"Online event","repinned":false,"country":"","localized_country_name":""},"is_online_event":true,"group":{"created":1425052059000,"name":"allegro Tech","id":18465254,"join_mode":"open","lat":52.2599983215332,"lon":21.020000457763672,"urlname":"allegrotech","who":"Techs","localized_location":"Warsaw, Poland","state":"","country":"pl","region":"en_US","timezone":"Europe/Warsaw"},"link":"https://www.meetup.com/allegrotech/events/277660388/","description":"***REJESTRACJA***Ze względu na ograniczoną liczbę miejsc, prosimy o rejestrację na wydarzenie poprzez stronę: https://app.evenea.pl/event/allegro-tech-labs-8/Wybierając przycisk \"Chcę wziąć udział w warsztatach\" przeniesiesz się na formularz rejestracyjny…","visibility":"public","member_pay_fee":false}],"podcasts":[{"creator":{"name":["Piotr Michoński"]},"title":"Infrastruktura Allegro","link":"https://podcast.allegro.tech/infrastruktura_Allegro","pubDate":"Tue, 01 Jun 2021 00:00:00 GMT","author":{"name":["Piotr Michoński"]},"enclosure":{"url":"https://www.buzzsprout.com/887914/8623783-sezon-ii-11-infrastruktura-allegro-piotr-michonski.mp3","type":"audio/mpeg"},"content":"Jak jest zbudowane środowisko uruchomienia aplikacji Allegro? Jak działają serwerownie firmy i ile ich potrzeba, a które elementy Allegro działają w chmurze publicznej? Jak przebiegała transformacja w Allegro i co zmieniało się przez lata? Jak wzrost biznesu wpływa na wielkość infrastruktury i jak infrastruktura Allegro odczuła przyjście pandemii? O tym, a także o rozwoju liderów technologii w Allegro oraz o historii powstania dżingla do naszych podcastów, opowie Piotr Michoński - menadżer Zespołów tworzących infrastrukturę Allegro.","contentSnippet":"Jak jest zbudowane środowisko uruchomienia aplikacji Allegro? Jak działają serwerownie firmy i ile ich potrzeba, a które elementy Allegro działają w chmurze publicznej? Jak przebiegała transformacja w Allegro i co zmieniało się przez lata? Jak wzrost biznesu wpływa na wielkość infrastruktury i jak infrastruktura Allegro odczuła przyjście pandemii? O tym, a także o rozwoju liderów technologii w Allegro oraz o historii powstania dżingla do naszych podcastów, opowie Piotr Michoński - menadżer Zespołów tworzących infrastrukturę Allegro.","guid":"https://podcast.allegro.tech/infrastruktura_Allegro","isoDate":"2021-06-01T00:00:00.000Z","itunes":{"author":"Piotr Michoński","summary":"Jak jest zbudowane środowisko uruchomienia aplikacji Allegro? Jak działają serwerownie firmy i ile ich potrzeba, a które elementy Allegro działają w chmurze publicznej? Jak przebiegała transformacja w Allegro i co zmieniało się przez lata? Jak wzrost biznesu wpływa na wielkość infrastruktury i jak infrastruktura Allegro odczuła przyjście pandemii? O tym, a także o rozwoju liderów technologii w Allegro oraz o historii powstania dżingla do naszych podcastów, opowie Piotr Michoński - menadżer Zespołów tworzących infrastrukturę Allegro.","explicit":"false"}},{"creator":{"name":["Dariusz Eliasz"]},"title":"Praca architekta ekosystemu big data w Allegro","link":"https://podcast.allegro.tech/praca_architekta_ekosystemu_big_data_w_Allegro","pubDate":"Thu, 20 May 2021 00:00:00 GMT","author":{"name":["Dariusz Eliasz"]},"enclosure":{"url":"https://www.buzzsprout.com/887914/8554742-sezon-ii-10-przetwarzanie-danych-w-allegro-dariusz-eliasz.mp3","type":"audio/mpeg"},"content":"Jak wygląda praca architekta ekosystemu big data w Allegro? Jakie zadania realizuje nasz zespół odpowiedzialny za narzędzia i infrastrukturę dla przetwarzania danych? Kiedy możemy mówić o dużych danych i ile petabajtów przetwarza Allegro? Skąd pochodzą dane Allegro i dlaczego jest ich tak dużo oraz z jakiego powodu dopiero teraz przenosimy się do chmury? O tym wszystkim opowie zdobywca statuetki Allegro Tech Hero - Dariusz Eliasz – Team Manager & Platform Architect w Allegro.","contentSnippet":"Jak wygląda praca architekta ekosystemu big data w Allegro? Jakie zadania realizuje nasz zespół odpowiedzialny za narzędzia i infrastrukturę dla przetwarzania danych? Kiedy możemy mówić o dużych danych i ile petabajtów przetwarza Allegro? Skąd pochodzą dane Allegro i dlaczego jest ich tak dużo oraz z jakiego powodu dopiero teraz przenosimy się do chmury? O tym wszystkim opowie zdobywca statuetki Allegro Tech Hero - Dariusz Eliasz – Team Manager & Platform Architect w Allegro.","guid":"https://podcast.allegro.tech/praca_architekta_ekosystemu_big_data_w_Allegro","isoDate":"2021-05-20T00:00:00.000Z","itunes":{"author":"Dariusz Eliasz","summary":"Jak wygląda praca architekta ekosystemu big data w Allegro? Jakie zadania realizuje nasz zespół odpowiedzialny za narzędzia i infrastrukturę dla przetwarzania danych? Kiedy możemy mówić o dużych danych i ile petabajtów przetwarza Allegro? Skąd pochodzą dane Allegro i dlaczego jest ich tak dużo oraz z jakiego powodu dopiero teraz przenosimy się do chmury? O tym wszystkim opowie zdobywca statuetki Allegro Tech Hero - Dariusz Eliasz – Team Manager & Platform Architect w Allegro.","explicit":"false"}},{"creator":{"name":["Bartosz Gałek"]},"title":"Od inżyniera do lidera w Allegro","link":"https://podcast.allegro.tech/od_inzyniera_do_lidera_w_allegro","pubDate":"Thu, 06 May 2021 00:00:00 GMT","author":{"name":["Bartosz Gałek"]},"enclosure":{"url":"https://www.buzzsprout.com/887914/8455586-sezon-ii-9-od-inzyniera-do-lidera-w-allegro-bartosz-galek.mp3","type":"audio/mpeg"},"content":"Czym jest Opbox i jakie wyzwania przed nim stoją? Jak w Allegro angażujemy się w rozwój kultury Open Source? Ile mamy projektów na GitHubie i jak świętujemy Hacktoberfest? W jaki sposób można rozwinąć się od inżyniera do lidera? Na te pytania w najnowszym Allegro Tech Podcast odpowie Bartek Gałek, Team Leader w Allegro.","contentSnippet":"Czym jest Opbox i jakie wyzwania przed nim stoją? Jak w Allegro angażujemy się w rozwój kultury Open Source? Ile mamy projektów na GitHubie i jak świętujemy Hacktoberfest? W jaki sposób można rozwinąć się od inżyniera do lidera? Na te pytania w najnowszym Allegro Tech Podcast odpowie Bartek Gałek, Team Leader w Allegro.","guid":"https://podcast.allegro.tech/od_inzyniera_do_lidera_w_allegro","isoDate":"2021-05-06T00:00:00.000Z","itunes":{"author":"Bartosz Gałek","summary":"Czym jest Opbox i jakie wyzwania przed nim stoją? Jak w Allegro angażujemy się w rozwój kultury Open Source? Ile mamy projektów na GitHubie i jak świętujemy Hacktoberfest? W jaki sposób można rozwinąć się od inżyniera do lidera? Na te pytania w najnowszym Allegro Tech Podcast odpowie Bartek Gałek, Team Leader w Allegro.","explicit":"false"}},{"creator":{"name":["Michał Bareja"]},"title":"Inżynier w zespole produktowym","link":"https://podcast.allegro.tech/inzynier_w_zespole_produktowym","pubDate":"Thu, 22 Apr 2021 00:00:00 GMT","author":{"name":["Michał Bareja"]},"enclosure":{"url":"https://www.buzzsprout.com/887914/8371888.mp3","type":"audio/mpeg"},"content":"Czym zajmuje się Team Leader w Allegro? Jak wygląda cykl życia oprogramowania w zespole produktowym? Jaka jest rola inżyniera oprogramowania w rozwoju produktu? W jaki sposób wykorzystujemy w Allegro Event Storming? Jakich technologii używają programiści w zespołach produktowych? Między innymi o tym opowie Michał Bareja, Team Manager w Allegro, w rozmowie z Piotrem Betkierem.","contentSnippet":"Czym zajmuje się Team Leader w Allegro? Jak wygląda cykl życia oprogramowania w zespole produktowym? Jaka jest rola inżyniera oprogramowania w rozwoju produktu? W jaki sposób wykorzystujemy w Allegro Event Storming? Jakich technologii używają programiści w zespołach produktowych? Między innymi o tym opowie Michał Bareja, Team Manager w Allegro, w rozmowie z Piotrem Betkierem.","guid":"https://podcast.allegro.tech/inzynier_w_zespole_produktowym","isoDate":"2021-04-22T00:00:00.000Z","itunes":{"author":"Michał Bareja","summary":"Czym zajmuje się Team Leader w Allegro? Jak wygląda cykl życia oprogramowania w zespole produktowym? Jaka jest rola inżyniera oprogramowania w rozwoju produktu? W jaki sposób wykorzystujemy w Allegro Event Storming? Jakich technologii używają programiści w zespołach produktowych? Między innymi o tym opowie Michał Bareja, Team Manager w Allegro, w rozmowie z Piotrem Betkierem.","explicit":"false"}}]},"__N_SSG":true}