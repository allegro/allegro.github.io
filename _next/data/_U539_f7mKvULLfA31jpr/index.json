{"pageProps":{"posts":[{"title":"Impact of data model on MongoDB database size","link":"https://blog.allegro.tech/2021/01/impact-of-the-data-model-on-the-MongoDB-database-size.html","pubDate":"Thu, 14 Jan 2021 00:00:00 +0100","authors":{"author":[{"name":["Michał Knasiecki"],"photo":["https://blog.allegro.tech/img/authors/michal.knasiecki.jpg"],"url":["https://blog.allegro.tech/authors/michal.knasiecki"]}]},"content":"<p>So I was tuning one of our services in order to speed up some MongoDB queries. Incidentally, my attention was caught by\nthe size of one of the collections that contains archived objects and therefore is rarely used. Unfortunately I wasn’t\nable to reduce the size of the documents stored there, but I started to wonder: is it possible to store the same data\nin a more compact way? Mongo stores <code class=\"language-plaintext highlighter-rouge\">JSON</code> that allows many different ways of expressing similar data, so there seems\nto be room for improvements.</p>\n\n<p>It is worth asking: why make such an effort in the era of Big Data and unlimited resources? There are several reasons.</p>\n\n<p>First of all, the resources are not unlimited and at the end we have physical drives that cost money to buy, replace,\nmaintain, and be supplied with power.</p>\n\n<p>Secondly, less stored data results in less time to access it. Moreover, less data means that more of it will fit into\ncache, so the next data access will be an order of magnitude faster.</p>\n\n<p>I decided to do some experiments and check how the model design affects the size of database files.</p>\n\n<p>I used a local MongoDB Community Edition 4.4 installation and I initially tested collections containing 1\nmillion and 10 million documents. One of the variants contained up to 100 million, but the results were proportional\n(nearly linear). Therefore, in the end I decided to stop at 1M collections, because loading the data was simply much faster.</p>\n\n<p>Having access to local database files, I could easily check the size of the files storing individual collections.\nHowever, it turned out to be unnecessary, because the same data can be obtained with the command:</p>\n\n<div class=\"language-sql highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"n\">db</span><span class=\"p\">.</span><span class=\"n\">getCollection</span><span class=\"p\">(</span><span class=\"s1\">'COLLECTION_NAME'</span><span class=\"p\">).</span><span class=\"n\">stats</span><span class=\"p\">()</span>\n</code></pre></div></div>\n\n<p><img src=\"/img/articles/2021-01-14-impact-of-the-data-model-on-the-MongoDB-database-size/collection-stats.png\" alt=\"Collection stats\" /></p>\n\n<p>The following fields are expressed in bytes:</p>\n\n<ul>\n  <li><code class=\"language-plaintext highlighter-rouge\">size</code>: size of all collection documents, before compression,</li>\n  <li><code class=\"language-plaintext highlighter-rouge\">avgObjSize</code>: average document size, before compression,</li>\n  <li><code class=\"language-plaintext highlighter-rouge\">storageSize</code>: file size on the disk; this is the value after the data is compressed, the same value is returned by\n<code class=\"language-plaintext highlighter-rouge\">ls</code> command,</li>\n  <li><code class=\"language-plaintext highlighter-rouge\">freeStorageSize</code>: size of unused space allocated for the data; Mongo does not increase the file size byte-by-byte,\nbut allocates a percentage of the current file size and this value indicates how much data will still fit into the file.</li>\n</ul>\n\n<p>To present the results I used (storageSize - freeStorageSize) value that indicates the actual space occupied by the data.</p>\n\n<p>My local MongoDB instance had compression enabled. Not every storage engine has this option enabled by default, so when\nyou start your own analysis it is worth checking how it is in your particular case.</p>\n\n<h3 id=\"id-field-type\">Id field type</h3>\n\n<p>In the beginning I decided to check <code class=\"language-plaintext highlighter-rouge\">ID</code> fields. Not the collection primary key, mind you – it is ‘ObjectId’ type by\ndefault and generally shouldn’t be changed. I decided to focus on user\nand offers identifiers which, although numerical, are often saved as String in Mongo. I believe it comes partly due to\nthe contract of our services - identifiers in <code class=\"language-plaintext highlighter-rouge\">JSON</code> most often come as Strings and in this form they are later stored\nin our databases.</p>\n\n<p>Let’s start with some theory: the number of <code class=\"language-plaintext highlighter-rouge\">int32</code> type in Mongo has a fixed size of 4 bytes. The same number\nwritten as a <code class=\"language-plaintext highlighter-rouge\">String</code> of characters has a size dependent on the number of digits; additionally it contains the length of\nthe text (4 bytes) and a terminator character. For example, the text “0” is 12 bytes long and “1234567890” is 25 bytes\nlong. So in theory we should get interesting results, but what does it look like in reality?</p>\n\n<p>I prepared 2 collections, one million documents each, containing the following documents:</p>\n\n<div class=\"language-json highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"p\">{</span><span class=\"w\"> </span><span class=\"nl\">\"_id\"</span><span class=\"w\"> </span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"mi\">1</span><span class=\"w\"> </span><span class=\"p\">}</span><span class=\"w\">\n</span></code></pre></div></div>\n\n<p>and</p>\n\n<div class=\"language-json highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"p\">{</span><span class=\"w\"> </span><span class=\"nl\">\"_id\"</span><span class=\"w\"> </span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"1\"</span><span class=\"w\"> </span><span class=\"p\">}</span><span class=\"w\">\n</span></code></pre></div></div>\n\n<p>The values of identifiers were consecutive natural numbers. Here is the comparison of results:</p>\n\n<p><img src=\"/img/articles/2021-01-14-impact-of-the-data-model-on-the-MongoDB-database-size/chart-id.png\" alt=\"String vs int32 size\" /></p>\n\n<p>As you can see the difference is significant since the size on disk decreased by half. Additionally, it is worth\nnoting here that my data is synthetic and the identifiers start from 1. The advantage of a numerical identifier over a\n<code class=\"language-plaintext highlighter-rouge\">String</code> increases the more digits a number has, so benefits should be even better for the real life data.</p>\n\n<p>Encouraged by the success I decided to check if field type had any influence on the size of an index created on it. In\nthis case, unfortunately, I got disappointed: the sizes were similar. This is due to the fact that MongoDB\nuses a hash function when creating indexes, so physically both indexes are composed of numerical values.\nHowever, if we are dealing with hashing, maybe at least searching by index in a numerical field is faster?</p>\n\n<p>I made a comparison of searching for a million and 10 million documents by indexed key in a random but the same order\nfor both collections. Again, a missed shot: both tests ended up with a similar result, so the conclusion is this:\nit is worth using numerical identifiers, because they require less disk space, but we will not get additional benefits\nassociated with indexes on these fields.</p>\n\n<p><img src=\"/img/articles/2021-01-14-impact-of-the-data-model-on-the-MongoDB-database-size/chart-search-1M.png\" alt=\"String vs int32 search time 1M\" /></p>\n\n<p><img src=\"/img/articles/2021-01-14-impact-of-the-data-model-on-the-MongoDB-database-size/chart-search-10M.png\" alt=\"String vs int32 search time 10M\" /></p>\n\n<h3 id=\"simple-and-complex-structures\">Simple and complex structures</h3>\n\n<p>Let’s move on to more complex data. Our models are often built from smaller structures grouping data such as user,\norder or offer. I decided to compare the size of documents storing such structures and their flat counterparts:</p>\n\n<div class=\"language-json highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"p\">{</span><span class=\"nl\">\"_id\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"mi\">1</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"nl\">\"user\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"p\">{</span><span class=\"nl\">\"name\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"Jan\"</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"nl\">\"surname\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"Nowak\"</span><span class=\"p\">}}</span><span class=\"w\">\n</span></code></pre></div></div>\n\n<p>and</p>\n\n<div class=\"language-json highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"p\">{</span><span class=\"nl\">\"_id\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"mi\">1</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"nl\">\"userName\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"Jan\"</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"nl\">\"userSurname\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"Nowak\"</span><span class=\"p\">}</span><span class=\"w\">\n</span></code></pre></div></div>\n\n<p>In both cases we store identical data, the documents differ only in the schema. Take a look at the result:</p>\n\n<p><img src=\"/img/articles/2021-01-14-impact-of-the-data-model-on-the-MongoDB-database-size/chart-struct-1.png\" alt=\"complex vs simple size\" /></p>\n\n<p>There is a slight reduction by 0.4MB. It may seem not much compared to the effect we achieved for the field\ncontaining an ID. However, we have to bear in mind that in this case we were dealing with a more complex document. It\ncontained – in addition to the compared fields – a numerical type identifier that, as we remember from previous\nexperiments, takes up about 5MB of disk space.</p>\n\n<p>Taking this into account in the above results we are talking about a decrease from 3.4MB to 3MB. It actually looks\nbetter as percentage - we saved 12% of the space needed to store personal data.</p>\n\n<p>Let’s go back to the discussed documents for a moment:</p>\n\n<div class=\"language-json highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"p\">{</span><span class=\"nl\">\"_id\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"mi\">1</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"nl\">\"user\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"p\">{</span><span class=\"nl\">\"name\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"Jan\"</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"nl\">\"surname\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"Nowak\"</span><span class=\"p\">}}</span><span class=\"w\">\n</span></code></pre></div></div>\n\n<p>and</p>\n\n<div class=\"language-json highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"p\">{</span><span class=\"nl\">\"_id\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"mi\">1</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"nl\">\"userName\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"Jan\"</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"nl\">\"userSurname\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"Nowak\"</span><span class=\"p\">}</span><span class=\"w\">\n</span></code></pre></div></div>\n\n<p>A watchful eye will notice that I used longer field names in the document after flattening. So instead of <code class=\"language-plaintext highlighter-rouge\">user.name</code>\nand <code class=\"language-plaintext highlighter-rouge\">user.surname</code> I made <code class=\"language-plaintext highlighter-rouge\">userName</code> and <code class=\"language-plaintext highlighter-rouge\">userSurname</code>. I did it automatically, a bit unconsciously, to make the\nresulting <code class=\"language-plaintext highlighter-rouge\">JSON</code> more readable.  However, if by changing only the schema of the document from compound to flat we managed\nto reduce the size of the data, maybe it is worth to go a step further and shorten the field names?</p>\n\n<p>I decided to add a third document for comparison, flattened and with shorter field names:</p>\n\n<div class=\"language-json highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"p\">{</span><span class=\"nl\">\"_id\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"mi\">1</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"nl\">\"name\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"Jan\"</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"nl\">\"surname\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"Nowak\"</span><span class=\"p\">}</span><span class=\"w\">\n</span></code></pre></div></div>\n\n<p>The results are shown in the chart below:</p>\n\n<p><img src=\"/img/articles/2021-01-14-impact-of-the-data-model-on-the-MongoDB-database-size/chart-struct-2.png\" alt=\"complex vs simple vs short size\" /></p>\n\n<p>The result is even better than just flattening. Apart from the document’s key size, we achieved a decrease from\n3.4MB to 2MB. Why does this happen even though we store exactly the same data?</p>\n\n<p>The reason for the decrease is the nature of NoSQL databases that, unlike the relational ones, do not have a schema\ndefined at the level of the entire collection. If someone is very stubborn, they can store user data, offers, orders and\npayments in one collection. It would still be possible to read, index and search that data. This is because the\ndatabase, in addition to the data itself, stores its schema with each document. Thus, the total size of a document\nconsists of its data and its schema. And that explains the whole puzzle. By reducing the size of the schema, we also\nreduce the size of each document, i.e. the size of the final file with the collection data. It is worth taking this into\naccount when designing a collection schema in order not to blow it up too much. Of course, you cannot go to extremes, because\nthat would lead us to fields named <code class=\"language-plaintext highlighter-rouge\">a</code>, <code class=\"language-plaintext highlighter-rouge\">b</code> and <code class=\"language-plaintext highlighter-rouge\">c</code>, what would make the collection completely illegible for a human.</p>\n\n<p>For very large collections, however, this approach is used, an example of which is the MongoDB operation log\nthat contains fields called:</p>\n\n<ul>\n  <li>h</li>\n  <li>op</li>\n  <li>ns</li>\n  <li>o</li>\n  <li>o2</li>\n  <li>b</li>\n</ul>\n\n<h3 id=\"empty-fields\">Empty fields</h3>\n\n<p>Since we are at the document’s schema, it is still worth looking at the problem of blank fields. In the case of\n<code class=\"language-plaintext highlighter-rouge\">JSON</code> the lack of value in a certain field can be written in two ways, either directly - by writing null in its value -\nor by not serializing the field at all. I prepared a comparison of documents with the following structure:</p>\n\n<div class=\"language-json highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"p\">{</span><span class=\"w\"> </span><span class=\"nl\">\"id\"</span><span class=\"w\"> </span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"mi\">1</span><span class=\"w\"> </span><span class=\"p\">}</span><span class=\"w\">\n</span></code></pre></div></div>\n\n<p>and</p>\n\n<div class=\"language-json highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"p\">{</span><span class=\"w\"> </span><span class=\"nl\">\"id\"</span><span class=\"w\"> </span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"mi\">1</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"nl\">\"phone\"</span><span class=\"w\"> </span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"kc\">null</span><span class=\"p\">}</span><span class=\"w\">\n</span></code></pre></div></div>\n\n<p>The meaning of the data in both documents is identical - the user has no phone number. However, the schema of the second\ndocument is different from the first one because it contains two fields.</p>\n\n<p>Here are the results:</p>\n\n<p><img src=\"/img/articles/2021-01-14-impact-of-the-data-model-on-the-MongoDB-database-size/chart-null.png\" alt=\"null vs empty size\" /></p>\n\n<p>The results are quite surprising: saving a million null’s is quite expensive as it takes more than 1MB on a disk.</p>\n\n<h3 id=\"enumerated-types\">Enumerated types</h3>\n\n<p>Now, let’s also look at the enumerated types. You can store them in two ways, either by using a label:</p>\n\n<div class=\"language-json highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"p\">{</span><span class=\"nl\">\"_id\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"mi\">1</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"nl\">\"source\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"ALLEGRO\"</span><span class=\"p\">}</span><span class=\"w\">\n</span></code></pre></div></div>\n\n<p>or by ordinal value:</p>\n\n<div class=\"language-json highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"p\">{</span><span class=\"nl\">\"_id\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"mi\">1</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"nl\">\"source\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"mi\">1</span><span class=\"p\">}</span><span class=\"w\">\n</span></code></pre></div></div>\n\n<p>Here an analogy with the first experiment can be found: again we replace a character string with a numerical value.\nSince we got a great result the first time, maybe we could repeat it here?</p>\n\n<p><img src=\"/img/articles/2021-01-14-impact-of-the-data-model-on-the-MongoDB-database-size/chart-enums-1.png\" alt=\"label vs index size\" /></p>\n\n<p>Unfortunately, the result is disappointing and the reduction in size is small. However, if we think more deeply, we will\ncome to the conclusion that it could not have been otherwise. The enumerated types are not unique identifiers. We are\ndealing only with a few possible values, appearing many times in the whole collection, so it’s a great chance for\nMongoDB data compression to prove its worth. Most likely the values from the first collection have been automatically replaced by the\nengine to its corresponding ordinal values. Additionally, we don’t have any profit on the schema here, because they are\nthe same in both cases. The situation might be different for collections that do not have data compression enabled.</p>\n\n<p>This is a good time to take a closer look at how <a href=\"https://www.mongodb.com/blog/post/new-compression-options-mongodb-30\">snappy</a>\ncompression works in MongoDB. I’ve prepared two more collections, with identical data, but with data compression\nturned off. The results are shown in the chart below, compiled together with the collections with data compression turned on.</p>\n\n<p><img src=\"/img/articles/2021-01-14-impact-of-the-data-model-on-the-MongoDB-database-size/chart-enums-2.png\" alt=\"snappy vs plain size\" /></p>\n\n<p>It is clear that the use of an ordinal value instead of a label of the enumerated type brings considerable profit for\ncollections with data compression disabled. In case of lack of compression it is definitely worth considering using numerical\nvalues.</p>\n\n<h3 id=\"useless-_class-field\">Useless _class field</h3>\n\n<p>If we use <code class=\"language-plaintext highlighter-rouge\">spring-data</code>, each of our collections additionally contains a <code class=\"language-plaintext highlighter-rouge\">_class</code> field storing the package and the\nname of the entity class containing the mapping for documents from this collection. This mechanism is used to support\ninheritance of model classes, that is not widely used. In most cases this field stores exactly the same values for each\ndocument what makes it useless. And how much does it cost? Let’s compare the following documents:</p>\n\n<div class=\"language-json highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"p\">{</span><span class=\"nl\">\"_id\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"mi\">1</span><span class=\"p\">}</span><span class=\"w\">\n</span></code></pre></div></div>\n\n<p>and</p>\n\n<div class=\"language-json highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"p\">{</span><span class=\"nl\">\"_id\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"mi\">1</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"nl\">\"_class\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"pl.allegro.some.project.domain.sample\"</span><span class=\"p\">}</span><span class=\"w\">\n</span></code></pre></div></div>\n\n<p><img src=\"/img/articles/2021-01-14-impact-of-the-data-model-on-the-MongoDB-database-size/chart-class.png\" alt=\"_class field size\" /></p>\n\n<p>The difference is considerable, over 50%. Bearing in mind that the compression is enabled, I believe that the impact of\nthe data itself is small and the result is caused by the schema of the collection containing only the key being half as small\nas the one with the key (1 field vs 2 fields). After removal of the <code class=\"language-plaintext highlighter-rouge\">_class</code> field from a document with more fields,\nthe difference expressed in percent will be much smaller of course. However, storing useless data does not make sense.</p>\n\n<h3 id=\"useless-indexes\">Useless indexes</h3>\n\n<p>When investigating the case with identifiers stored as strings, I checked whether manipulating the data type could reduce\nthe index size. Unfortunately I did not succeed, so I decided to focus on the problem of redundant indexes.</p>\n\n<p>It is usually a good practice to cover each query with an index so that the number of <code class=\"language-plaintext highlighter-rouge\">collscan</code> operations is as small as\npossible. This often leads to situations where we have too many indexes. We add more queries, create new indexes for\nthem, and often it turns out that this newly created index takes over the queries of another one.\nAs a result, we have to maintain many unnecessary indexes, wasting disk space and CPU time.</p>\n\n<p>Fortunately, it is possible to check the number of index uses with the query:</p>\n\n<div class=\"language-sql highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"n\">db</span><span class=\"p\">.</span><span class=\"n\">COLLECTION_NAME</span><span class=\"p\">.</span><span class=\"k\">aggregate</span><span class=\"p\">([</span><span class=\"err\">{$</span><span class=\"n\">indexStats</span><span class=\"p\">:</span><span class=\"err\">{}}</span><span class=\"p\">])</span>\n</code></pre></div></div>\n\n<p>It allows you to find indexes that are not used so you can safely delete them.</p>\n\n<p>And finally, one more thing. Indexes can be removed quickly, but they take much longer to rebuild. This means that the\nconsequences of removal of an index by mistake can be severe. Therefore it is better to make sure that the index to be\ndeleted is definitely not used by any query. The latest MongoDB 4.4 provides a command that helps to avoid errors:</p>\n\n<div class=\"language-sql highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"n\">db</span><span class=\"p\">.</span><span class=\"n\">COLLECTION_NAME</span><span class=\"p\">.</span><span class=\"n\">hideIndex</span><span class=\"p\">(</span><span class=\"o\">&lt;</span><span class=\"k\">index</span><span class=\"o\">&gt;</span><span class=\"p\">)</span>\n</code></pre></div></div>\n\n<p>The above-mentioned command hides the index from the query optimizer. It does not take this index into account when\nbuilding the query execution plan, but the index is still updated when modifying documents. Thanks to that, if it turns\nout that it was needed, we are able to restore it immediately and it will still be up-to-date.</p>\n\n<h3 id=\"conclusion\">Conclusion</h3>\n\n<p>Using a few simple techniques, preparing several versions of the schema and using stats() command we are able to design\na model which does not overload our infrastructure. I encourage you to experiment with your own databases. Maybe you too\ncan save some disk space and CPU time.</p>\n","contentSnippet":"So I was tuning one of our services in order to speed up some MongoDB queries. Incidentally, my attention was caught by\nthe size of one of the collections that contains archived objects and therefore is rarely used. Unfortunately I wasn’t\nable to reduce the size of the documents stored there, but I started to wonder: is it possible to store the same data\nin a more compact way? Mongo stores JSON that allows many different ways of expressing similar data, so there seems\nto be room for improvements.\nIt is worth asking: why make such an effort in the era of Big Data and unlimited resources? There are several reasons.\nFirst of all, the resources are not unlimited and at the end we have physical drives that cost money to buy, replace,\nmaintain, and be supplied with power.\nSecondly, less stored data results in less time to access it. Moreover, less data means that more of it will fit into\ncache, so the next data access will be an order of magnitude faster.\nI decided to do some experiments and check how the model design affects the size of database files.\nI used a local MongoDB Community Edition 4.4 installation and I initially tested collections containing 1\nmillion and 10 million documents. One of the variants contained up to 100 million, but the results were proportional\n(nearly linear). Therefore, in the end I decided to stop at 1M collections, because loading the data was simply much faster.\nHaving access to local database files, I could easily check the size of the files storing individual collections.\nHowever, it turned out to be unnecessary, because the same data can be obtained with the command:\n\ndb.getCollection('COLLECTION_NAME').stats()\n\n\n\nThe following fields are expressed in bytes:\nsize: size of all collection documents, before compression,\navgObjSize: average document size, before compression,\nstorageSize: file size on the disk; this is the value after the data is compressed, the same value is returned by\nls command,\nfreeStorageSize: size of unused space allocated for the data; Mongo does not increase the file size byte-by-byte,\nbut allocates a percentage of the current file size and this value indicates how much data will still fit into the file.\nTo present the results I used (storageSize - freeStorageSize) value that indicates the actual space occupied by the data.\nMy local MongoDB instance had compression enabled. Not every storage engine has this option enabled by default, so when\nyou start your own analysis it is worth checking how it is in your particular case.\nId field type\nIn the beginning I decided to check ID fields. Not the collection primary key, mind you – it is ‘ObjectId’ type by\ndefault and generally shouldn’t be changed. I decided to focus on user\nand offers identifiers which, although numerical, are often saved as String in Mongo. I believe it comes partly due to\nthe contract of our services - identifiers in JSON most often come as Strings and in this form they are later stored\nin our databases.\nLet’s start with some theory: the number of int32 type in Mongo has a fixed size of 4 bytes. The same number\nwritten as a String of characters has a size dependent on the number of digits; additionally it contains the length of\nthe text (4 bytes) and a terminator character. For example, the text “0” is 12 bytes long and “1234567890” is 25 bytes\nlong. So in theory we should get interesting results, but what does it look like in reality?\nI prepared 2 collections, one million documents each, containing the following documents:\n\n{ \"_id\" : 1 }\n\n\nand\n\n{ \"_id\" : \"1\" }\n\n\nThe values of identifiers were consecutive natural numbers. Here is the comparison of results:\n\nAs you can see the difference is significant since the size on disk decreased by half. Additionally, it is worth\nnoting here that my data is synthetic and the identifiers start from 1. The advantage of a numerical identifier over a\nString increases the more digits a number has, so benefits should be even better for the real life data.\nEncouraged by the success I decided to check if field type had any influence on the size of an index created on it. In\nthis case, unfortunately, I got disappointed: the sizes were similar. This is due to the fact that MongoDB\nuses a hash function when creating indexes, so physically both indexes are composed of numerical values.\nHowever, if we are dealing with hashing, maybe at least searching by index in a numerical field is faster?\nI made a comparison of searching for a million and 10 million documents by indexed key in a random but the same order\nfor both collections. Again, a missed shot: both tests ended up with a similar result, so the conclusion is this:\nit is worth using numerical identifiers, because they require less disk space, but we will not get additional benefits\nassociated with indexes on these fields.\n\n\nSimple and complex structures\nLet’s move on to more complex data. Our models are often built from smaller structures grouping data such as user,\norder or offer. I decided to compare the size of documents storing such structures and their flat counterparts:\n\n{\"_id\": 1, \"user\": {\"name\": \"Jan\", \"surname\": \"Nowak\"}}\n\n\nand\n\n{\"_id\": 1, \"userName\": \"Jan\", \"userSurname\": \"Nowak\"}\n\n\nIn both cases we store identical data, the documents differ only in the schema. Take a look at the result:\n\nThere is a slight reduction by 0.4MB. It may seem not much compared to the effect we achieved for the field\ncontaining an ID. However, we have to bear in mind that in this case we were dealing with a more complex document. It\ncontained – in addition to the compared fields – a numerical type identifier that, as we remember from previous\nexperiments, takes up about 5MB of disk space.\nTaking this into account in the above results we are talking about a decrease from 3.4MB to 3MB. It actually looks\nbetter as percentage - we saved 12% of the space needed to store personal data.\nLet’s go back to the discussed documents for a moment:\n\n{\"_id\": 1, \"user\": {\"name\": \"Jan\", \"surname\": \"Nowak\"}}\n\n\nand\n\n{\"_id\": 1, \"userName\": \"Jan\", \"userSurname\": \"Nowak\"}\n\n\nA watchful eye will notice that I used longer field names in the document after flattening. So instead of user.name\nand user.surname I made userName and userSurname. I did it automatically, a bit unconsciously, to make the\nresulting JSON more readable.  However, if by changing only the schema of the document from compound to flat we managed\nto reduce the size of the data, maybe it is worth to go a step further and shorten the field names?\nI decided to add a third document for comparison, flattened and with shorter field names:\n\n{\"_id\": 1, \"name\": \"Jan\", \"surname\": \"Nowak\"}\n\n\nThe results are shown in the chart below:\n\nThe result is even better than just flattening. Apart from the document’s key size, we achieved a decrease from\n3.4MB to 2MB. Why does this happen even though we store exactly the same data?\nThe reason for the decrease is the nature of NoSQL databases that, unlike the relational ones, do not have a schema\ndefined at the level of the entire collection. If someone is very stubborn, they can store user data, offers, orders and\npayments in one collection. It would still be possible to read, index and search that data. This is because the\ndatabase, in addition to the data itself, stores its schema with each document. Thus, the total size of a document\nconsists of its data and its schema. And that explains the whole puzzle. By reducing the size of the schema, we also\nreduce the size of each document, i.e. the size of the final file with the collection data. It is worth taking this into\naccount when designing a collection schema in order not to blow it up too much. Of course, you cannot go to extremes, because\nthat would lead us to fields named a, b and c, what would make the collection completely illegible for a human.\nFor very large collections, however, this approach is used, an example of which is the MongoDB operation log\nthat contains fields called:\nh\n  \nop\nns\no\n  \no2\nb\n\n\nEmpty fields\nSince we are at the document’s schema, it is still worth looking at the problem of blank fields. In the case of\nJSON the lack of value in a certain field can be written in two ways, either directly - by writing null in its value -\nor by not serializing the field at all. I prepared a comparison of documents with the following structure:\n\n{ \"id\" : 1 }\n\n\nand\n\n{ \"id\" : 1, \"phone\" : null}\n\n\nThe meaning of the data in both documents is identical - the user has no phone number. However, the schema of the second\ndocument is different from the first one because it contains two fields.\nHere are the results:\n\nThe results are quite surprising: saving a million null’s is quite expensive as it takes more than 1MB on a disk.\nEnumerated types\nNow, let’s also look at the enumerated types. You can store them in two ways, either by using a label:\n\n{\"_id\": 1, \"source\": \"ALLEGRO\"}\n\n\nor by ordinal value:\n\n{\"_id\": 1, \"source\": 1}\n\n\nHere an analogy with the first experiment can be found: again we replace a character string with a numerical value.\nSince we got a great result the first time, maybe we could repeat it here?\n\nUnfortunately, the result is disappointing and the reduction in size is small. However, if we think more deeply, we will\ncome to the conclusion that it could not have been otherwise. The enumerated types are not unique identifiers. We are\ndealing only with a few possible values, appearing many times in the whole collection, so it’s a great chance for\nMongoDB data compression to prove its worth. Most likely the values from the first collection have been automatically replaced by the\nengine to its corresponding ordinal values. Additionally, we don’t have any profit on the schema here, because they are\nthe same in both cases. The situation might be different for collections that do not have data compression enabled.\nThis is a good time to take a closer look at how snappy\ncompression works in MongoDB. I’ve prepared two more collections, with identical data, but with data compression\nturned off. The results are shown in the chart below, compiled together with the collections with data compression turned on.\n\nIt is clear that the use of an ordinal value instead of a label of the enumerated type brings considerable profit for\ncollections with data compression disabled. In case of lack of compression it is definitely worth considering using numerical\nvalues.\nUseless _class field\nIf we use spring-data, each of our collections additionally contains a _class field storing the package and the\nname of the entity class containing the mapping for documents from this collection. This mechanism is used to support\ninheritance of model classes, that is not widely used. In most cases this field stores exactly the same values for each\ndocument what makes it useless. And how much does it cost? Let’s compare the following documents:\n\n{\"_id\": 1}\n\n\nand\n\n{\"_id\": 1, \"_class\": \"pl.allegro.some.project.domain.sample\"}\n\n\n\nThe difference is considerable, over 50%. Bearing in mind that the compression is enabled, I believe that the impact of\nthe data itself is small and the result is caused by the schema of the collection containing only the key being half as small\nas the one with the key (1 field vs 2 fields). After removal of the _class field from a document with more fields,\nthe difference expressed in percent will be much smaller of course. However, storing useless data does not make sense.\nUseless indexes\nWhen investigating the case with identifiers stored as strings, I checked whether manipulating the data type could reduce\nthe index size. Unfortunately I did not succeed, so I decided to focus on the problem of redundant indexes.\nIt is usually a good practice to cover each query with an index so that the number of collscan operations is as small as\npossible. This often leads to situations where we have too many indexes. We add more queries, create new indexes for\nthem, and often it turns out that this newly created index takes over the queries of another one.\nAs a result, we have to maintain many unnecessary indexes, wasting disk space and CPU time.\nFortunately, it is possible to check the number of index uses with the query:\n\ndb.COLLECTION_NAME.aggregate([{$indexStats:{}}])\n\n\nIt allows you to find indexes that are not used so you can safely delete them.\nAnd finally, one more thing. Indexes can be removed quickly, but they take much longer to rebuild. This means that the\nconsequences of removal of an index by mistake can be severe. Therefore it is better to make sure that the index to be\ndeleted is definitely not used by any query. The latest MongoDB 4.4 provides a command that helps to avoid errors:\n\ndb.COLLECTION_NAME.hideIndex(<index>)\n\n\nThe above-mentioned command hides the index from the query optimizer. It does not take this index into account when\nbuilding the query execution plan, but the index is still updated when modifying documents. Thanks to that, if it turns\nout that it was needed, we are able to restore it immediately and it will still be up-to-date.\nConclusion\nUsing a few simple techniques, preparing several versions of the schema and using stats() command we are able to design\na model which does not overload our infrastructure. I encourage you to experiment with your own databases. Maybe you too\ncan save some disk space and CPU time.","guid":"https://blog.allegro.tech/2021/01/impact-of-the-data-model-on-the-MongoDB-database-size.html","categories":["mongodb"],"isoDate":"2021-01-13T23:00:00.000Z","thumbnail":"images/post-headers/mongodb.png"},{"title":"Speeding up warm builds in Xcode","link":"https://blog.allegro.tech/2020/12/speeding-up-warm-builds.html","pubDate":"Mon, 28 Dec 2020 00:00:00 +0100","authors":{"author":[{"name":["Maciej Piotrowski"],"photo":["https://blog.allegro.tech/img/authors/maciej.piotrowski.jpg"],"url":["https://blog.allegro.tech/authors/maciej.piotrowski"]}]},"content":"<p>Programmers who have ever developed software for Apple platforms in the early days of <strong>Swift</strong> language might remember ridiculous\ntimes it took to compile the whole project. For large and complicated codebase times used to range from 10 up to 40 minutes.\nOver the years our toolset has improved alongside with compilation times, but slow build times of source code can still be a nightmare.</p>\n\n<p>When we wait a few minutes for a build, we navigate ourselves towards different activities and start e.g. watching funny animal pictures or\nYouTube videos, easily <strong>loosing context</strong> of the task at hand. What becomes annoying for us is <strong>slow feedback</strong> of code correctness.</p>\n\n<p>In the <a href=\"https://allegro.tech/2020/12/speeding-up-ios-builds-with-bazel.html\">past issue</a> my colleague has written about a solution\nto slow <strong>clean</strong> builds.In this post I will focus on <strong>warm</strong> builds improvement.</p>\n\n<h2 id=\"clean-and-incremental-builds\">Clean and incremental builds</h2>\n\n<p>There are two terms used in the realm of <a href=\"https://developer.apple.com/xcode/\">Xcode</a> when it comes to distinguishing types of\ncompilation: <strong>clean</strong> and <strong>incremental</strong> build. The first refers to the time it takes to build a project from scratch. The latter is the time\nit takes to build only whatever changed since the last build and to integrate the changes into a build product.</p>\n\n<p>You might also be familiar with the term <strong>warm</strong> build. It’s used interchangeably with <strong>incremental</strong> build term, but for the sake of this\npost I will be using it to refer to <em>the time it takes to build a product since the last clean build without introducing any source code\nchanges</em>.</p>\n\n<h2 id=\"why-and-what-for\">Why and what for</h2>\n\n<p>Why bothering with improving <strong>warm</strong> build times? Well, for a small projects built on super fast workstations it might take a fraction of a\nsecond to do a warm build, but as projects grow and multiple <em>Build Phases</em> get added to a target so grow the build times. These times\nare noticeable especially when one builds the target without an introduction of changes to the source code.</p>\n\n<p>Before we started improving the warm build of the Allegro app for iOS platform it took 18 seconds to perform the build on our\nContinuous Integration (CI) servers (Mac Mini, 6-Core 3.2 GHz CPU, 32 GB Ram).</p>\n\n<p>Is 18 seconds too much? When you put it into a perspective of 1 year:</p>\n\n<blockquote>\n  <p>18 seconds × 6 builds per hour × 8 hours per day × 20 days per month × 12 months per year = 207360 seconds = 3456 minutes = 57\nhours 36 minutes</p>\n</blockquote>\n\n<p>It means that on average a programmer spends around 57 hours 36 minutes yearly to wait for a feedback if their code is correct. Is it\nmuch? I leave the answer to you, but it definitely hinders developer’s experience and distracts the developer from their job.</p>\n\n<p>To make the developer’s experience better, we, the iOS Mobile Core Team at Allegro, have set the goal to minimize the time developers\nspend between hitting the build button and getting the feedback on their code as quickly as possible.</p>\n\n<p>How could the goal be achieved? Well, before I answer that, let’s put some light onto how to actually measure build times.</p>\n\n<h2 id=\"measurements\">Measurements</h2>\n\n<p>Developers building software for Apple platforms use the <a href=\"https://developer.apple.com/xcode/\">Xcode</a> application which has a command\nline interface called <code class=\"language-plaintext highlighter-rouge\">xcodebuild</code>. The Xcode has an option to output times for build phases from the menu\n<code class=\"language-plaintext highlighter-rouge\">Product &gt; Perform Action &gt; Build With Timing Summary</code> (doesn’t seem to work on Xcode 12.2 at the time of writing this\nblog post). To get build times with <code class=\"language-plaintext highlighter-rouge\">xcodebuild</code> for our Allegro app for each build phase of the main target the following command\ncan be used:</p>\n\n<div class=\"language-sh highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>xcodebuild <span class=\"nt\">-workspace</span> <span class=\"s1\">'Allegro/Allegro.xcworkspace'</span> <span class=\"se\">\\</span>\n<span class=\"nt\">-scheme</span> <span class=\"s1\">'Allegro'</span> <span class=\"se\">\\</span>\n<span class=\"nt\">-configuration</span> <span class=\"s1\">'Debug'</span> <span class=\"se\">\\</span>\n<span class=\"nt\">-sdk</span> <span class=\"s1\">'iphonesimulator'</span> <span class=\"se\">\\</span>\n<span class=\"nt\">-arch</span> <span class=\"s1\">'x86_64'</span> <span class=\"se\">\\</span>\n<span class=\"nt\">-showBuildTimingSummary</span> <span class=\"se\">\\</span>\nbuild <span class=\"se\">\\</span>\n| <span class=\"nb\">sed</span> <span class=\"nt\">-n</span> <span class=\"nt\">-e</span> <span class=\"s1\">'/Build Timing Summary/,$p'</span>\n</code></pre></div></div>\n\n<p>In the case of the Allegro app it outputs the following lines when I do a <strong>clean</strong> build with Xcode 12.2’s <code class=\"language-plaintext highlighter-rouge\">xcodebuild</code>\n(Mac Book Pro 2.2 GHz 6-Core Intel Core i7 CPU, 32 GB RAM):</p>\n\n<div class=\"language-sh highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>Build Timing Summary\n\nCompileC <span class=\"o\">(</span>49 tasks<span class=\"o\">)</span> | 174.459 seconds\n\nCompileSwiftSources <span class=\"o\">(</span>3 tasks<span class=\"o\">)</span> | 31.747 seconds\n\nCompileStoryboard <span class=\"o\">(</span>6 tasks<span class=\"o\">)</span> | 29.057 seconds\n\nPhaseScriptExecution <span class=\"o\">(</span>8 tasks<span class=\"o\">)</span> | 22.320 seconds\n\nDitto <span class=\"o\">(</span>21 tasks<span class=\"o\">)</span> | 22.282 seconds\n\nLd <span class=\"o\">(</span>3 tasks<span class=\"o\">)</span> | 13.432 seconds\n\nCompileAssetCatalog <span class=\"o\">(</span>1 task<span class=\"o\">)</span> | 6.620 seconds\n\nValidateEmbeddedBinary <span class=\"o\">(</span>2 tasks<span class=\"o\">)</span> | 6.528 seconds\n\nCompileXIB <span class=\"o\">(</span>1 task<span class=\"o\">)</span> | 5.000 seconds\n\nCodeSign <span class=\"o\">(</span>3 tasks<span class=\"o\">)</span> | 1.419 seconds\n\nCopyPNGFile <span class=\"o\">(</span>3 tasks<span class=\"o\">)</span> | 1.236 seconds\n\nTouch <span class=\"o\">(</span>4 tasks<span class=\"o\">)</span> | 0.318 seconds\n\nLibtool <span class=\"o\">(</span>1 task<span class=\"o\">)</span> | 0.241 seconds\n\nLinkStoryboards <span class=\"o\">(</span>2 tasks<span class=\"o\">)</span> | 0.108 seconds\n</code></pre></div></div>\n\n<p>There’s a lot of tasks. When it comes to source code compilation process, we can lower down build times by splitting the source code\ninto modular frameworks, using binary caching techniques and adding explicit types for expressions in Swift.</p>\n\n<p>In the case of a <strong>warm build</strong> the only phases listed are:</p>\n\n<div class=\"language-sh highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>Build Timing Summary\n\nPhaseScriptExecution <span class=\"o\">(</span>6 tasks<span class=\"o\">)</span> | 23.350 seconds\n\nValidateEmbeddedBinary <span class=\"o\">(</span>2 tasks<span class=\"o\">)</span> | 2.424 seconds\n\n<span class=\"k\">**</span> BUILD SUCCEEDED <span class=\"k\">**</span> <span class=\"o\">[</span>27.238 sec]\n</code></pre></div></div>\n\n<p>Thanks to performing the <strong>warm build</strong> it can be easily noticed that there’s a room for improvement when it comes to\n<code class=\"language-plaintext highlighter-rouge\">PhaseScriptExecution</code> part of the build process. This is actually the part over which we have the control of. Let’s see, what we can\ndo in order to speed up the build time by playing with what and how scripts get executed.</p>\n\n<h2 id=\"cleaning-up-run-scripts\">Cleaning up run scripts</h2>\n\n<p>First thing we did with for our iOS application target was selecting scripts which can be run only for Release builds. There’s an easy way\nin Xcode to mark them as runnable for such builds only - just select <code class=\"language-plaintext highlighter-rouge\">For install builds only</code> checkbox.</p>\n\n<p><img src=\"/img/articles/2020-12-28-speeding-up-warm-builds/xcode-run-for-release.png\" alt=\"Run script: For install builds only - checkbox in Xcode\" /></p>\n\n<p>What jobs are great for running only for Release builds? We selected a few:</p>\n\n<ul>\n  <li>uploading debug symbols to 3rd party monitoring services</li>\n  <li>setting endpoints or enabling Apple Transport Security (ATS) for Debug/Release builds</li>\n  <li>selecting proper <code class=\"language-plaintext highlighter-rouge\">.plist</code> files for Debug/Release builds</li>\n</ul>\n\n<p>Not all tasks can be selected as Release - only. Some of them need to be run for Debug and Release builds, but they don’t have to be\nrun for every build. Xcode 12 introduced a neat feature - running the script based on dependency analysis.</p>\n\n<p><img src=\"/img/articles/2020-12-28-speeding-up-warm-builds/xcode-dependency-analysis.png\" alt=\"Run script: Based on dependency analysis - checkbox in Xcode\" /></p>\n\n<p>Selecting the checkbox isn’t enough to benefit from dependency analysis. Xcode analyses dependencies of a script, i.e. it verifies if the\ninputs of the script have changed since the last run and if the outputs of the script exist. The potential problem occurred for scripts in our\nproject - they didn’t have explicit inputs and outputs defined so we couldn’t tap into the brand new feature of Xcode.</p>\n\n<h2 id=\"defining-inputs-and-outputs-for-scripts\">Defining inputs and outputs for scripts</h2>\n\n<p>One of the scripts in our project which is time-consuming copies bundles with resources of each module. Our Xcode workspace consists\nof multiple projects. The main project contains the application target which depends on modules built by other projects. The projects\ncontain static frameworks with resources. The resources for each framework are wrapped in <code class=\"language-plaintext highlighter-rouge\">.bundle</code> wrapper and are embedded in\nthe framework. All frameworks are linked statically to the application and their bundles are copied by the script to the application\nwrapper (<code class=\"language-plaintext highlighter-rouge\">.app</code>).</p>\n\n<p>The list with <code class=\"language-plaintext highlighter-rouge\">.bundle</code> files to be copied became an input to our script. We also created a list with paths to which bundles are copied.\nXcode uses a <code class=\"language-plaintext highlighter-rouge\">.xcfilelist</code> format for such lists, but it’s just a file with newline-separated values. The\n<code class=\"language-plaintext highlighter-rouge\">copy-bundles-input.xcfilelist</code> input to our script looks as such:</p>\n\n<div class=\"language-sh highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"si\">$(</span>BUILT_PRODUCTS_DIR<span class=\"si\">)</span>/ModuleX.framework/ModuleX.bundle\n<span class=\"si\">$(</span>BUILT_PRODUCTS_DIR<span class=\"si\">)</span>/ModuleY.framework/ModuleY.bundle\n<span class=\"si\">$(</span>BUILT_PRODUCTS_DIR<span class=\"si\">)</span>/ModuleZ.framework/ModuleZ.bundle\n</code></pre></div></div>\n\n<p>and the <code class=\"language-plaintext highlighter-rouge\">copy-bundles-output.xcfilelist</code> output:</p>\n\n<div class=\"language-sh highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"si\">$(</span>TARGET_BUILD_DIR<span class=\"si\">)</span>/<span class=\"si\">$(</span>EXECUTABLE_FOLDER_PATH<span class=\"si\">)</span>/ModuleX.bundle\n<span class=\"si\">$(</span>TARGET_BUILD_DIR<span class=\"si\">)</span>/<span class=\"si\">$(</span>EXECUTABLE_FOLDER_PATH<span class=\"si\">)</span>/ModuleY.bundle\n<span class=\"si\">$(</span>TARGET_BUILD_DIR<span class=\"si\">)</span>/<span class=\"si\">$(</span>EXECUTABLE_FOLDER_PATH<span class=\"si\">)</span>/ModuleZ.bundle\n</code></pre></div></div>\n\n<p>File lists can be accessed in a script through environment variables. Each script can have many of them and they are indexed from 0:</p>\n\n<ul>\n  <li><code class=\"language-plaintext highlighter-rouge\">SCRIPT_INPUT_FILE_LIST_0</code></li>\n  <li>…</li>\n  <li><code class=\"language-plaintext highlighter-rouge\">SCRIPT_INPUT_FILE_LIST_1024</code></li>\n  <li><code class=\"language-plaintext highlighter-rouge\">SCRIPT_OUTPUT_FILE_LIST_0</code></li>\n  <li>…</li>\n  <li><code class=\"language-plaintext highlighter-rouge\">SCRIPT_OUTPUT_FILE_LIST_1024</code></li>\n</ul>\n\n<p>There is also a possibility to use input and output files instead of a list (not shown on the screens):</p>\n\n<ul>\n  <li><code class=\"language-plaintext highlighter-rouge\">SCRIPT_INPUT_FILE_0</code></li>\n  <li>…</li>\n  <li><code class=\"language-plaintext highlighter-rouge\">SCRIPT_INPUT_FILE_1024</code></li>\n  <li><code class=\"language-plaintext highlighter-rouge\">SCRIPT_OUTPUT_FILE_0</code></li>\n  <li>…</li>\n  <li><code class=\"language-plaintext highlighter-rouge\">SCRIPT_OUTPUT_FILE_1024</code></li>\n  <li>and additionally the <code class=\"language-plaintext highlighter-rouge\">SCRIPT_INPUT_FILE_COUNT</code> and <code class=\"language-plaintext highlighter-rouge\">SCRIPT_OUTPUT_FILE_COUNT</code> can be used</li>\n</ul>\n\n<p>We based our script copying resource bundles only on file lists and it’s actually quite simple - it just copies files from the\ninput file list to the destination which is the path to the executable.</p>\n\n<div class=\"language-sh highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"nv\">destination</span><span class=\"o\">=</span><span class=\"s2\">\"</span><span class=\"k\">${</span><span class=\"nv\">TARGET_BUILD_DIR</span><span class=\"k\">}</span><span class=\"s2\">/</span><span class=\"k\">${</span><span class=\"nv\">EXECUTABLE_FOLDER_PATH</span><span class=\"k\">}</span><span class=\"s2\">\"</span>\n<span class=\"nb\">grep</span> <span class=\"nt\">-v</span> <span class=\"s1\">'^ *#'</span> &lt; <span class=\"s2\">\"</span><span class=\"k\">${</span><span class=\"nv\">SCRIPT_INPUT_FILE_LIST_0</span><span class=\"k\">}</span><span class=\"s2\">\"</span> | <span class=\"k\">while </span><span class=\"nv\">IFS</span><span class=\"o\">=</span> <span class=\"nb\">read</span> <span class=\"nt\">-r</span> bundle_path\n<span class=\"k\">do\n    if</span> <span class=\"o\">[</span> <span class=\"nt\">-d</span> <span class=\"s2\">\"</span><span class=\"nv\">$bundle_path</span><span class=\"s2\">\"</span> <span class=\"o\">]</span><span class=\"p\">;</span> <span class=\"k\">then\n        </span>rsync <span class=\"nt\">-auv</span> <span class=\"s2\">\"</span><span class=\"k\">${</span><span class=\"nv\">bundle_path</span><span class=\"k\">}</span><span class=\"s2\">\"</span> <span class=\"s2\">\"</span><span class=\"k\">${</span><span class=\"nv\">destination</span><span class=\"k\">}</span><span class=\"s2\">\"</span> <span class=\"o\">||</span> <span class=\"nb\">exit </span>1\n    <span class=\"k\">fi\ndone</span>\n</code></pre></div></div>\n\n<p>In the end we tapped into using Xcode’s dependency analysis for a few run scripts and it allowed us to improve warm build time.</p>\n\n<div class=\"language-sh highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>Build Timing Summary\n\nPhaseScriptExecution <span class=\"o\">(</span>6 tasks<span class=\"o\">)</span> | 3.666 seconds\n\nValidateEmbeddedBinary <span class=\"o\">(</span>2 tasks<span class=\"o\">)</span> | 2.314 seconds\n\n<span class=\"k\">**</span> BUILD SUCCEEDED <span class=\"k\">**</span> <span class=\"o\">[</span>7.500 sec]\n</code></pre></div></div>\n\n<p><img src=\"/img/articles/2020-12-28-speeding-up-warm-builds/warm-build-graph.png\" alt=\"Allegro iOS - graph depicting Warm Build Time change over months\" /></p>\n\n<p>At the time of writing the <strong>warm build time</strong> on our CI machines takes <strong>4 seconds</strong>. The overall goal of speeding up builds is so that\nthe <strong>clean build</strong> time becomes equal to <strong>warm build</strong>.</p>\n\n<h2 id=\"links\">Links</h2>\n\n<p>Some useful links related to improving compilation times for Xcode projects:</p>\n\n<ul>\n  <li><a href=\"https://www.onswiftwings.com/posts/build-time-optimization-part1/\">Xcode Build Time Optimization - Part 1</a></li>\n  <li><a href=\"https://www.onswiftwings.com/posts/build-time-optimization-part2/\">Xcode Build Time Optimization - Part 2</a></li>\n  <li><a href=\"https://eisel.me/signing\">Disabling code signing for Debug builds</a></li>\n</ul>\n","contentSnippet":"Programmers who have ever developed software for Apple platforms in the early days of Swift language might remember ridiculous\ntimes it took to compile the whole project. For large and complicated codebase times used to range from 10 up to 40 minutes.\nOver the years our toolset has improved alongside with compilation times, but slow build times of source code can still be a nightmare.\nWhen we wait a few minutes for a build, we navigate ourselves towards different activities and start e.g. watching funny animal pictures or\nYouTube videos, easily loosing context of the task at hand. What becomes annoying for us is slow feedback of code correctness.\nIn the past issue my colleague has written about a solution\nto slow clean builds.In this post I will focus on warm builds improvement.\nClean and incremental builds\nThere are two terms used in the realm of Xcode when it comes to distinguishing types of\ncompilation: clean and incremental build. The first refers to the time it takes to build a project from scratch. The latter is the time\nit takes to build only whatever changed since the last build and to integrate the changes into a build product.\nYou might also be familiar with the term warm build. It’s used interchangeably with incremental build term, but for the sake of this\npost I will be using it to refer to the time it takes to build a product since the last clean build without introducing any source code\nchanges.\nWhy and what for\nWhy bothering with improving warm build times? Well, for a small projects built on super fast workstations it might take a fraction of a\nsecond to do a warm build, but as projects grow and multiple Build Phases get added to a target so grow the build times. These times\nare noticeable especially when one builds the target without an introduction of changes to the source code.\nBefore we started improving the warm build of the Allegro app for iOS platform it took 18 seconds to perform the build on our\nContinuous Integration (CI) servers (Mac Mini, 6-Core 3.2 GHz CPU, 32 GB Ram).\nIs 18 seconds too much? When you put it into a perspective of 1 year:\n18 seconds × 6 builds per hour × 8 hours per day × 20 days per month × 12 months per year = 207360 seconds = 3456 minutes = 57\nhours 36 minutes\nIt means that on average a programmer spends around 57 hours 36 minutes yearly to wait for a feedback if their code is correct. Is it\nmuch? I leave the answer to you, but it definitely hinders developer’s experience and distracts the developer from their job.\nTo make the developer’s experience better, we, the iOS Mobile Core Team at Allegro, have set the goal to minimize the time developers\nspend between hitting the build button and getting the feedback on their code as quickly as possible.\nHow could the goal be achieved? Well, before I answer that, let’s put some light onto how to actually measure build times.\nMeasurements\nDevelopers building software for Apple platforms use the Xcode application which has a command\nline interface called xcodebuild. The Xcode has an option to output times for build phases from the menu\nProduct > Perform Action > Build With Timing Summary (doesn’t seem to work on Xcode 12.2 at the time of writing this\nblog post). To get build times with xcodebuild for our Allegro app for each build phase of the main target the following command\ncan be used:\n\nxcodebuild -workspace 'Allegro/Allegro.xcworkspace' \\\n-scheme 'Allegro' \\\n-configuration 'Debug' \\\n-sdk 'iphonesimulator' \\\n-arch 'x86_64' \\\n-showBuildTimingSummary \\\nbuild \\\n| sed -n -e '/Build Timing Summary/,$p'\n\n\nIn the case of the Allegro app it outputs the following lines when I do a clean build with Xcode 12.2’s xcodebuild\n(Mac Book Pro 2.2 GHz 6-Core Intel Core i7 CPU, 32 GB RAM):\n\nBuild Timing Summary\n\nCompileC (49 tasks) | 174.459 seconds\n\nCompileSwiftSources (3 tasks) | 31.747 seconds\n\nCompileStoryboard (6 tasks) | 29.057 seconds\n\nPhaseScriptExecution (8 tasks) | 22.320 seconds\n\nDitto (21 tasks) | 22.282 seconds\n\nLd (3 tasks) | 13.432 seconds\n\nCompileAssetCatalog (1 task) | 6.620 seconds\n\nValidateEmbeddedBinary (2 tasks) | 6.528 seconds\n\nCompileXIB (1 task) | 5.000 seconds\n\nCodeSign (3 tasks) | 1.419 seconds\n\nCopyPNGFile (3 tasks) | 1.236 seconds\n\nTouch (4 tasks) | 0.318 seconds\n\nLibtool (1 task) | 0.241 seconds\n\nLinkStoryboards (2 tasks) | 0.108 seconds\n\n\nThere’s a lot of tasks. When it comes to source code compilation process, we can lower down build times by splitting the source code\ninto modular frameworks, using binary caching techniques and adding explicit types for expressions in Swift.\nIn the case of a warm build the only phases listed are:\n\nBuild Timing Summary\n\nPhaseScriptExecution (6 tasks) | 23.350 seconds\n\nValidateEmbeddedBinary (2 tasks) | 2.424 seconds\n\n** BUILD SUCCEEDED ** [27.238 sec]\n\n\nThanks to performing the warm build it can be easily noticed that there’s a room for improvement when it comes to\nPhaseScriptExecution part of the build process. This is actually the part over which we have the control of. Let’s see, what we can\ndo in order to speed up the build time by playing with what and how scripts get executed.\nCleaning up run scripts\nFirst thing we did with for our iOS application target was selecting scripts which can be run only for Release builds. There’s an easy way\nin Xcode to mark them as runnable for such builds only - just select For install builds only checkbox.\n\nWhat jobs are great for running only for Release builds? We selected a few:\nuploading debug symbols to 3rd party monitoring services\nsetting endpoints or enabling Apple Transport Security (ATS) for Debug/Release builds\nselecting proper .plist files for Debug/Release builds\nNot all tasks can be selected as Release - only. Some of them need to be run for Debug and Release builds, but they don’t have to be\nrun for every build. Xcode 12 introduced a neat feature - running the script based on dependency analysis.\n\nSelecting the checkbox isn’t enough to benefit from dependency analysis. Xcode analyses dependencies of a script, i.e. it verifies if the\ninputs of the script have changed since the last run and if the outputs of the script exist. The potential problem occurred for scripts in our\nproject - they didn’t have explicit inputs and outputs defined so we couldn’t tap into the brand new feature of Xcode.\nDefining inputs and outputs for scripts\nOne of the scripts in our project which is time-consuming copies bundles with resources of each module. Our Xcode workspace consists\nof multiple projects. The main project contains the application target which depends on modules built by other projects. The projects\ncontain static frameworks with resources. The resources for each framework are wrapped in .bundle wrapper and are embedded in\nthe framework. All frameworks are linked statically to the application and their bundles are copied by the script to the application\nwrapper (.app).\nThe list with .bundle files to be copied became an input to our script. We also created a list with paths to which bundles are copied.\nXcode uses a .xcfilelist format for such lists, but it’s just a file with newline-separated values. The\ncopy-bundles-input.xcfilelist input to our script looks as such:\n\n$(BUILT_PRODUCTS_DIR)/ModuleX.framework/ModuleX.bundle\n$(BUILT_PRODUCTS_DIR)/ModuleY.framework/ModuleY.bundle\n$(BUILT_PRODUCTS_DIR)/ModuleZ.framework/ModuleZ.bundle\n\n\nand the copy-bundles-output.xcfilelist output:\n\n$(TARGET_BUILD_DIR)/$(EXECUTABLE_FOLDER_PATH)/ModuleX.bundle\n$(TARGET_BUILD_DIR)/$(EXECUTABLE_FOLDER_PATH)/ModuleY.bundle\n$(TARGET_BUILD_DIR)/$(EXECUTABLE_FOLDER_PATH)/ModuleZ.bundle\n\n\nFile lists can be accessed in a script through environment variables. Each script can have many of them and they are indexed from 0:\nSCRIPT_INPUT_FILE_LIST_0\n…\n  \nSCRIPT_INPUT_FILE_LIST_1024\nSCRIPT_OUTPUT_FILE_LIST_0\n…\n  \nSCRIPT_OUTPUT_FILE_LIST_1024\nThere is also a possibility to use input and output files instead of a list (not shown on the screens):\nSCRIPT_INPUT_FILE_0\n…\n  \nSCRIPT_INPUT_FILE_1024\nSCRIPT_OUTPUT_FILE_0\n…\n  \nSCRIPT_OUTPUT_FILE_1024\nand additionally the SCRIPT_INPUT_FILE_COUNT and SCRIPT_OUTPUT_FILE_COUNT can be used\nWe based our script copying resource bundles only on file lists and it’s actually quite simple - it just copies files from the\ninput file list to the destination which is the path to the executable.\n\ndestination=\"${TARGET_BUILD_DIR}/${EXECUTABLE_FOLDER_PATH}\"\ngrep -v '^ *#' < \"${SCRIPT_INPUT_FILE_LIST_0}\" | while IFS= read -r bundle_path\ndo\n    if [ -d \"$bundle_path\" ]; then\n        rsync -auv \"${bundle_path}\" \"${destination}\" || exit 1\n    fi\ndone\n\n\nIn the end we tapped into using Xcode’s dependency analysis for a few run scripts and it allowed us to improve warm build time.\n\nBuild Timing Summary\n\nPhaseScriptExecution (6 tasks) | 3.666 seconds\n\nValidateEmbeddedBinary (2 tasks) | 2.314 seconds\n\n** BUILD SUCCEEDED ** [7.500 sec]\n\n\n\nAt the time of writing the warm build time on our CI machines takes 4 seconds. The overall goal of speeding up builds is so that\nthe clean build time becomes equal to warm build.\nLinks\nSome useful links related to improving compilation times for Xcode projects:\nXcode Build Time Optimization - Part 1\nXcode Build Time Optimization - Part 2\nDisabling code signing for Debug builds","guid":"https://blog.allegro.tech/2020/12/speeding-up-warm-builds.html","categories":["ios","xcode","swift","objectivec"],"isoDate":"2020-12-27T23:00:00.000Z","thumbnail":"images/post-headers/xcode.png"},{"title":"Speeding up iOS builds with Bazel","link":"https://blog.allegro.tech/2020/12/speeding-up-ios-builds-with-bazel.html","pubDate":"Thu, 17 Dec 2020 00:00:00 +0100","authors":{"author":[{"name":["Kamil Pyć"],"photo":["https://blog.allegro.tech/img/authors/kamil.pyc.jpg"],"url":["https://blog.allegro.tech/authors/kamil.pyc"]}]},"content":"<p>When we developed our Allegro iOS app adding new features and with more people contributing to the codebase, we noticed\nthat build times began to grow. In order to have precise metrics, we started to track clean build time as well as the\namount of code we had. Do these two metrics grow at the same pace?</p>\n\n<h3 id=\"slowing-down\">Slowing down</h3>\n\n<p><img src=\"/img/articles/2020-12-17-speeding-up-ios-builds-with-bazel/build_time_chart.png\" alt=\"Build time chart\" /></p>\n\n<p>Our measurements started in May 2019 with combined 300k lines of Objective-C and Swift code that took around\n~177 seconds to compile. One year later we increased code size by 33% but compilation time grew by 50%.\nIt’s worth noting that this time is measured on our CI machine which is more powerful than a laptop machine —\nbuild times are about 50% slower on our work Macbooks. To put it into perspective - on average developers do 8\nclean builds each day and they will now take about 40 minutes of their work. As we have 25 developers contributing\nto the project, this will add up to 16 hours each day and over 300 hours monthly!\nWe had to make some changes in order to not spend most of our time waiting for the app to compile.\nEven if we have split application into smaller projects it all needs to be built into one single application.\nSince it’s a monolith that needs to be linked together, one “service” cannot be changed in a running app as you would\nin microservice backend infrastructure.</p>\n\n<p><img src=\"/img/articles/2020-12-17-speeding-up-ios-builds-with-bazel/build_details.png\" alt=\"Build details\" /></p>\n\n<p>At first we tried to speed things up with building our 3rd party dependencies with Carthage. However, this was not very\nefficient being only a small fraction of our code base. Any improvement was quickly eaten up by adding new code that\nneeded time to compile. The direction of not compiling the same code over and over was what we were aiming for.</p>\n\n<h3 id=\"bazel\">Bazel</h3>\n\n<p>Before we started looking for a solution we outlined what was important for us:</p>\n\n<ul>\n  <li>Ideally it should be transparent to our developers - they should only notice an increase in speed</li>\n  <li>It should work with modules that mix Obj-C and Swift</li>\n  <li>It should be easy to turn off and switch to standard building Xcode if something goes sideways</li>\n</ul>\n\n<p>Basically, this meant we wanted to keep our current setup while adding a mechanism letting us to share compiled\nartifacts(preferably without any special developer integration). Our eyes turned to the open source build\nsystems - <a href=\"https://bazel.build\">Bazel</a> and <a href=\"https://buck.build\">Buck</a>. Comparing these two, we chose Bazel since it\nprovides better support for custom actions\nwith its Starlak language and it’s more popular in the iOS community.</p>\n\n<p>Bazel is Google’s build system that supports C++, Android, iOS, Go and a wide variety of other\nlanguage platforms on Windows, macOS, and Linux. One of its key features is its caching mechanism - both local and\nremote.</p>\n\n<p><img src=\"/img/articles/2020-12-17-speeding-up-ios-builds-with-bazel/bazel_logo.png\" alt=\"Bazel logo\" /></p>\n\n<p>Bazel already provides sets of Apple rules that can build a complete application but it didn’t meet our requirements\nsince mixing Swift and Obj-C is not possible. Another problem is that we would need to do the whole transition at once\nsince you cannot simply migrate only a part of the project. We decided to create a custom rule that would use\nxcodebuild to build frameworks - this means we would use the same build system we currently use in everyday development\nand we wouldn’t have to change our current project.</p>\n\n<p>Custom rules can be written in Starlak language. In our case we needed to wrap\n<code class=\"language-plaintext highlighter-rouge\">xcodebuild</code> into a <code class=\"language-plaintext highlighter-rouge\">sh_binary</code> action:</p>\n\n<div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"n\">sh_binary</span><span class=\"p\">(</span>\n    <span class=\"n\">name</span> <span class=\"o\">=</span> <span class=\"s\">\"xcodebuild\"</span><span class=\"p\">,</span>\n    <span class=\"n\">srcs</span> <span class=\"o\">=</span> <span class=\"p\">[</span><span class=\"s\">\"/usr/bin/xcodebuild\"</span><span class=\"p\">],</span>\n    <span class=\"n\">visibility</span> <span class=\"o\">=</span> <span class=\"p\">[</span><span class=\"s\">\"//visibility:public\"</span><span class=\"p\">]</span>\n<span class=\"p\">)</span>\n</code></pre></div></div>\n\n<p>Then, we can create a rule that will call <code class=\"language-plaintext highlighter-rouge\">xcodebuild</code> and produce <code class=\"language-plaintext highlighter-rouge\">target.framework</code>:</p>\n\n<div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"k\">def</span> <span class=\"nf\">_impl</span><span class=\"p\">(</span><span class=\"n\">ctx</span><span class=\"p\">):</span>\n  <span class=\"n\">name</span> <span class=\"o\">=</span> <span class=\"n\">ctx</span><span class=\"p\">.</span><span class=\"n\">label</span><span class=\"p\">.</span><span class=\"n\">name</span>\n  <span class=\"n\">pbxProj</span> <span class=\"o\">=</span> <span class=\"n\">ctx</span><span class=\"p\">.</span><span class=\"nb\">file</span><span class=\"p\">.</span><span class=\"n\">project</span>\n  <span class=\"n\">output_config</span> <span class=\"o\">=</span> <span class=\"s\">\"CONFIGURATION_BUILD_DIR=../%s\"</span> <span class=\"o\">%</span> <span class=\"n\">ctx</span><span class=\"p\">.</span><span class=\"n\">outputs</span><span class=\"p\">.</span><span class=\"n\">framework</span><span class=\"p\">.</span><span class=\"n\">dirname</span>\n\n  <span class=\"n\">ctx</span><span class=\"p\">.</span><span class=\"n\">actions</span><span class=\"p\">.</span><span class=\"n\">run</span><span class=\"p\">(</span>\n        <span class=\"n\">inputs</span> <span class=\"o\">=</span> <span class=\"p\">[</span><span class=\"n\">pbxProj</span><span class=\"p\">]</span> <span class=\"o\">+</span> <span class=\"n\">ctx</span><span class=\"p\">.</span><span class=\"n\">files</span><span class=\"p\">.</span><span class=\"n\">srcs</span><span class=\"p\">,</span>\n        <span class=\"n\">outputs</span> <span class=\"o\">=</span> <span class=\"p\">[</span><span class=\"n\">ctx</span><span class=\"p\">.</span><span class=\"n\">outputs</span><span class=\"p\">.</span><span class=\"n\">framework</span><span class=\"p\">],</span>\n        <span class=\"n\">arguments</span> <span class=\"o\">=</span> <span class=\"p\">[</span><span class=\"s\">\"build\"</span><span class=\"p\">,</span> <span class=\"s\">\"-project\"</span><span class=\"p\">,</span> <span class=\"n\">pbxProj</span><span class=\"p\">.</span><span class=\"n\">path</span><span class=\"p\">,</span> <span class=\"s\">\"-scheme\"</span><span class=\"p\">,</span> <span class=\"n\">name</span><span class=\"p\">,</span> <span class=\"n\">output_config</span><span class=\"p\">],</span>\n        <span class=\"n\">progress_message</span> <span class=\"o\">=</span> <span class=\"s\">\"Building framework %s\"</span> <span class=\"o\">%</span> <span class=\"n\">name</span><span class=\"p\">,</span>\n        <span class=\"n\">executable</span> <span class=\"o\">=</span> <span class=\"n\">ctx</span><span class=\"p\">.</span><span class=\"n\">executable</span><span class=\"p\">.</span><span class=\"n\">xcodebuild</span><span class=\"p\">,</span>\n    <span class=\"p\">)</span>\n\n<span class=\"n\">framework</span> <span class=\"o\">=</span> <span class=\"n\">rule</span><span class=\"p\">(</span>\n    <span class=\"n\">implementation</span> <span class=\"o\">=</span> <span class=\"n\">_impl</span><span class=\"p\">,</span>\n    <span class=\"n\">attrs</span> <span class=\"o\">=</span> <span class=\"p\">{</span>\n        <span class=\"s\">\"srcs\"</span><span class=\"p\">:</span> <span class=\"n\">attr</span><span class=\"p\">.</span><span class=\"n\">label_list</span><span class=\"p\">(</span><span class=\"n\">allow_files</span> <span class=\"o\">=</span> <span class=\"bp\">True</span><span class=\"p\">),</span>\n        <span class=\"s\">\"project\"</span><span class=\"p\">:</span> <span class=\"n\">attr</span><span class=\"p\">.</span><span class=\"n\">label</span><span class=\"p\">(</span>\n            <span class=\"n\">allow_single_file</span> <span class=\"o\">=</span> <span class=\"bp\">True</span><span class=\"p\">,</span>\n            <span class=\"n\">mandatory</span> <span class=\"o\">=</span> <span class=\"bp\">True</span><span class=\"p\">,</span>\n        <span class=\"p\">),</span>\n        <span class=\"s\">\"xcodebuild\"</span><span class=\"p\">:</span> <span class=\"n\">attr</span><span class=\"p\">.</span><span class=\"n\">label</span><span class=\"p\">(</span>\n            <span class=\"n\">executable</span> <span class=\"o\">=</span> <span class=\"bp\">True</span><span class=\"p\">,</span>\n            <span class=\"n\">cfg</span> <span class=\"o\">=</span> <span class=\"s\">\"host\"</span><span class=\"p\">,</span>\n            <span class=\"n\">allow_files</span> <span class=\"o\">=</span> <span class=\"bp\">True</span><span class=\"p\">,</span>\n            <span class=\"n\">default</span> <span class=\"o\">=</span> <span class=\"n\">Label</span><span class=\"p\">(</span><span class=\"s\">\"//bazel/xcodebuild\"</span><span class=\"p\">)</span>\n        <span class=\"p\">),</span>\n      <span class=\"p\">},</span>\n      <span class=\"n\">outputs</span> <span class=\"o\">=</span> <span class=\"p\">{</span><span class=\"s\">\"framework\"</span><span class=\"p\">:</span> <span class=\"s\">\"%{name}.framework\"</span><span class=\"p\">},</span>\n<span class=\"p\">)</span>\n</code></pre></div></div>\n\n<p>With that we can now build any project we want to, in this case AFNetworking library:</p>\n\n<div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>\n<span class=\"n\">load</span><span class=\"p\">(</span><span class=\"s\">\"//bazel:xcodebuild.bzl\"</span><span class=\"p\">,</span> <span class=\"s\">\"framework\"</span><span class=\"p\">)</span>\n\n<span class=\"n\">framework</span><span class=\"p\">(</span>\n   <span class=\"n\">name</span> <span class=\"o\">=</span> <span class=\"s\">\"AFNetworking\"</span><span class=\"p\">,</span>\n   <span class=\"n\">project</span> <span class=\"o\">=</span> <span class=\"s\">\"Pods/AFNetworking.xcodeproj\"</span><span class=\"p\">,</span>\n   <span class=\"n\">srcs</span> <span class=\"o\">=</span> <span class=\"n\">glob</span><span class=\"p\">([</span><span class=\"s\">\"Pods/AFNetworking/**/*\"</span><span class=\"p\">]),</span>\n<span class=\"p\">)</span>\n\n</code></pre></div></div>\n\n<p>Then we can call:</p>\n\n<div class=\"language-shell highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>./bazel/bazelisk build //:AFNetworking\n</code></pre></div></div>\n\n<p>and this should be given as an output:</p>\n\n<div class=\"language-shell highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"k\">**</span> BUILD SUCCEEDED <span class=\"k\">**</span> <span class=\"o\">[</span>11.279 sec]\n\nTarget //:AFNetworking:\n  bazel-bin/AFNetworking.framework\nINFO: Elapsed <span class=\"nb\">time</span>: 12.427s, Critical Path: 12.28s\nINFO: 1 process: 1 local.\nINFO: Build completed successfully, 2 total actions<span class=\"sb\">`</span>\n</code></pre></div></div>\n\n<p>Thanks to Bazel, build will only be performed once and rebuild only when any of the target files change.\nOnce we point to a remote cache with –remote_http_cache we can share this artefact in a shared remote cache.\nIt’s amazing how easy it is to set up a remote cache.</p>\n\n<p>How can we use Bazel from Xcode, though? Unfortunately, Xcode is not known for great support of external build systems\nand there is no way of doing it ourselves since it’s closed source. The only way of extending it are plugins whose\ncapabilities are very limited. Fortunately, there is a way: we can use Build Phases that are run each time a project is\nbuilt. It’s a simple Run Script phase that invokes Bazel and copies created frameworks to BUILT_PRODUCTS_DIR.\nWhen developers are not working on a given module, we use our special tool that will generate a workspace without it\nand this target will be built with Bazel in this Build Phase. Thanks to shared remote cache, most of the time instead\nof compiling it we would just download precompiled frameworks.</p>\n\n<p>After migrating all of our modules to Bazel we were able to significantly reduce our clean build time. It dropped over\nthreefold, going from 260s to just 85s. Developers’ experience improved as well, because Xcode is a lot more responsive\nthan before because of reducing the number of projects included in the workspace.</p>\n\n<p>It’s worth noting that if any of our scripts or build artefacts contain e.g. local paths they will cause misses in\nour cache. To prevent this we monitor our local and CI builds times and cache hits to detect such situations.</p>\n\n<h3 id=\"tests\">Tests</h3>\n\n<p>A couple years ago we moved all of our iOS projects to a single monorepo.\nThis has drastically simplified development since we don’t have to maintain a pyramid of hundreds of dependencies\nbetween dozens of repositories anymore. One downside is that all projects combined produce over 15.000 unit tests that\ntake over an hour to build and run. We didn’t want to wait that long in each PR, so we decided to run only a selected\nportion of tests affected by introduced changes. To achieve this we had to maintain a list of dependencies between\ndifferent projects and that was obviously very error prone.\nThe chart below shows just a small portion of our dependency tree (generated in Bazel).</p>\n\n<p><img src=\"/img/articles/2020-12-17-speeding-up-ios-builds-with-bazel/dependency_graph.png\" alt=\"Dependency graph\" /></p>\n\n<p>After the migration to Bazel we can query our dependency graph to get a list of targets that a given file affects\nand run unit tests for that target. That improved our experience since we used to manually maintain list of\ndependencies beetwen our module which was error prone and time consuming.</p>\n\n<p><img src=\"/img/articles/2020-12-17-speeding-up-ios-builds-with-bazel/query.png\" alt=\"Sample query\" /></p>\n\n<p>Build results can be cached the same way as build artifacts.\nThis has dramatically reduced test times of our master branch test plan, as we can run <code class=\"language-plaintext highlighter-rouge\">bazel test //...</code> and only test\ntargets that have not been run previously. Take a look at the below chart to see how good our result are:</p>\n\n<p><img src=\"/img/articles/2020-12-17-speeding-up-ios-builds-with-bazel/tests_time_chart.png\" alt=\"Tests time chart\" /></p>\n\n<h3 id=\"conclusion\">Conclusion</h3>\n\n<p>Integrating Bazel into an iOS project requires some effort, but in our opinion it’s worth it, especially in large scale\nprojects. We observe more and more companies struggling with fast and scalable builds. Some of the key tech players,\nincluding Lyft, Pinterest and LinkedIn, switched to Bazel for building iOS apps as well. It’s worth watching Keith\nSmiley &amp; Dave Lee <a href=\"https://www.youtube.com/watch?v=NAPeWoimGx8\">talk</a> from Bazel Conf about migration of Lyft app to\nBazel.</p>\n\n<p>We still have the main app target with a large amount of source code that always needs to be compiled.\nCurrently we are working on splitting the app target into modules, so we can cache this code as well and reduce build\ntime even further. In the future we will try to make the same Bazel migration with our Android application to achieve\nthe same build speed improvement and have single build tool for both platforms. We will also try out try another\npromising feature, called Remote Execution - so we can use remote workers to perform remote builds. We estimate that\nafter completion of these plans, we can further reduce our build times to about 10 seconds.</p>\n","contentSnippet":"When we developed our Allegro iOS app adding new features and with more people contributing to the codebase, we noticed\nthat build times began to grow. In order to have precise metrics, we started to track clean build time as well as the\namount of code we had. Do these two metrics grow at the same pace?\nSlowing down\n\nOur measurements started in May 2019 with combined 300k lines of Objective-C and Swift code that took around\n~177 seconds to compile. One year later we increased code size by 33% but compilation time grew by 50%.\nIt’s worth noting that this time is measured on our CI machine which is more powerful than a laptop machine —\nbuild times are about 50% slower on our work Macbooks. To put it into perspective - on average developers do 8\nclean builds each day and they will now take about 40 minutes of their work. As we have 25 developers contributing\nto the project, this will add up to 16 hours each day and over 300 hours monthly!\nWe had to make some changes in order to not spend most of our time waiting for the app to compile.\nEven if we have split application into smaller projects it all needs to be built into one single application.\nSince it’s a monolith that needs to be linked together, one “service” cannot be changed in a running app as you would\nin microservice backend infrastructure.\n\nAt first we tried to speed things up with building our 3rd party dependencies with Carthage. However, this was not very\nefficient being only a small fraction of our code base. Any improvement was quickly eaten up by adding new code that\nneeded time to compile. The direction of not compiling the same code over and over was what we were aiming for.\nBazel\nBefore we started looking for a solution we outlined what was important for us:\nIdeally it should be transparent to our developers - they should only notice an increase in speed\nIt should work with modules that mix Obj-C and Swift\nIt should be easy to turn off and switch to standard building Xcode if something goes sideways\nBasically, this meant we wanted to keep our current setup while adding a mechanism letting us to share compiled\nartifacts(preferably without any special developer integration). Our eyes turned to the open source build\nsystems - Bazel and Buck. Comparing these two, we chose Bazel since it\nprovides better support for custom actions\nwith its Starlak language and it’s more popular in the iOS community.\nBazel is Google’s build system that supports C++, Android, iOS, Go and a wide variety of other\nlanguage platforms on Windows, macOS, and Linux. One of its key features is its caching mechanism - both local and\nremote.\n\nBazel already provides sets of Apple rules that can build a complete application but it didn’t meet our requirements\nsince mixing Swift and Obj-C is not possible. Another problem is that we would need to do the whole transition at once\nsince you cannot simply migrate only a part of the project. We decided to create a custom rule that would use\nxcodebuild to build frameworks - this means we would use the same build system we currently use in everyday development\nand we wouldn’t have to change our current project.\nCustom rules can be written in Starlak language. In our case we needed to wrap\nxcodebuild into a sh_binary action:\n\nsh_binary(\n    name = \"xcodebuild\",\n    srcs = [\"/usr/bin/xcodebuild\"],\n    visibility = [\"//visibility:public\"]\n)\n\n\nThen, we can create a rule that will call xcodebuild and produce target.framework:\n\ndef _impl(ctx):\n  name = ctx.label.name\n  pbxProj = ctx.file.project\n  output_config = \"CONFIGURATION_BUILD_DIR=../%s\" % ctx.outputs.framework.dirname\n\n  ctx.actions.run(\n        inputs = [pbxProj] + ctx.files.srcs,\n        outputs = [ctx.outputs.framework],\n        arguments = [\"build\", \"-project\", pbxProj.path, \"-scheme\", name, output_config],\n        progress_message = \"Building framework %s\" % name,\n        executable = ctx.executable.xcodebuild,\n    )\n\nframework = rule(\n    implementation = _impl,\n    attrs = {\n        \"srcs\": attr.label_list(allow_files = True),\n        \"project\": attr.label(\n            allow_single_file = True,\n            mandatory = True,\n        ),\n        \"xcodebuild\": attr.label(\n            executable = True,\n            cfg = \"host\",\n            allow_files = True,\n            default = Label(\"//bazel/xcodebuild\")\n        ),\n      },\n      outputs = {\"framework\": \"%{name}.framework\"},\n)\n\n\nWith that we can now build any project we want to, in this case AFNetworking library:\n\n\nload(\"//bazel:xcodebuild.bzl\", \"framework\")\n\nframework(\n   name = \"AFNetworking\",\n   project = \"Pods/AFNetworking.xcodeproj\",\n   srcs = glob([\"Pods/AFNetworking/**/*\"]),\n)\n\n\n\nThen we can call:\n\n./bazel/bazelisk build //:AFNetworking\n\n\nand this should be given as an output:\n\n** BUILD SUCCEEDED ** [11.279 sec]\n\nTarget //:AFNetworking:\n  bazel-bin/AFNetworking.framework\nINFO: Elapsed time: 12.427s, Critical Path: 12.28s\nINFO: 1 process: 1 local.\nINFO: Build completed successfully, 2 total actions`\n\n\nThanks to Bazel, build will only be performed once and rebuild only when any of the target files change.\nOnce we point to a remote cache with –remote_http_cache we can share this artefact in a shared remote cache.\nIt’s amazing how easy it is to set up a remote cache.\nHow can we use Bazel from Xcode, though? Unfortunately, Xcode is not known for great support of external build systems\nand there is no way of doing it ourselves since it’s closed source. The only way of extending it are plugins whose\ncapabilities are very limited. Fortunately, there is a way: we can use Build Phases that are run each time a project is\nbuilt. It’s a simple Run Script phase that invokes Bazel and copies created frameworks to BUILT_PRODUCTS_DIR.\nWhen developers are not working on a given module, we use our special tool that will generate a workspace without it\nand this target will be built with Bazel in this Build Phase. Thanks to shared remote cache, most of the time instead\nof compiling it we would just download precompiled frameworks.\nAfter migrating all of our modules to Bazel we were able to significantly reduce our clean build time. It dropped over\nthreefold, going from 260s to just 85s. Developers’ experience improved as well, because Xcode is a lot more responsive\nthan before because of reducing the number of projects included in the workspace.\nIt’s worth noting that if any of our scripts or build artefacts contain e.g. local paths they will cause misses in\nour cache. To prevent this we monitor our local and CI builds times and cache hits to detect such situations.\nTests\nA couple years ago we moved all of our iOS projects to a single monorepo.\nThis has drastically simplified development since we don’t have to maintain a pyramid of hundreds of dependencies\nbetween dozens of repositories anymore. One downside is that all projects combined produce over 15.000 unit tests that\ntake over an hour to build and run. We didn’t want to wait that long in each PR, so we decided to run only a selected\nportion of tests affected by introduced changes. To achieve this we had to maintain a list of dependencies between\ndifferent projects and that was obviously very error prone.\nThe chart below shows just a small portion of our dependency tree (generated in Bazel).\n\nAfter the migration to Bazel we can query our dependency graph to get a list of targets that a given file affects\nand run unit tests for that target. That improved our experience since we used to manually maintain list of\ndependencies beetwen our module which was error prone and time consuming.\n\nBuild results can be cached the same way as build artifacts.\nThis has dramatically reduced test times of our master branch test plan, as we can run bazel test //... and only test\ntargets that have not been run previously. Take a look at the below chart to see how good our result are:\n\nConclusion\nIntegrating Bazel into an iOS project requires some effort, but in our opinion it’s worth it, especially in large scale\nprojects. We observe more and more companies struggling with fast and scalable builds. Some of the key tech players,\nincluding Lyft, Pinterest and LinkedIn, switched to Bazel for building iOS apps as well. It’s worth watching Keith\nSmiley & Dave Lee talk from Bazel Conf about migration of Lyft app to\nBazel.\nWe still have the main app target with a large amount of source code that always needs to be compiled.\nCurrently we are working on splitting the app target into modules, so we can cache this code as well and reduce build\ntime even further. In the future we will try to make the same Bazel migration with our Android application to achieve\nthe same build speed improvement and have single build tool for both platforms. We will also try out try another\npromising feature, called Remote Execution - so we can use remote workers to perform remote builds. We estimate that\nafter completion of these plans, we can further reduce our build times to about 10 seconds.","guid":"https://blog.allegro.tech/2020/12/speeding-up-ios-builds-with-bazel.html","categories":["ios","bazel"],"isoDate":"2020-12-16T23:00:00.000Z","thumbnail":"images/post-headers/bazel.png"},{"title":"Big data marketing. The story of how technology behind Allegro marketing works.","link":"https://blog.allegro.tech/2020/12/bigdata-marketing.html","pubDate":"Mon, 07 Dec 2020 00:00:00 +0100","authors":{"author":[{"name":["Filip Błaszczyk"],"photo":["https://blog.allegro.tech/img/authors/filip.blaszczyk.jpg"],"url":["https://blog.allegro.tech/authors/filip.blaszczyk"]},{"name":["Piotr Góralczyk"],"photo":["https://blog.allegro.tech/img/authors/piotr.goralczyk.jpg"],"url":["https://blog.allegro.tech/authors/piotr.goralczyk"]},{"name":["Grzegorz Kaczmarczyk"],"photo":["https://blog.allegro.tech/img/authors/grzegorz.kaczmarczyk.jpg"],"url":["https://blog.allegro.tech/authors/grzegorz.kaczmarczyk"]}]},"content":"<p>Marketing is a very important department in every company. In case of Allegro,\nmarketing is especially difficult because you have so many products to promote.\nIn this post we will tell the story of a platform we built for marketing\npurposes.</p>\n\n<h2 id=\"background\">Background</h2>\n\n<p>Allegro is the biggest e-commerce platform in Poland and one of top 10 largest e-commerce platforms worldwide.\nOur catalog holds almost 200 million offers at this moment (December 2020), and the number is still growing.\nThe marketing team uses tools such as\n<a href=\"https://www.google.com/retail/solutions/merchant-center/\">Google Merchant Center</a>\nand <a href=\"https://www.facebook.com/business/ads\">Facebook Ads</a> in order to advertise\nAllegro offers and get more traffic to the platform. To integrate with\nthem we need to prepare an XML file containing information about our offers.\nSuch XML files are called <em>“feed”</em> . We will use this name later on.</p>\n\n<p>Since we need to find specific offers, using a search engine may seem natural.\nHowever, Allegro’s search engine is not suited for this task. Feed could contain\nmillions of offers, what would result in deep pagination issues. A decision\nwas made to simply generate static files using batch jobs run in our <a href=\"https://hadoop.apache.org/\">Hadoop</a>\necosystem. The ability to handle large volumes of data, powerful query\ncapabilities as well as access to various datasets across the platform were\nmajor advantages. <a href=\"https://spark.apache.org/\">Apache Spark</a>, an already tried and tested tool, was an\nobvious choice.</p>\n\n<p>Since we didn’t expect the number of feeds to exceed a few dozen, every\nfeed was calculated in a separate (but executed in parallel) Spark job.\nBusiness users created every feed definition by providing predicates that offers must\nsatisfy to be included in feed, as well as expected XML format and recipient.\nYou can see that architecture in the diagram below. <code class=\"language-plaintext highlighter-rouge\">AggregateGeneratorJob</code> and\n<code class=\"language-plaintext highlighter-rouge\">FeedGeneratorJob</code> were batch jobs written in Apache Spark. First one collected\ndata from different sources on Hive and HDFS, then assembled them into a single\nParquet-based file called simply “aggregate” (we will use this name later on).\nSecond job, <code class=\"language-plaintext highlighter-rouge\">FeedGeneratorJob</code> generated and uploaded a single feed (XML file)\nto S3. All jobs were run in parallel.</p>\n\n<p><img src=\"/img/articles/2020-12-07-bigdata-marketing/old_arch.svg\" alt=\"Old architecture diagram\" /></p>\n\n<p>But soon, against initial assumptions, a number of feeds exploded. Eventually,\nwe encountered as much as… 1300 feeds! Updating all of them, to\npresent current data in advertisements, took more than 24 hours.\nWe managed to improve this situation a little by vertical scaling and\nremoving some of unused/poor performing feeds. However, it was just a temporary\nimprovement, since it still took as much as 13 hours to refresh all the feeds.</p>\n\n<p>We were yet to find out that poor performance was just the tip of the iceberg. Much bigger\nproblem was the architecture that no longer suited our needs and made\nimplementing new features time-consuming. Codebase used a then acclaimed\n<a href=\"https://medium.com/rahasak/scala-cake-pattern-e0cd894dae4e\">cake (anti)pattern</a>\nthat turned out to work poorly in connection with Spark. It caused serious serialization issues.\nAdd to that leaky monitoring and handwritten scheduler, and you will\nget a full picture of our despair. Besides, the tool itself became very\nimportant. It handled more and more integrations and was crucial for the\ncompany.</p>\n\n<h2 id=\"brave-new-world-solution\">Brave new <del>world</del> solution</h2>\n\n<p>At that moment we knew that we needed a new solution.\nWe decided that our target solution should let us:</p>\n\n<ul>\n  <li>Reduce the execution time to 1h (2h at most) while keeping same amount of resources</li>\n  <li>Query over any offer property (in old solution we had only some predefined predicates)</li>\n  <li>Choose offer by a key (or in programmers language: group by + aggregate)</li>\n  <li>Introduce new data sources quickly</li>\n  <li>Create feed with arbitrary size: from just a few offers to whole catalog</li>\n  <li>Scale horizontally</li>\n  <li>Last but not least: integrate with our partners not only by files\nbut also using an event-based approach (streaming API)</li>\n</ul>\n\n<p>These requirements would be easy to comply with in case of a “normal” shop with\na few thousands products. However, Allegro operates on a much larger scale of:</p>\n\n<ul>\n  <li>almost 200M offers (and still growing)</li>\n  <li>~25-40M changes in offers per day</li>\n</ul>\n\n<p>Also, Allegro is based on microservices architecture and we don’t have a single\nDB with full information about the system. This leaves us with yet another\nproblem: how to gather all needed data. We have to use information\non offers, sellers, campaigns, ratings, products and few others.\nSo the first item on our TODO list was to find a solution for collecting\nthe data. In Allegro most of the services use\n<a href=\"http://hermes.allegro.tech/\">Hermes</a> as a message broker. Also, all of the\ndata that is sent by Hermes is dumped to HDFS in near real-time manner. To\nmake this clearer, let me show you that on diagram:</p>\n\n<p><img src=\"/img/articles/2020-12-07-bigdata-marketing/hermes.svg\" alt=\"Event flow in Allegro\" /></p>\n\n<p>At that moment, we wondered which approach would suit our requirements\nbest. We saw three options here:</p>\n\n<ul>\n  <li>Find some existing solution in our platform and customize it for our needs</li>\n  <li>Use Hermes topics directly (online)</li>\n  <li>Collect data from HDFS (offline)</li>\n</ul>\n\n<p>First option would be nice, but there was one problem… we haven’t found any\nsuitable source. So basically, we had to choose between collecting all data\nonline vs offline. Beside the most obvious difference, latency, what else\ndifferentiates these solutions?</p>\n\n<p>It is always more difficult to join data online. We would need to maintain a\ndatabase with the whole state and we would be prone to all kinds of\nconcurrency-related bugs. In case of any detected problem we would have to\nrecover using historical data.</p>\n\n<p>Offline solution would be similar to what we had in the old platform\n(<code class=\"language-plaintext highlighter-rouge\">AggregateGeneratorJob</code> described before). Joins between various data sources\nwould be straightforward. We wouldn’t have any problems with concurrency.\nRecreating data is easy, since basically it is done on every job execution,\nalthough we pay for that with latency. The question though was how long would it take\nto create such aggregate and how much latency we would get at that stage.\nConsidering it was easy to implement we decided to simply measure it. In the end it\nturned out not that bad: in typical cases we were able to maintain\nlatency of about 30 minutes.</p>\n\n<p><img src=\"/img/articles/2020-12-07-bigdata-marketing/aggregate-job.svg\" alt=\"Aggregate job\" /></p>\n\n<p>That was acceptable for a start. In case of it being not enough, we could always transform\nit later into delta architecture and read the newest data (or at least some subset of it,\nfor example products prices) from Hermes to bring it up-to-date.</p>\n\n<p>Once we had a data source, we had to find a way to generate feeds based on\nit. We were a bit biased against Apache Spark, because of poor performance\nand hard maintainability of the old platform. Back then we didn’t know that\nproblem was in our solution, not in Spark itself. That’s why we decided to\nspend a few weeks on research. We made a few prototypes based on Spark\nStreaming, Kafka Streams and on databases. We even had an idea of writing our\nown engine for computation. During that research we came up with the idea of generating\nfeeds in an efficient way and… we realized that it will be\npretty easy to implement in Spark! <strong>We also made an important decision: we\nwill focus on generating files, and get back to streaming API later</strong>.</p>\n\n<h2 id=\"i-am-speed\">I am speed</h2>\n\n<p>Basically, in order to generate feed we need to look through offers catalog and find\nall offers matching defined criteria. In a database you typically use indexes\non a subset of fields to simplify searching. Since we need the possibility of\nmaking predicates on all fields as well as of integrating all offers with our\npartner, we decided to go for linear scanning. Is it bad? Well, it depends on next steps.\nSince we decided on linear scanning, we knew that complexity of our process\nwould be at least O(N). We could handle that, but only as long as we would be\nable to make complexity independent of the number of feeds (integrations). Even more\nimportantly, we had to take care of scalability. It would be best to partition data and\ncalculate it concurrently, while sharing as little common state as possible.</p>\n\n<p>In the old process every feed was calculated by a separate job. Although we had\nhundreds of integrations, lots of them use the same XML schema (later called\n“template”). Also, lots of feeds use similar predicates, so their results can\nbe cached and reused. Therefore, our first priority was to calculate everything in one\ngo. In order to achieve that we simply divide our aggregate into partitions and for each partition we\nevaluate predicates to figure out which feed includes which offer. When we\nknow that, we also know in which templates we need to render an offer. At the\nend of the process we pivot and write data to appropriate partition-files on\nS3. From S3 we serve partitioned files using our file server that knows how to\nassemble parts and serve them as a single piece.</p>\n\n<p>Ok, so how much speedup we gained thanks to that approach? After rewriting we\nwere able to recalculate all feeds in a little over 1h (comparing to 13h previously).\nIt wasn’t all, though. Not only have we sped the process up 13 times, we also\nreduced memory usage twofold! And well, in the end we used the same tools, but in a better way.</p>\n\n<p><img src=\"/img/articles/2020-12-07-bigdata-marketing/engine.svg\" alt=\"How engine works\" /></p>\n\n<h2 id=\"streaming-api\">Streaming API</h2>\n\n<p>After we drank our champagne, and cooled down a little bit, we had to return to\nthe problem with providing streaming API. Following ambitious targets of our\nmarketing business, we wanted to integrate Allegro’s offers in a more effective\nway. This type of integration results in smaller latency of products’ updates\nand also fewer resources are required on the partner’s side.</p>\n\n<p>At that moment we returned to the problem stated before: <strong>what should\nwe use as a source of data?</strong> Catching every event in the moment that it was\nproduced would be very difficult due to the scale and resources required\nto handle such traffic.</p>\n\n<p>Moreover, being a constant listener to all events emitted in our platform and\nsending them instantly to various partners’ APIs brings no benefits in terms\nof data freshness. This is due to the fact that updates are not applied\nimmediately by partners’ sides - even though the latency is lower than in the\nXML solution, it still occurs and can take up to a couple of hours.</p>\n\n<p>We decided that we can start with taking data from offers’ aggregate built for\nfile-based feeds. We didn’t want to send all offers that should be integrated\nwith a partner at every job’s run because in most cases we would generate\nredundant events. Some offers didn’t change between two successive runs at all,\nsome of them were newly added or removed from the feed, but in most\ncases they had only partial updates e.g. price change. So we had the idea of <strong>sending\njust the difference between the previous and current event\nfeed state</strong>. How? Here’s a simplified version of algorithm for this approach:</p>\n\n<ul>\n  <li>load latest state of all Allegro’s offers from HDFS - let’s call it aggregate,</li>\n  <li>extract from aggregate and save on HDFS only the offers that should be\n included in event feed - let’s call it snapshot X,</li>\n  <li>load the previous state of the event feed (from the previous run) - snapshot\n Y,</li>\n  <li>make a full join on X and Y using offer’s unique key - dataset Z of type\n <code class=\"language-plaintext highlighter-rouge\">Tuple(OfferStateX, OfferStateY)</code>,</li>\n  <li>decide to generate appropriate events based on dataset Z:\n    <ul>\n      <li>if both values are non-empty, generate an event with the calculated difference between state X and Y,</li>\n      <li>if the value of X is empty, generate an event on removal from the feed,</li>\n      <li>if the Y value is empty, generate an event on addition of a new offer to the event feed,</li>\n    </ul>\n  </li>\n  <li>send generated events to Kafka topic that is constantly consumed by the\n service (connector) responsible for sending offers to a marketing partner,</li>\n  <li>save snapshot X on HDFS (in the next run it will act as a snapshot Y)</li>\n</ul>\n\n<p><img src=\"/img/articles/2020-12-07-bigdata-marketing/streaming-api.svg\" alt=\"Streaming API architecture\" /></p>\n\n<p>I’m sure you’re wondering how much latency this solution adds.\nWell, it turned out to be only 20 minutes and in our case it is totally acceptable.\nIt is also worth mentioning that our Kafka topic is scalable in case a new partnership appears.\nThis is because the event model contains information about its destinations.\nThanks to this approach, we reduce the amount of data sent, thus limiting the\ntraffic of millions of sent events to just tens of thousands.</p>\n\n<h2 id=\"self-healing-system\">Self-healing system</h2>\n\n<p>Every complex system is prone to inconsistencies. Especially when this\ncomplexity increases and it is hard to stop that - as it was before our big\nrefactor. New architecture let us create a self-healing and fully controllable\nsystem that is convenient to maintain even taking into account the scale we\nface everyday. When designing the architecture we focused mainly on two things:\navailability and control.</p>\n\n<h3 id=\"availability\">Availability</h3>\n\n<p>The first step that should be considered is: <strong>how to properly\ndefine the responsibilities of individual system components?</strong></p>\n\n<p>System works as a sum of cooperating elements. But it is easier to maintain\nthese elements when they have very specific tasks to handle. There are three\nmain components in our system (introduced earlier):</p>\n\n<ol>\n  <li>Aggregator - knows everything about every offer’s current state. It gathers\nall needed data and saves them to HDFS,</li>\n  <li>Generator - it takes data generated by Aggregate, filters it and prepares\nfor delivery to a partner,</li>\n  <li>Connector (possibility of having many connectors) - holds integration with\npartner, acts as a data mapper and sender.</li>\n</ol>\n\n<p><em>Aggregator</em> and <em>Generator</em> are <strong>based on a complementary set of information\nabout the whole system and offers’ state at a certain point in time</strong>.\nSo in case the aggregate contains damaged offers and the generator already\ntook it to prepare for sending, in the next cycle it will get overwritten by fixed ones.\nThis happens because each cycle run’s results overwrite the previous ones.\nAdditionally, both Aggregate and Generator stage results are persisted on HDFS.\nThanks to this we can run the whole computation for any period of time and go back to\nany system state. Also, the Generator stage can be based on data generated at\nany time. In case Aggregate is failing while generating new data, Generator\nworks properly using earlier data.</p>\n\n<p>Then, we have a Connector. It consumes events from Kafka and pushes them,\nin appropriate form, on partner’s API. It has no responsibility for checking\ndata or state correctness. It simply gets what the Generator prepared and\ntries to deliver it to a partner. Thanks to this separation of responsibility,\nConnector is not dependent on Generator - even if Generator has a breakdown,\nthe Connector at worst may have nothing to do.</p>\n\n<h3 id=\"control\">Control</h3>\n\n<p>In the previous paragraph we mentioned a few processing issues we are\nstruggling with. However, we also proved that despite this our system can still\nwork in such conditions - maybe not as effectively as in standard scenarios,\nbut it still does. To react faster, we’ve managed to make quite “garbage\ndata”-resistant, notifications-based alerting system that will alarm about\nanomalies occuring during computation. In short, if the difference\nbetween states of previous and current Generator run is significant (experience\nbased numbers), the system will stop and inform us about it so that we can\ndecide if this change is acceptable or not. (By difference between states I\nmean difference between parameters such as feed’s offer count, number of\noffers’ parameters changes etc.) Once the change is approved, the system returns\nto its work. Otherwise, data is not propagated from Generator to Kafka, resulting\nin lack of data to be consumed by Connector. Even if we pass some incorrect\ndata to a partner and it will be too late to retreat, we have a\nspecial mechanism refreshing any offer that was updated more than 28 days\nago. So if an offer wasn’t updated for such a long time, it doesn’t matter if\nit is damaged or not – it will be refreshed eventually.</p>\n\n<h2 id=\"summary\">Summary</h2>\n\n<p>Key takeaway points:</p>\n\n<ul>\n  <li>Just because something does not work well it doesn’t mean the tool is bad.\nMaybe there is something wrong with the way you are using it?</li>\n  <li>Ideas can be complex but it doesn’t mean that they have to be complicated!</li>\n  <li>Research is key. Even if your business tells you there is no time for it, insist on it.\nOtherwise you will end up spending even more time on fixes.</li>\n  <li>Apache Spark is a beast. It can simplify your computation dramatically and\ngive amazing results with it, but at the same time you need to\nthink more about how your data will be calculated. One small problem may result\nin slow computation. Unfortunately lots of them are hard to notice.</li>\n  <li><a href=\"https://allegro.pl/praca\">Join us</a> if you like such challenges :-)</li>\n</ul>\n","contentSnippet":"Marketing is a very important department in every company. In case of Allegro,\nmarketing is especially difficult because you have so many products to promote.\nIn this post we will tell the story of a platform we built for marketing\npurposes.\nBackground\nAllegro is the biggest e-commerce platform in Poland and one of top 10 largest e-commerce platforms worldwide.\nOur catalog holds almost 200 million offers at this moment (December 2020), and the number is still growing.\nThe marketing team uses tools such as\nGoogle Merchant Center\nand Facebook Ads in order to advertise\nAllegro offers and get more traffic to the platform. To integrate with\nthem we need to prepare an XML file containing information about our offers.\nSuch XML files are called “feed” . We will use this name later on.\nSince we need to find specific offers, using a search engine may seem natural.\nHowever, Allegro’s search engine is not suited for this task. Feed could contain\nmillions of offers, what would result in deep pagination issues. A decision\nwas made to simply generate static files using batch jobs run in our Hadoop\necosystem. The ability to handle large volumes of data, powerful query\ncapabilities as well as access to various datasets across the platform were\nmajor advantages. Apache Spark, an already tried and tested tool, was an\nobvious choice.\nSince we didn’t expect the number of feeds to exceed a few dozen, every\nfeed was calculated in a separate (but executed in parallel) Spark job.\nBusiness users created every feed definition by providing predicates that offers must\nsatisfy to be included in feed, as well as expected XML format and recipient.\nYou can see that architecture in the diagram below. AggregateGeneratorJob and\nFeedGeneratorJob were batch jobs written in Apache Spark. First one collected\ndata from different sources on Hive and HDFS, then assembled them into a single\nParquet-based file called simply “aggregate” (we will use this name later on).\nSecond job, FeedGeneratorJob generated and uploaded a single feed (XML file)\nto S3. All jobs were run in parallel.\n\nBut soon, against initial assumptions, a number of feeds exploded. Eventually,\nwe encountered as much as… 1300 feeds! Updating all of them, to\npresent current data in advertisements, took more than 24 hours.\nWe managed to improve this situation a little by vertical scaling and\nremoving some of unused/poor performing feeds. However, it was just a temporary\nimprovement, since it still took as much as 13 hours to refresh all the feeds.\nWe were yet to find out that poor performance was just the tip of the iceberg. Much bigger\nproblem was the architecture that no longer suited our needs and made\nimplementing new features time-consuming. Codebase used a then acclaimed\ncake (anti)pattern\nthat turned out to work poorly in connection with Spark. It caused serious serialization issues.\nAdd to that leaky monitoring and handwritten scheduler, and you will\nget a full picture of our despair. Besides, the tool itself became very\nimportant. It handled more and more integrations and was crucial for the\ncompany.\nBrave new world solution\nAt that moment we knew that we needed a new solution.\nWe decided that our target solution should let us:\nReduce the execution time to 1h (2h at most) while keeping same amount of resources\nQuery over any offer property (in old solution we had only some predefined predicates)\nChoose offer by a key (or in programmers language: group by + aggregate)\nIntroduce new data sources quickly\nCreate feed with arbitrary size: from just a few offers to whole catalog\nScale horizontally\nLast but not least: integrate with our partners not only by files\nbut also using an event-based approach (streaming API)\nThese requirements would be easy to comply with in case of a “normal” shop with\na few thousands products. However, Allegro operates on a much larger scale of:\nalmost 200M offers (and still growing)\n~25-40M changes in offers per day\nAlso, Allegro is based on microservices architecture and we don’t have a single\nDB with full information about the system. This leaves us with yet another\nproblem: how to gather all needed data. We have to use information\non offers, sellers, campaigns, ratings, products and few others.\nSo the first item on our TODO list was to find a solution for collecting\nthe data. In Allegro most of the services use\nHermes as a message broker. Also, all of the\ndata that is sent by Hermes is dumped to HDFS in near real-time manner. To\nmake this clearer, let me show you that on diagram:\n\nAt that moment, we wondered which approach would suit our requirements\nbest. We saw three options here:\nFind some existing solution in our platform and customize it for our needs\nUse Hermes topics directly (online)\nCollect data from HDFS (offline)\nFirst option would be nice, but there was one problem… we haven’t found any\nsuitable source. So basically, we had to choose between collecting all data\nonline vs offline. Beside the most obvious difference, latency, what else\ndifferentiates these solutions?\nIt is always more difficult to join data online. We would need to maintain a\ndatabase with the whole state and we would be prone to all kinds of\nconcurrency-related bugs. In case of any detected problem we would have to\nrecover using historical data.\nOffline solution would be similar to what we had in the old platform\n(AggregateGeneratorJob described before). Joins between various data sources\nwould be straightforward. We wouldn’t have any problems with concurrency.\nRecreating data is easy, since basically it is done on every job execution,\nalthough we pay for that with latency. The question though was how long would it take\nto create such aggregate and how much latency we would get at that stage.\nConsidering it was easy to implement we decided to simply measure it. In the end it\nturned out not that bad: in typical cases we were able to maintain\nlatency of about 30 minutes.\n\nThat was acceptable for a start. In case of it being not enough, we could always transform\nit later into delta architecture and read the newest data (or at least some subset of it,\nfor example products prices) from Hermes to bring it up-to-date.\nOnce we had a data source, we had to find a way to generate feeds based on\nit. We were a bit biased against Apache Spark, because of poor performance\nand hard maintainability of the old platform. Back then we didn’t know that\nproblem was in our solution, not in Spark itself. That’s why we decided to\nspend a few weeks on research. We made a few prototypes based on Spark\nStreaming, Kafka Streams and on databases. We even had an idea of writing our\nown engine for computation. During that research we came up with the idea of generating\nfeeds in an efficient way and… we realized that it will be\npretty easy to implement in Spark! We also made an important decision: we\nwill focus on generating files, and get back to streaming API later.\nI am speed\nBasically, in order to generate feed we need to look through offers catalog and find\nall offers matching defined criteria. In a database you typically use indexes\non a subset of fields to simplify searching. Since we need the possibility of\nmaking predicates on all fields as well as of integrating all offers with our\npartner, we decided to go for linear scanning. Is it bad? Well, it depends on next steps.\nSince we decided on linear scanning, we knew that complexity of our process\nwould be at least O(N). We could handle that, but only as long as we would be\nable to make complexity independent of the number of feeds (integrations). Even more\nimportantly, we had to take care of scalability. It would be best to partition data and\ncalculate it concurrently, while sharing as little common state as possible.\nIn the old process every feed was calculated by a separate job. Although we had\nhundreds of integrations, lots of them use the same XML schema (later called\n“template”). Also, lots of feeds use similar predicates, so their results can\nbe cached and reused. Therefore, our first priority was to calculate everything in one\ngo. In order to achieve that we simply divide our aggregate into partitions and for each partition we\nevaluate predicates to figure out which feed includes which offer. When we\nknow that, we also know in which templates we need to render an offer. At the\nend of the process we pivot and write data to appropriate partition-files on\nS3. From S3 we serve partitioned files using our file server that knows how to\nassemble parts and serve them as a single piece.\nOk, so how much speedup we gained thanks to that approach? After rewriting we\nwere able to recalculate all feeds in a little over 1h (comparing to 13h previously).\nIt wasn’t all, though. Not only have we sped the process up 13 times, we also\nreduced memory usage twofold! And well, in the end we used the same tools, but in a better way.\n\nStreaming API\nAfter we drank our champagne, and cooled down a little bit, we had to return to\nthe problem with providing streaming API. Following ambitious targets of our\nmarketing business, we wanted to integrate Allegro’s offers in a more effective\nway. This type of integration results in smaller latency of products’ updates\nand also fewer resources are required on the partner’s side.\nAt that moment we returned to the problem stated before: what should\nwe use as a source of data? Catching every event in the moment that it was\nproduced would be very difficult due to the scale and resources required\nto handle such traffic.\nMoreover, being a constant listener to all events emitted in our platform and\nsending them instantly to various partners’ APIs brings no benefits in terms\nof data freshness. This is due to the fact that updates are not applied\nimmediately by partners’ sides - even though the latency is lower than in the\nXML solution, it still occurs and can take up to a couple of hours.\nWe decided that we can start with taking data from offers’ aggregate built for\nfile-based feeds. We didn’t want to send all offers that should be integrated\nwith a partner at every job’s run because in most cases we would generate\nredundant events. Some offers didn’t change between two successive runs at all,\nsome of them were newly added or removed from the feed, but in most\ncases they had only partial updates e.g. price change. So we had the idea of sending\njust the difference between the previous and current event\nfeed state. How? Here’s a simplified version of algorithm for this approach:\nload latest state of all Allegro’s offers from HDFS - let’s call it aggregate,\nextract from aggregate and save on HDFS only the offers that should be\n included in event feed - let’s call it snapshot X,\nload the previous state of the event feed (from the previous run) - snapshot\n Y,\nmake a full join on X and Y using offer’s unique key - dataset Z of type\n Tuple(OfferStateX, OfferStateY),\ndecide to generate appropriate events based on dataset Z:\n    \nif both values are non-empty, generate an event with the calculated difference between state X and Y,\nif the value of X is empty, generate an event on removal from the feed,\nif the Y value is empty, generate an event on addition of a new offer to the event feed,\nsend generated events to Kafka topic that is constantly consumed by the\n service (connector) responsible for sending offers to a marketing partner,\nsave snapshot X on HDFS (in the next run it will act as a snapshot Y)\n\nI’m sure you’re wondering how much latency this solution adds.\nWell, it turned out to be only 20 minutes and in our case it is totally acceptable.\nIt is also worth mentioning that our Kafka topic is scalable in case a new partnership appears.\nThis is because the event model contains information about its destinations.\nThanks to this approach, we reduce the amount of data sent, thus limiting the\ntraffic of millions of sent events to just tens of thousands.\nSelf-healing system\nEvery complex system is prone to inconsistencies. Especially when this\ncomplexity increases and it is hard to stop that - as it was before our big\nrefactor. New architecture let us create a self-healing and fully controllable\nsystem that is convenient to maintain even taking into account the scale we\nface everyday. When designing the architecture we focused mainly on two things:\navailability and control.\nAvailability\nThe first step that should be considered is: how to properly\ndefine the responsibilities of individual system components?\nSystem works as a sum of cooperating elements. But it is easier to maintain\nthese elements when they have very specific tasks to handle. There are three\nmain components in our system (introduced earlier):\nAggregator - knows everything about every offer’s current state. It gathers\nall needed data and saves them to HDFS,\nGenerator - it takes data generated by Aggregate, filters it and prepares\nfor delivery to a partner,\nConnector (possibility of having many connectors) - holds integration with\npartner, acts as a data mapper and sender.\nAggregator and Generator are based on a complementary set of information\nabout the whole system and offers’ state at a certain point in time.\nSo in case the aggregate contains damaged offers and the generator already\ntook it to prepare for sending, in the next cycle it will get overwritten by fixed ones.\nThis happens because each cycle run’s results overwrite the previous ones.\nAdditionally, both Aggregate and Generator stage results are persisted on HDFS.\nThanks to this we can run the whole computation for any period of time and go back to\nany system state. Also, the Generator stage can be based on data generated at\nany time. In case Aggregate is failing while generating new data, Generator\nworks properly using earlier data.\nThen, we have a Connector. It consumes events from Kafka and pushes them,\nin appropriate form, on partner’s API. It has no responsibility for checking\ndata or state correctness. It simply gets what the Generator prepared and\ntries to deliver it to a partner. Thanks to this separation of responsibility,\nConnector is not dependent on Generator - even if Generator has a breakdown,\nthe Connector at worst may have nothing to do.\nControl\nIn the previous paragraph we mentioned a few processing issues we are\nstruggling with. However, we also proved that despite this our system can still\nwork in such conditions - maybe not as effectively as in standard scenarios,\nbut it still does. To react faster, we’ve managed to make quite “garbage\ndata”-resistant, notifications-based alerting system that will alarm about\nanomalies occuring during computation. In short, if the difference\nbetween states of previous and current Generator run is significant (experience\nbased numbers), the system will stop and inform us about it so that we can\ndecide if this change is acceptable or not. (By difference between states I\nmean difference between parameters such as feed’s offer count, number of\noffers’ parameters changes etc.) Once the change is approved, the system returns\nto its work. Otherwise, data is not propagated from Generator to Kafka, resulting\nin lack of data to be consumed by Connector. Even if we pass some incorrect\ndata to a partner and it will be too late to retreat, we have a\nspecial mechanism refreshing any offer that was updated more than 28 days\nago. So if an offer wasn’t updated for such a long time, it doesn’t matter if\nit is damaged or not – it will be refreshed eventually.\nSummary\nKey takeaway points:\nJust because something does not work well it doesn’t mean the tool is bad.\nMaybe there is something wrong with the way you are using it?\nIdeas can be complex but it doesn’t mean that they have to be complicated!\nResearch is key. Even if your business tells you there is no time for it, insist on it.\nOtherwise you will end up spending even more time on fixes.\nApache Spark is a beast. It can simplify your computation dramatically and\ngive amazing results with it, but at the same time you need to\nthink more about how your data will be calculated. One small problem may result\nin slow computation. Unfortunately lots of them are hard to notice.\nJoin us if you like such challenges :-)","guid":"https://blog.allegro.tech/2020/12/bigdata-marketing.html","categories":["architecture","bigdata","spark"],"isoDate":"2020-12-06T23:00:00.000Z","thumbnail":"images/post-headers/spark.png"}],"jobs":[{"id":"743999732620819","name":"Senior Software Engineer","uuid":"54116ab3-33fa-44db-b6f0-46bf7b03fbaa","refNumber":"REF1898H","company":{"identifier":"Allegro","name":"Allegro"},"releasedDate":"2021-01-26T13:29:05.000Z","location":{"city":"Kraków","region":"Lesser Poland Voivodeship","country":"pl","remote":false},"industry":{"id":"internet","label":"Internet"},"department":{"id":"1013462","label":"Technology - Product Development"},"function":{"id":"information_technology","label":"Information Technology"},"typeOfEmployment":{"label":"Full-time"},"experienceLevel":{"id":"mid_senior_level","label":"Mid-Senior Level"},"customField":[{"fieldId":"58c15608e4b01d4b19ddf790","fieldLabel":"Proces rekrutacji","valueId":"c807eec2-8a53-4b55-b7c5-c03180f2059b","valueLabel":"IT Allegro"},{"fieldId":"COUNTRY","fieldLabel":"Country","valueId":"pl","valueLabel":"Poland"},{"fieldId":"58c13159e4b01d4b19ddf729","fieldLabel":"Department","valueId":"1013462","valueLabel":"Technology - Product Development"},{"fieldId":"58c158e9e4b0614667d5973e","fieldLabel":"Więcej lokalizacji","valueId":"fa9e2c82-b44f-40df-a699-4dea59cb0c13","valueLabel":"Nie"},{"fieldId":"58c13159e4b01d4b19ddf728","fieldLabel":"Brands","valueId":"4ccb4fab-6c3f-4ed0-9140-8533fe17447f","valueLabel":"Allegro"}],"ref":"https://api.smartrecruiters.com/v1/companies/allegro/postings/743999732620819","creator":{"name":"Jagoda Rusiniak"},"language":{"code":"pl","label":"Polish","labelNative":"polski"}},{"id":"743999732620056","name":"Senior Software Engineer","uuid":"8db5abff-5e5b-4feb-a06b-c164dd337c1f","refNumber":"REF1903G","company":{"identifier":"Allegro","name":"Allegro"},"releasedDate":"2021-01-26T13:21:46.000Z","location":{"city":"Warszawa","country":"pl","remote":false},"industry":{"id":"internet","label":"Internet"},"department":{"id":"1013462","label":"Technology - Product Development"},"function":{"id":"information_technology","label":"Information Technology"},"typeOfEmployment":{"label":"Full-time"},"experienceLevel":{"id":"mid_senior_level","label":"Mid-Senior Level"},"customField":[{"fieldId":"58c15608e4b01d4b19ddf790","fieldLabel":"Proces rekrutacji","valueId":"c807eec2-8a53-4b55-b7c5-c03180f2059b","valueLabel":"IT Allegro"},{"fieldId":"COUNTRY","fieldLabel":"Country","valueId":"pl","valueLabel":"Poland"},{"fieldId":"58c13159e4b01d4b19ddf729","fieldLabel":"Department","valueId":"1013462","valueLabel":"Technology - Product Development"},{"fieldId":"58c13159e4b01d4b19ddf728","fieldLabel":"Brands","valueId":"4ccb4fab-6c3f-4ed0-9140-8533fe17447f","valueLabel":"Allegro"},{"fieldId":"5cdab2c84cedfd0006e7758c","fieldLabel":"Key words","valueLabel":"java, scala, kotlin, developer, programista, inżynier, architekt, architecture, architect"}],"ref":"https://api.smartrecruiters.com/v1/companies/allegro/postings/743999732620056","creator":{"name":"Jagoda Rusiniak"},"language":{"code":"pl","label":"Polish","labelNative":"polski"}},{"id":"743999732606435","name":"Software Engineer (Big Data Team)","uuid":"ca7c0429-f844-4933-acac-1a6910b14da9","refNumber":"REF2348I","company":{"identifier":"Allegro","name":"Allegro"},"releasedDate":"2021-01-26T10:07:45.000Z","location":{"city":"Warszawa, Kraków, Poznań, Toruń","region":"Masovian Voivodeship","country":"pl","remote":false},"industry":{"id":"internet","label":"Internet"},"department":{"id":"1013462","label":"Technology - Product Development"},"function":{"id":"information_technology","label":"Information Technology"},"typeOfEmployment":{"label":"Full-time"},"experienceLevel":{"id":"mid_senior_level","label":"Mid-Senior Level"},"customField":[{"fieldId":"58c1575ee4b01d4b19ddf797","fieldLabel":"Kraków","valueId":"2cfcfcc1-da44-43f7-8eaa-3907faf2797c","valueLabel":"Tak"},{"fieldId":"58c158e9e4b0614667d5973e","fieldLabel":"Więcej lokalizacji","valueId":"3a186e8d-a82c-4955-af81-fcef9a63928a","valueLabel":"Tak"},{"fieldId":"58c1576ee4b0614667d59732","fieldLabel":"Toruń","valueId":"a6765624-e047-4a26-9481-9621086d8b96","valueLabel":"Nie"},{"fieldId":"58c156e6e4b01d4b19ddf793","fieldLabel":"Warszawa","valueId":"6a428533-1586-4372-89ec-65fb45366363","valueLabel":"Tak"},{"fieldId":"58c15608e4b01d4b19ddf790","fieldLabel":"Proces rekrutacji","valueId":"c807eec2-8a53-4b55-b7c5-c03180f2059b","valueLabel":"IT Allegro"},{"fieldId":"COUNTRY","fieldLabel":"Country","valueId":"pl","valueLabel":"Poland"},{"fieldId":"58c15788e4b0614667d59733","fieldLabel":"Błonie","valueId":"25f6cb8c-81b3-434a-93ec-6dc851d5808d","valueLabel":"Nie"},{"fieldId":"58c15798e4b01d4b19ddf79b","fieldLabel":"Wrocław","valueId":"31873284-1e97-427d-8918-6ce504344351","valueLabel":"Nie"},{"fieldId":"58c13159e4b01d4b19ddf729","fieldLabel":"Department","valueId":"1013462","valueLabel":"Technology - Product Development"},{"fieldId":"58c13159e4b01d4b19ddf728","fieldLabel":"Brands","valueId":"4ccb4fab-6c3f-4ed0-9140-8533fe17447f","valueLabel":"Allegro"},{"fieldId":"58c156b9e4b01d4b19ddf792","fieldLabel":"Poznań","valueId":"92fb81b1-a3eb-4b30-8571-0dfdfcb911d9","valueLabel":"Nie"},{"fieldId":"5cdab2c84cedfd0006e7758c","fieldLabel":"Key words","valueLabel":"Software Engineer, Developer, Programista, Java, Scala, Kotlin, Big Data, Hadoop, Spark"}],"ref":"https://api.smartrecruiters.com/v1/companies/allegro/postings/743999732606435","creator":{"name":"Angelika Szymkiewicz"},"language":{"code":"pl","label":"Polish","labelNative":"polski"}},{"id":"743999732498611","name":"Consumer Strategy Team Leader","uuid":"9140e17b-f9dc-49ba-a951-d6f11fb212ec","refNumber":"REF2515K","company":{"identifier":"Allegro","name":"Allegro"},"releasedDate":"2021-01-25T15:09:15.000Z","location":{"city":"Warszawa, Poznań","region":"Masovian Voivodeship","country":"pl","remote":false},"industry":{"id":"internet","label":"Internet"},"department":{"id":"1013462","label":"Technology - Product Development"},"function":{"id":"analyst","label":"Analyst"},"typeOfEmployment":{"label":"Full-time"},"experienceLevel":{"id":"mid_senior_level","label":"Mid-Senior Level"},"customField":[{"fieldId":"58c1575ee4b01d4b19ddf797","fieldLabel":"Kraków","valueId":"bcab9d4f-8624-4d85-8e3d-dbf12239b972","valueLabel":"Nie"},{"fieldId":"58c158e9e4b0614667d5973e","fieldLabel":"Więcej lokalizacji","valueId":"3a186e8d-a82c-4955-af81-fcef9a63928a","valueLabel":"Tak"},{"fieldId":"58c1576ee4b0614667d59732","fieldLabel":"Toruń","valueId":"a6765624-e047-4a26-9481-9621086d8b96","valueLabel":"Nie"},{"fieldId":"58c156e6e4b01d4b19ddf793","fieldLabel":"Warszawa","valueId":"6a428533-1586-4372-89ec-65fb45366363","valueLabel":"Tak"},{"fieldId":"58c15608e4b01d4b19ddf790","fieldLabel":"Proces rekrutacji","valueId":"c807eec2-8a53-4b55-b7c5-c03180f2059b","valueLabel":"IT Allegro"},{"fieldId":"COUNTRY","fieldLabel":"Country","valueId":"pl","valueLabel":"Poland"},{"fieldId":"58c15788e4b0614667d59733","fieldLabel":"Błonie","valueId":"25f6cb8c-81b3-434a-93ec-6dc851d5808d","valueLabel":"Nie"},{"fieldId":"58c15798e4b01d4b19ddf79b","fieldLabel":"Wrocław","valueId":"31873284-1e97-427d-8918-6ce504344351","valueLabel":"Nie"},{"fieldId":"58c13159e4b01d4b19ddf729","fieldLabel":"Department","valueId":"1013462","valueLabel":"Technology - Product Development"},{"fieldId":"58c13159e4b01d4b19ddf728","fieldLabel":"Brands","valueId":"4ccb4fab-6c3f-4ed0-9140-8533fe17447f","valueLabel":"Allegro"},{"fieldId":"5dee624907e370138f7ad0bd","fieldLabel":"Kompetencje Allegro","valueId":"236b702e-45b9-4c4b-82d4-01c640aaf881","valueLabel":"Nie"},{"fieldId":"58c156b9e4b01d4b19ddf792","fieldLabel":"Poznań","valueId":"ac20917b-cb6a-4280-aff3-4fae2532a33e","valueLabel":"Tak"},{"fieldId":"5cdab2c84cedfd0006e7758c","fieldLabel":"Key words","valueLabel":"lider, leader, analityk, analyst, business"}],"ref":"https://api.smartrecruiters.com/v1/companies/allegro/postings/743999732498611","creator":{"name":"Angelika Szymkiewicz"},"language":{"code":"pl","label":"Polish","labelNative":"polski"}},{"id":"743999732293504","name":"Big Data Engineer","uuid":"ef47f1e0-2de5-4727-8e1e-9ea14809062f","refNumber":"REF1795E","company":{"identifier":"Allegro","name":"Allegro"},"releasedDate":"2021-01-22T15:30:09.000Z","location":{"city":"Poznań, Warszawa, Kraków, Toruń","country":"pl","remote":false},"industry":{"id":"internet","label":"Internet"},"department":{"id":"1013462","label":"Technology - Product Development"},"function":{"id":"information_technology","label":"Information Technology"},"typeOfEmployment":{"label":"Full-time"},"experienceLevel":{"id":"mid_senior_level","label":"Mid-Senior Level"},"customField":[{"fieldId":"58c1575ee4b01d4b19ddf797","fieldLabel":"Kraków","valueId":"2cfcfcc1-da44-43f7-8eaa-3907faf2797c","valueLabel":"Tak"},{"fieldId":"58c15608e4b01d4b19ddf790","fieldLabel":"Proces rekrutacji","valueId":"c807eec2-8a53-4b55-b7c5-c03180f2059b","valueLabel":"IT Allegro"},{"fieldId":"COUNTRY","fieldLabel":"Country","valueId":"pl","valueLabel":"Poland"},{"fieldId":"58c158e9e4b0614667d5973e","fieldLabel":"Więcej lokalizacji","valueId":"3a186e8d-a82c-4955-af81-fcef9a63928a","valueLabel":"Tak"},{"fieldId":"58c13159e4b01d4b19ddf729","fieldLabel":"Department","valueId":"1013462","valueLabel":"Technology - Product Development"},{"fieldId":"58c13159e4b01d4b19ddf728","fieldLabel":"Brands","valueId":"4ccb4fab-6c3f-4ed0-9140-8533fe17447f","valueLabel":"Allegro"},{"fieldId":"58c156b9e4b01d4b19ddf792","fieldLabel":"Poznań","valueId":"ac20917b-cb6a-4280-aff3-4fae2532a33e","valueLabel":"Tak"},{"fieldId":"58c156e6e4b01d4b19ddf793","fieldLabel":"Warszawa","valueId":"6a428533-1586-4372-89ec-65fb45366363","valueLabel":"Tak"},{"fieldId":"5cdab2c84cedfd0006e7758c","fieldLabel":"Key words","valueLabel":"Java, Scala, Spark, Druid, Airflow, Big Data, bigdata, Google Cloud Platform, Dataflow, BigQuery, unix, linux, developer, programista, programmer, dev, inżynier, engineer, Spark, Hadoop"}],"ref":"https://api.smartrecruiters.com/v1/companies/allegro/postings/743999732293504","creator":{"name":"Adriana Gesiarz"},"language":{"code":"pl","label":"Polish","labelNative":"polski"}}],"events":[{"created":1596114124000,"duration":5400000,"id":"272249143","name":"Allegro Tech Talks #13 - Cyfrodziewczyny","date_in_series_pattern":false,"status":"past","time":1596643200000,"local_date":"2020-08-05","local_time":"18:00","updated":1596650397000,"utc_offset":7200000,"waitlist_count":0,"yes_rsvp_count":40,"venue":{"id":26906060,"name":"Online event","repinned":false,"country":"","localized_country_name":""},"is_online_event":true,"group":{"created":1425052059000,"name":"allegro Tech","id":18465254,"join_mode":"open","lat":52.2599983215332,"lon":21.020000457763672,"urlname":"allegrotech","who":"Techs","localized_location":"Warsaw, Poland","state":"","country":"pl","region":"en_US","timezone":"Europe/Warsaw"},"link":"https://www.meetup.com/allegrotech/events/272249143/","description":"Allegro Tech Live to nowa (w 100% zdalna) odsłona naszych stacjonarnych meetupów Allegro Tech Talks. Zazwyczaj spotykaliśmy się w naszych biurach, ale tym razem to…","how_to_find_us":"https://www.facebook.com/allegro.tech/","visibility":"public","member_pay_fee":false},{"created":1593525736000,"duration":3600000,"id":"271624844","name":"Allegro Tech Live #12 - Wszystko o licencjach Open Source","date_in_series_pattern":false,"status":"past","time":1593619200000,"local_date":"2020-07-01","local_time":"18:00","updated":1593625843000,"utc_offset":7200000,"waitlist_count":0,"yes_rsvp_count":34,"venue":{"id":26906060,"name":"Online event","repinned":false,"country":"","localized_country_name":""},"is_online_event":true,"group":{"created":1425052059000,"name":"allegro Tech","id":18465254,"join_mode":"open","lat":52.2599983215332,"lon":21.020000457763672,"urlname":"allegrotech","who":"Techs","localized_location":"Warsaw, Poland","state":"","country":"pl","region":"en_US","timezone":"Europe/Warsaw"},"link":"https://www.meetup.com/allegrotech/events/271624844/","description":"Allegro Tech Live to nowa (w 100% zdalna) odsłona naszych stacjonarnych meetupów Allegro Tech Talks. Zazwyczaj spotykaliśmy się w naszych biurach, ale tym razem to…","how_to_find_us":"https://www.facebook.com/allegro.tech","visibility":"public","member_pay_fee":false},{"created":1592578931000,"duration":7200000,"id":"271396824","name":"Allegro Tech Live #11","date_in_series_pattern":false,"status":"past","time":1593014400000,"local_date":"2020-06-24","local_time":"18:00","updated":1593026541000,"utc_offset":7200000,"waitlist_count":0,"yes_rsvp_count":62,"venue":{"id":26906060,"name":"Online event","repinned":false,"country":"","localized_country_name":""},"is_online_event":true,"group":{"created":1425052059000,"name":"allegro Tech","id":18465254,"join_mode":"open","lat":52.2599983215332,"lon":21.020000457763672,"urlname":"allegrotech","who":"Techs","localized_location":"Warsaw, Poland","state":"","country":"pl","region":"en_US","timezone":"Europe/Warsaw"},"link":"https://www.meetup.com/allegrotech/events/271396824/","description":"Allegro Tech Live to nowa (w 100% zdalna) odsłona naszych stacjonarnych meetupów Allegro Tech Talks. Zazwyczaj spotykaliśmy się w naszych biurach, ale tym razem to…","how_to_find_us":"https://www.facebook.com/allegro.tech","visibility":"public","member_pay_fee":false},{"created":1591785401000,"duration":7200000,"id":"271201706","name":"Allegro Tech Live #10 - ML","date_in_series_pattern":false,"status":"past","time":1592409600000,"local_date":"2020-06-17","local_time":"18:00","updated":1592418866000,"utc_offset":7200000,"waitlist_count":0,"yes_rsvp_count":69,"venue":{"id":26906060,"name":"Online event","repinned":false,"country":"","localized_country_name":""},"is_online_event":true,"group":{"created":1425052059000,"name":"allegro Tech","id":18465254,"join_mode":"open","lat":52.2599983215332,"lon":21.020000457763672,"urlname":"allegrotech","who":"Techs","localized_location":"Warsaw, Poland","state":"","country":"pl","region":"en_US","timezone":"Europe/Warsaw"},"link":"https://www.meetup.com/allegrotech/events/271201706/","description":"Allegro Tech Live to nowa (w 100% zdalna) odsłona naszych stacjonarnych meetupów Allegro Tech Talks. Zazwyczaj spotykaliśmy się w naszych biurach, ale tym razem to…","how_to_find_us":"https://www.facebook.com/allegro.tech/","visibility":"public","member_pay_fee":false}],"podcasts":[{"creator":{"name":["Ireneusz Gawlik"]},"title":"Badania i rozwój ML w Allegro","link":"https://podcast.allegro.tech/badania_i_rozwoj_ml_w_allegro","pubDate":"Mon, 29 Jun 2020 00:00:00 GMT","author":{"name":["Ireneusz Gawlik"]},"enclosure":{"url":"https://www.buzzsprout.com/887914/2998819-13-badania-i-rozwoj-ml-w-allegro-ireneusz-gawlik.mp3","type":"audio/mpeg"},"content":"Gdzie kryje się Machine Learning w Allegro? Jakie projekty już dzisiaj korzystają z mocy sztucznej inteligencji? Jak na codzień pracuje grupa badaczy nie tylko aplikująca, ale też rozwijająca algorytmy uczenia maszynowego? Irek, Team Manager w grupie Allegro ML Research, opowiada więcej o tym jak się tworzy AI w Polsce.","contentSnippet":"Gdzie kryje się Machine Learning w Allegro? Jakie projekty już dzisiaj korzystają z mocy sztucznej inteligencji? Jak na codzień pracuje grupa badaczy nie tylko aplikująca, ale też rozwijająca algorytmy uczenia maszynowego? Irek, Team Manager w grupie Allegro ML Research, opowiada więcej o tym jak się tworzy AI w Polsce.","guid":"https://podcast.allegro.tech/badania_i_rozwoj_ml_w_allegro","isoDate":"2020-06-29T00:00:00.000Z","itunes":{"author":"Ireneusz Gawlik","summary":"Gdzie kryje się Machine Learning w Allegro? Jakie projekty już dzisiaj korzystają z mocy sztucznej inteligencji? Jak na codzień pracuje grupa badaczy nie tylko aplikująca, ale też rozwijająca algorytmy uczenia maszynowego? Irek, Team Manager w grupie Allegro ML Research, opowiada więcej o tym jak się tworzy AI w Polsce.","explicit":"false"}},{"creator":{"name":["Kasia Wróbel"]},"title":"Projektowanie UX w Allegro, czyli wszystko zależy od doświadczenia","link":"https://podcast.allegro.tech/projektowanie_ux_w_allegro","pubDate":"Mon, 15 Jun 2020 00:00:00 GMT","author":{"name":["Kasia Wróbel"]},"enclosure":{"url":"https://www.buzzsprout.com/887914/3011998-11-projektowanie-ux-w-allegro-czyli-wszystko-zalezy-od-doswiadczenia-katarzyna-wrobel.mp3","type":"audio/mpeg"},"content":"Projektowanie na styku produktu, biznesu i potrzeb użytkownika. Budzenie ciekawości, czyli czy jesteś w stanie zmienić coś, co sprawia problem tak wielu osobom na raz? Kasia opowiada o \"układaniu klocków\", których użytkownik wie jak użyć.","contentSnippet":"Projektowanie na styku produktu, biznesu i potrzeb użytkownika. Budzenie ciekawości, czyli czy jesteś w stanie zmienić coś, co sprawia problem tak wielu osobom na raz? Kasia opowiada o \"układaniu klocków\", których użytkownik wie jak użyć.","guid":"https://podcast.allegro.tech/projektowanie_ux_w_allegro","isoDate":"2020-06-15T00:00:00.000Z","itunes":{"author":"Kasia Wróbel","summary":"Projektowanie na styku produktu, biznesu i potrzeb użytkownika. Budzenie ciekawości, czyli czy jesteś w stanie zmienić coś, co sprawia problem tak wielu osobom na raz? Kasia opowiada o \"układaniu klocków\", których użytkownik wie jak użyć.","explicit":"false"}},{"creator":{"name":["Kamil Borzym"]},"title":"Mobile w Allegro","link":"https://podcast.allegro.tech/mobile_w_allegro","pubDate":"Mon, 01 Jun 2020 00:00:00 GMT","author":{"name":["Kamil Borzym"]},"enclosure":{"url":"https://www.buzzsprout.com/887914/3011989-11-mobile-w-allegro-kamil-borzym.mp3","type":"audio/mpeg"},"content":"Cały świat obserwuje jak telefony przejmują władzę nad Internetem. Ponad połowa ruchu na Allegro pochodzi z urządzeń mobilnych, a większość tego ruchu to aplikacje natywne. Żarty się skończyły ;) Kamil opowie Wam jak specjaliści mobilni z Allegro skalują development swojej aplikacji.","contentSnippet":"Cały świat obserwuje jak telefony przejmują władzę nad Internetem. Ponad połowa ruchu na Allegro pochodzi z urządzeń mobilnych, a większość tego ruchu to aplikacje natywne. Żarty się skończyły ;) Kamil opowie Wam jak specjaliści mobilni z Allegro skalują development swojej aplikacji.","guid":"https://podcast.allegro.tech/mobile_w_allegro","isoDate":"2020-06-01T00:00:00.000Z","itunes":{"author":"Kamil Borzym","summary":"Cały świat obserwuje jak telefony przejmują władzę nad Internetem. Ponad połowa ruchu na Allegro pochodzi z urządzeń mobilnych, a większość tego ruchu to aplikacje natywne. Żarty się skończyły ;) Kamil opowie Wam jak specjaliści mobilni z Allegro skalują development swojej aplikacji.","explicit":"false"}},{"creator":{"name":["Martyna Niszczota"]},"title":"Misja zmiana branży zakończona sukcesem! Jak wygląda dzień Product Managera","link":"https://podcast.allegro.tech/misja_zmiana_branzy_zakonczona_sukcesem","pubDate":"Mon, 18 May 2020 00:00:00 GMT","author":{"name":["Martyna Niszczota"]},"enclosure":{"url":"https://www.buzzsprout.com/887914/2799574-9-ep09_martyna_niszczota.mp3","type":"audio/mpeg"},"content":"Czy pracując w IT trzeba rozmawiać z ludźmi? Budowanie relacji to mity i każdy skupia się na swoim kodzie? Gdzie w całej tej układance jest Klient, czy jest tylko odbiorcą naszych rozwiązań? Martyna odpowie na te pytania i przybliży nam jak wygląda jej dzień w roli Product Managera. Posiada ona background HRowy, dlatego jej spojrzenie na świat IT może różnić się od tego standardowego.","contentSnippet":"Czy pracując w IT trzeba rozmawiać z ludźmi? Budowanie relacji to mity i każdy skupia się na swoim kodzie? Gdzie w całej tej układance jest Klient, czy jest tylko odbiorcą naszych rozwiązań? Martyna odpowie na te pytania i przybliży nam jak wygląda jej dzień w roli Product Managera. Posiada ona background HRowy, dlatego jej spojrzenie na świat IT może różnić się od tego standardowego.","guid":"https://podcast.allegro.tech/misja_zmiana_branzy_zakonczona_sukcesem","isoDate":"2020-05-18T00:00:00.000Z","itunes":{"author":"Martyna Niszczota","summary":"Czy pracując w IT trzeba rozmawiać z ludźmi? Budowanie relacji to mity i każdy skupia się na swoim kodzie? Gdzie w całej tej układance jest Klient, czy jest tylko odbiorcą naszych rozwiązań? Martyna odpowie na te pytania i przybliży nam jak wygląda jej dzień w roli Product Managera. Posiada ona background HRowy, dlatego jej spojrzenie na świat IT może różnić się od tego standardowego.","explicit":"false"}}]},"__N_SSG":true}