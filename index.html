<!DOCTYPE html><html lang="pl"><head><meta charSet="utf-8"/><link rel="prefetch" href="https://allegrotechio.disqus.com/count.js"/><meta name="viewport" content="initial-scale=1.0, width=device-width"/><meta name="description" content="Allegro Tech to miejsce, w którym nasi inżynierowie dzielą się wiedzą oraz case study z wybranych projektów w firmie - w formie artykułów, podcastów oraz eventów."/><title>Allegro Tech</title><meta property="og:site_name" content="allegro.tech"/><meta property="og:title" content="allegro.tech"/><meta property="og:url" content="https://allegro.tech"/><meta property="og:type" content="site"/><meta property="og:image" content="https://allegro.tech/images/allegro-tech.png"/><link rel="shortcut icon" href="favicon.ico"/><link rel="canonical" href="https://allegro.tech" itemProp="url"/><link rel="preload" href="images/splash.jpg" as="image"/><script async="" src="https://www.googletagmanager.com/gtag/js?id=G-1M1FJ5PXWW"></script><script>
                    window.dataLayer = window.dataLayer || [];
                    function gtag(){dataLayer.push(arguments);}
                    gtag('js', new Date());
                    gtag('config', 'G-1M1FJ5PXWW');
                </script><meta name="next-head-count" content="15"/><link rel="preload" href="/_next/static/css/b2014360ddad0b69a0c7.css" as="style"/><link rel="stylesheet" href="/_next/static/css/b2014360ddad0b69a0c7.css" data-n-g=""/><link rel="preload" href="/_next/static/css/7d4027da99ef35fddfbb.css" as="style"/><link rel="stylesheet" href="/_next/static/css/7d4027da99ef35fddfbb.css" data-n-p=""/><noscript data-n-css=""></noscript><link rel="preload" href="/_next/static/chunks/main-23f4f2620a057c84f895.js" as="script"/><link rel="preload" href="/_next/static/chunks/webpack-50bee04d1dc61f8adf5b.js" as="script"/><link rel="preload" href="/_next/static/chunks/framework.b1290caeda235fc7bf40.js" as="script"/><link rel="preload" href="/_next/static/chunks/b651a894a35ea1e7d121ee74b6c7e8aa31faf049.71d68ec00d361c7548dd.js" as="script"/><link rel="preload" href="/_next/static/chunks/6753331a3a54fd35bc7d92cbaee2b67e833d028a.8bec33551a885abbb7ba.js" as="script"/><link rel="preload" href="/_next/static/chunks/pages/_app-3ea8ca3d27590bc99614.js" as="script"/><link rel="preload" href="/_next/static/chunks/pages/index-ea5413c5f647a984c342.js" as="script"/></head><body><div id="__next"><header class="m-padding-top_16 m-padding-top_24_lg m-padding-right_16 m-padding-right_24_lg m-padding-bottom_16 m-padding-bottom_24_lg m-padding-left_16 m-padding-left_24_lg m-card Header_navbar__2vWRp m-color-bg_card"><nav class="m-width-max_1600 m-margin-left_auto m-margin-right_auto m-display-flex m-flex-justify-between m-flex-items-center"><a href="/"><img src="images/logo.svg" alt="Allegro Tech" width="205" height="45"/></a><div><ul class="m-font-family_sans m-font-size_14 m-line-height_21 m-color_text m-text-size-adjust_100p m-list-style-type_none m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-padding-top_0 m-padding-right_0 m-padding-bottom_0 m-padding-left_0 m-display-flex@lg m-display-none"><li class="m-margin-left-16@lg"><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-display-block m-display-inline@lg m-padding-left-16 m-padding-right-16 m-padding-top-16 m-padding-bottom-16 m-padding-left-0@lg m-padding-top-0@lg m-padding-right-0@lg m-padding-bottom-0@lg m-link--signal m-line-height_21" href="https://blog.allegro.tech">Blog</a></li><li class="m-margin-left-16@lg"><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-display-block m-display-inline@lg m-padding-left-16 m-padding-right-16 m-padding-top-16 m-padding-bottom-16 m-padding-left-0@lg m-padding-top-0@lg m-padding-right-0@lg m-padding-bottom-0@lg m-link--signal m-line-height_21" href="https://podcast.allegro.tech">Podcast</a></li><li class="m-margin-left-16@lg"><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-display-block m-display-inline@lg m-padding-left-16 m-padding-right-16 m-padding-top-16 m-padding-bottom-16 m-padding-left-0@lg m-padding-top-0@lg m-padding-right-0@lg m-padding-bottom-0@lg m-link--signal m-line-height_21" href="https://github.com/Allegro">Open Source</a></li><li class="m-margin-left-16@lg"><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-display-block m-display-inline@lg m-padding-left-16 m-padding-right-16 m-padding-top-16 m-padding-bottom-16 m-padding-left-0@lg m-padding-top-0@lg m-padding-right-0@lg m-padding-bottom-0@lg m-link--signal m-line-height_21" href="https://www.meetup.com/allegrotech/events">Wydarzenia</a></li><li class="m-margin-left-16@lg"><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-display-block m-display-inline@lg m-padding-left-16 m-padding-right-16 m-padding-top-16 m-padding-bottom-16 m-padding-left-0@lg m-padding-top-0@lg m-padding-right-0@lg m-padding-bottom-0@lg m-link--signal m-line-height_21" href="https://praca.allegro.pl">Praca</a></li></ul><button class="m-display-none@lg m-height_40 m-line-height_40 m-border-style-top_none m-border-style-right_none m-border-style-bottom_none m-border-style-left_none m-border-radius-top-left_2 m-border-radius-top-right_2 m-border-radius-bottom-left_2 m-border-radius-bottom-right_2 m-cursor_pointer m-overflow_hidden m-appearance_none m-padding-left_4 m-padding-right_4 m-padding-top_4 m-padding-bottom_4 m-outline-style_dotted--focus m-outline-width_2 m-outline-color_teal m-outline-offset_n2 m-button" style="background:transparent" aria-label="Otwórz menu"><img src="https://assets.allegrostatic.com/metrum/icon/menu-23e046bf68.svg" alt="" class="m-icon" width="32" height="32"/></button></div></nav></header><div class="Header_hero__n5PN5"><div class="m-width-max_1600 m-margin-left_auto m-margin-right_auto m-display-flex m-flex-column m-flex-justify-end Header_image__15JNc"><div class="m-padding-top_16 m-padding-top_24_lg m-padding-right_16 m-padding-right_24_lg m-padding-bottom_16 m-padding-bottom_24_lg m-padding-left_16 m-padding-left_24_lg m-card m-color-bg_desk"><h2 class="m-font-family_roboto m-margin-left_0 m-margin-right_0 m-margin-top_0 m-color_text  m-font-weight_100 m-font-size_32 m-font-size_43_sm m-margin-bottom_16 m-margin-bottom_24_sm m-line-height_125">O nas</h2><p class="m-font-family_sans m-line-height_21 m-font-size_14 m-text-size-adjust_100p m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text">Allegro to jedna z najbardziej zaawansowanych technologicznie firm w naszej części Europy. Allegro to również ponad 1000 specjalistów IT, różnych specjalizacji, rozwijających nasz serwis. Unikatowa skala i złożoność problemów, które rozwiązujemy na co dzień, dają nam możliwość rozwoju przy bardzo różnorodnych projektach. Allegro Tech to miejsce, w którym nasi inżynierowie dzielą się wiedzą oraz case study z wybranych projektów w firmie – w formie artykułów, podcastów oraz eventów.</p></div></div></div><div class="m-width-max_1600 m-margin-left_auto m-margin-right_auto m-padding-top-24"><h2 class="m-font-family_roboto m-margin-left_0 m-margin-right_0 m-margin-top_0 m-color_text m-font-weight_300 m-font-size_27 m-font-size_36_sm m-margin-bottom_16 m-margin-bottom_24_sm m-line-height_125 m-padding-left-24 m-padding-right-24">Blog</h2><div class="m-display_flex m-flex-wrap_1 m-flex-grow_1 m-grid"><div class="m-grid__col m-grid__col--12 m-grid__col--6@sm m-grid__col--3@xl m-display-flex m-flex-direction_column"><article class="m-margin-bottom_16 m-display-flex m-flex-column m-flex-grow_1"><a href="https://blog.allegro.tech/2021/01/impact-of-the-data-model-on-the-MongoDB-database-size.html" title="Impact of data model on MongoDB database size"><img width="388" src="images/post-headers/mongodb.png" alt="Impact of data model on MongoDB database size" class="m-display-block m-width-fluid"/></a><div class="m-padding-top_16 m-padding-top_24_lg m-padding-right_16 m-padding-right_24_lg m-padding-left_16 m-padding-left_24_lg m-card m-display-flex m-flex-column m-flex-grow_1 m-padding-bottom-0 m-color-bg_card"><a href="https://blog.allegro.tech/2021/01/impact-of-the-data-model-on-the-MongoDB-database-size.html" title="Impact of data model on MongoDB database size" class="m-text-decoration_none"><h2 class="m-font-family_roboto m-margin-left_0 m-margin-right_0 m-margin-top_0 m-color_text m-font-weight_500 m-line-height_130 m-margin-bottom_8 m-margin-bottom_16_sm m-font-size_21 m-font-size_25_sm" style="text-overflow:ellipsis;display:-webkit-box;-webkit-box-orient:vertical;-webkit-line-clamp:2;min-height:2em;overflow:hidden">Impact of data model on MongoDB database size</h2></a><time class="m-font-family_sans m-line-height_21 m-font-size_14 m-text-size-adjust_100p m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text m-padding-bottom-16">13 dni temu</time><ul class="m-font-family_sans m-font-size_14 m-line-height_21 m-color_text m-text-size-adjust_100p m-list-style-type_none m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-padding-top_0 m-padding-right_0 m-padding-bottom_0 m-padding-left_0"><li class="m-margin-right-8 m-display-inline-block"><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-line-height_21" href="https://twitter.com/hashtag/mongodb">#<!-- -->mongodb</a></li></ul><p class="m-font-family_sans m-line-height_21 m-font-size_14 m-text-size-adjust_100p m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text m-flex-grow-1 m-padding-top-16">So I was tuning one of our services in order to speed up some MongoDB queries. Incidentally, my attention was caught by
the size of one…</p><div class="m-display-flex m-flex-justify-between m-padding-top-16 m-flex-items_center"><div class="m-display-flex m-flex-items_center"><div class="MuiAvatarGroup-root m-padding-right_16 Post_avatars__1CeVw"><div class="MuiAvatar-root MuiAvatar-circle MuiAvatarGroup-avatar" style="z-index:1"><img alt="Michał Knasiecki" src="https://blog.allegro.tech/img/authors/michal.knasiecki.jpg" class="MuiAvatar-img" width="32" height="32"/></div></div><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-line-height_21" href="https://blog.allegro.tech/authors/michal.knasiecki">Michał Knasiecki</a></div><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-line-height_21" href="https://blog.allegro.tech/2021/01/impact-of-the-data-model-on-the-MongoDB-database-size.html#disqus_thread">0 Comments</a></div><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-margin-top-16 m-display_block m-border-width_1 m-border-color_gray m-border-style-top_solid m-height_40 m-line-height_40 m-text-transform_uppercase m-letter-spacing_2 m-white-space_nowrap m-cursor_pointer m-overflow_hidden m-padding-left_16 m-padding-right_16 m-padding-top_0 m-padding-bottom_0 m-outline-style_dotted--focus m-outline-width_2 m-outline-color_teal m-outline-offset_n2 m-button m-box_border m-text-align_center m-display_inline-block m-color_secondary m-background-position_50p m-background-size_5000p m-transition-property_background-color m-transition-duration_fast m-button--secondary" href="https://blog.allegro.tech/2021/01/impact-of-the-data-model-on-the-MongoDB-database-size.html">przejdź do wpisu</a></div></article></div><div class="m-grid__col m-grid__col--12 m-grid__col--6@sm m-grid__col--3@xl m-display-flex m-flex-direction_column"><article class="m-margin-bottom_16 m-display-flex m-flex-column m-flex-grow_1"><a href="https://blog.allegro.tech/2020/12/speeding-up-warm-builds.html" title="Speeding up warm builds in Xcode"><img width="388" src="images/post-headers/xcode.png" alt="Speeding up warm builds in Xcode" class="m-display-block m-width-fluid"/></a><div class="m-padding-top_16 m-padding-top_24_lg m-padding-right_16 m-padding-right_24_lg m-padding-left_16 m-padding-left_24_lg m-card m-display-flex m-flex-column m-flex-grow_1 m-padding-bottom-0 m-color-bg_card"><a href="https://blog.allegro.tech/2020/12/speeding-up-warm-builds.html" title="Speeding up warm builds in Xcode" class="m-text-decoration_none"><h2 class="m-font-family_roboto m-margin-left_0 m-margin-right_0 m-margin-top_0 m-color_text m-font-weight_500 m-line-height_130 m-margin-bottom_8 m-margin-bottom_16_sm m-font-size_21 m-font-size_25_sm" style="text-overflow:ellipsis;display:-webkit-box;-webkit-box-orient:vertical;-webkit-line-clamp:2;min-height:2em;overflow:hidden">Speeding up warm builds in Xcode</h2></a><time class="m-font-family_sans m-line-height_21 m-font-size_14 m-text-size-adjust_100p m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text m-padding-bottom-16">30 dni temu</time><ul class="m-font-family_sans m-font-size_14 m-line-height_21 m-color_text m-text-size-adjust_100p m-list-style-type_none m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-padding-top_0 m-padding-right_0 m-padding-bottom_0 m-padding-left_0"><li class="m-margin-right-8 m-display-inline-block"><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-line-height_21" href="https://twitter.com/hashtag/ios">#<!-- -->ios</a></li><li class="m-margin-right-8 m-display-inline-block"><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-line-height_21" href="https://twitter.com/hashtag/xcode">#<!-- -->xcode</a></li><li class="m-margin-right-8 m-display-inline-block"><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-line-height_21" href="https://twitter.com/hashtag/swift">#<!-- -->swift</a></li><li class="m-margin-right-8 m-display-inline-block"><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-line-height_21" href="https://twitter.com/hashtag/objectivec">#<!-- -->objectivec</a></li></ul><p class="m-font-family_sans m-line-height_21 m-font-size_14 m-text-size-adjust_100p m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text m-flex-grow-1 m-padding-top-16">Programmers who have ever developed software for Apple platforms in the early days of Swift language might remember ridiculous
times it took to compile the whole…</p><div class="m-display-flex m-flex-justify-between m-padding-top-16 m-flex-items_center"><div class="m-display-flex m-flex-items_center"><div class="MuiAvatarGroup-root m-padding-right_16 Post_avatars__1CeVw"><div class="MuiAvatar-root MuiAvatar-circle MuiAvatarGroup-avatar" style="z-index:1"><img alt="Maciej Piotrowski" src="https://blog.allegro.tech/img/authors/maciej.piotrowski.jpg" class="MuiAvatar-img" width="32" height="32"/></div></div><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-line-height_21" href="https://blog.allegro.tech/authors/maciej.piotrowski">Maciej Piotrowski</a></div><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-line-height_21" href="https://blog.allegro.tech/2020/12/speeding-up-warm-builds.html#disqus_thread">0 Comments</a></div><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-margin-top-16 m-display_block m-border-width_1 m-border-color_gray m-border-style-top_solid m-height_40 m-line-height_40 m-text-transform_uppercase m-letter-spacing_2 m-white-space_nowrap m-cursor_pointer m-overflow_hidden m-padding-left_16 m-padding-right_16 m-padding-top_0 m-padding-bottom_0 m-outline-style_dotted--focus m-outline-width_2 m-outline-color_teal m-outline-offset_n2 m-button m-box_border m-text-align_center m-display_inline-block m-color_secondary m-background-position_50p m-background-size_5000p m-transition-property_background-color m-transition-duration_fast m-button--secondary" href="https://blog.allegro.tech/2020/12/speeding-up-warm-builds.html">przejdź do wpisu</a></div></article></div><div class="m-grid__col m-grid__col--12 m-grid__col--6@sm m-grid__col--3@xl m-display-flex m-flex-direction_column"><article class="m-margin-bottom_16 m-display-flex m-flex-column m-flex-grow_1"><a href="https://blog.allegro.tech/2020/12/speeding-up-ios-builds-with-bazel.html" title="Speeding up iOS builds with Bazel"><img width="388" src="images/post-headers/bazel.png" alt="Speeding up iOS builds with Bazel" class="m-display-block m-width-fluid"/></a><div class="m-padding-top_16 m-padding-top_24_lg m-padding-right_16 m-padding-right_24_lg m-padding-left_16 m-padding-left_24_lg m-card m-display-flex m-flex-column m-flex-grow_1 m-padding-bottom-0 m-color-bg_card"><a href="https://blog.allegro.tech/2020/12/speeding-up-ios-builds-with-bazel.html" title="Speeding up iOS builds with Bazel" class="m-text-decoration_none"><h2 class="m-font-family_roboto m-margin-left_0 m-margin-right_0 m-margin-top_0 m-color_text m-font-weight_500 m-line-height_130 m-margin-bottom_8 m-margin-bottom_16_sm m-font-size_21 m-font-size_25_sm" style="text-overflow:ellipsis;display:-webkit-box;-webkit-box-orient:vertical;-webkit-line-clamp:2;min-height:2em;overflow:hidden">Speeding up iOS builds with Bazel</h2></a><time class="m-font-family_sans m-line-height_21 m-font-size_14 m-text-size-adjust_100p m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text m-padding-bottom-16">około miesiąc temu</time><ul class="m-font-family_sans m-font-size_14 m-line-height_21 m-color_text m-text-size-adjust_100p m-list-style-type_none m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-padding-top_0 m-padding-right_0 m-padding-bottom_0 m-padding-left_0"><li class="m-margin-right-8 m-display-inline-block"><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-line-height_21" href="https://twitter.com/hashtag/ios">#<!-- -->ios</a></li><li class="m-margin-right-8 m-display-inline-block"><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-line-height_21" href="https://twitter.com/hashtag/bazel">#<!-- -->bazel</a></li></ul><p class="m-font-family_sans m-line-height_21 m-font-size_14 m-text-size-adjust_100p m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text m-flex-grow-1 m-padding-top-16">When we developed our Allegro iOS app adding new features and with more people contributing to the codebase, we noticed
that build times began to grow.…</p><div class="m-display-flex m-flex-justify-between m-padding-top-16 m-flex-items_center"><div class="m-display-flex m-flex-items_center"><div class="MuiAvatarGroup-root m-padding-right_16 Post_avatars__1CeVw"><div class="MuiAvatar-root MuiAvatar-circle MuiAvatarGroup-avatar" style="z-index:1"><img alt="Kamil Pyć" src="https://blog.allegro.tech/img/authors/kamil.pyc.jpg" class="MuiAvatar-img" width="32" height="32"/></div></div><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-line-height_21" href="https://blog.allegro.tech/authors/kamil.pyc">Kamil Pyć</a></div><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-line-height_21" href="https://blog.allegro.tech/2020/12/speeding-up-ios-builds-with-bazel.html#disqus_thread">0 Comments</a></div><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-margin-top-16 m-display_block m-border-width_1 m-border-color_gray m-border-style-top_solid m-height_40 m-line-height_40 m-text-transform_uppercase m-letter-spacing_2 m-white-space_nowrap m-cursor_pointer m-overflow_hidden m-padding-left_16 m-padding-right_16 m-padding-top_0 m-padding-bottom_0 m-outline-style_dotted--focus m-outline-width_2 m-outline-color_teal m-outline-offset_n2 m-button m-box_border m-text-align_center m-display_inline-block m-color_secondary m-background-position_50p m-background-size_5000p m-transition-property_background-color m-transition-duration_fast m-button--secondary" href="https://blog.allegro.tech/2020/12/speeding-up-ios-builds-with-bazel.html">przejdź do wpisu</a></div></article></div><div class="m-grid__col m-grid__col--12 m-grid__col--6@sm m-grid__col--3@xl m-display-flex m-flex-direction_column"><article class="m-margin-bottom_16 m-display-flex m-flex-column m-flex-grow_1"><a href="https://blog.allegro.tech/2020/12/bigdata-marketing.html" title="Big data marketing. The story of how technology behind Allegro marketing works."><img width="388" src="images/post-headers/spark.png" alt="Big data marketing. The story of how technology behind Allegro marketing works." class="m-display-block m-width-fluid"/></a><div class="m-padding-top_16 m-padding-top_24_lg m-padding-right_16 m-padding-right_24_lg m-padding-left_16 m-padding-left_24_lg m-card m-display-flex m-flex-column m-flex-grow_1 m-padding-bottom-0 m-color-bg_card"><a href="https://blog.allegro.tech/2020/12/bigdata-marketing.html" title="Big data marketing. The story of how technology behind Allegro marketing works." class="m-text-decoration_none"><h2 class="m-font-family_roboto m-margin-left_0 m-margin-right_0 m-margin-top_0 m-color_text m-font-weight_500 m-line-height_130 m-margin-bottom_8 m-margin-bottom_16_sm m-font-size_21 m-font-size_25_sm" style="text-overflow:ellipsis;display:-webkit-box;-webkit-box-orient:vertical;-webkit-line-clamp:2;min-height:2em;overflow:hidden">Big data marketing. The story of how technology behind Allegro marketing works.</h2></a><time class="m-font-family_sans m-line-height_21 m-font-size_14 m-text-size-adjust_100p m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text m-padding-bottom-16">około 2 miesiące temu</time><ul class="m-font-family_sans m-font-size_14 m-line-height_21 m-color_text m-text-size-adjust_100p m-list-style-type_none m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-padding-top_0 m-padding-right_0 m-padding-bottom_0 m-padding-left_0"><li class="m-margin-right-8 m-display-inline-block"><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-line-height_21" href="https://twitter.com/hashtag/architecture">#<!-- -->architecture</a></li><li class="m-margin-right-8 m-display-inline-block"><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-line-height_21" href="https://twitter.com/hashtag/bigdata">#<!-- -->bigdata</a></li><li class="m-margin-right-8 m-display-inline-block"><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-line-height_21" href="https://twitter.com/hashtag/spark">#<!-- -->spark</a></li></ul><p class="m-font-family_sans m-line-height_21 m-font-size_14 m-text-size-adjust_100p m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text m-flex-grow-1 m-padding-top-16">Marketing is a very important department in every company. In case of Allegro,
marketing is especially difficult because you have so many products to promote.
In this…</p><div class="m-display-flex m-flex-justify-between m-padding-top-16 m-flex-items_center"><div class="m-display-flex m-flex-items_center"><div class="MuiAvatarGroup-root m-padding-right_16 Post_avatars__1CeVw"><div class="MuiAvatar-root MuiAvatar-circle MuiAvatarGroup-avatar" style="z-index:3"><img alt="Filip Błaszczyk" src="https://blog.allegro.tech/img/authors/filip.blaszczyk.jpg" class="MuiAvatar-img" width="32" height="32"/></div><div class="MuiAvatar-root MuiAvatar-circle MuiAvatarGroup-avatar MuiAvatar-colorDefault" style="z-index:0">+<!-- -->2</div></div><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-line-height_21" href="https://blog.allegro.tech/authors/filip.blaszczyk">Filip Błaszczyk…</a></div><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-line-height_21" href="https://blog.allegro.tech/2020/12/bigdata-marketing.html#disqus_thread">0 Comments</a></div><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-margin-top-16 m-display_block m-border-width_1 m-border-color_gray m-border-style-top_solid m-height_40 m-line-height_40 m-text-transform_uppercase m-letter-spacing_2 m-white-space_nowrap m-cursor_pointer m-overflow_hidden m-padding-left_16 m-padding-right_16 m-padding-top_0 m-padding-bottom_0 m-outline-style_dotted--focus m-outline-width_2 m-outline-color_teal m-outline-offset_n2 m-button m-box_border m-text-align_center m-display_inline-block m-color_secondary m-background-position_50p m-background-size_5000p m-transition-property_background-color m-transition-duration_fast m-button--secondary" href="https://blog.allegro.tech/2020/12/bigdata-marketing.html">przejdź do wpisu</a></div></article></div></div><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-display_block m-margin-bottom_8 m-width_100 m-height_40 m-line-height_40 m-text-transform_uppercase m-letter-spacing_2 m-white-space_nowrap m-cursor_pointer m-overflow_hidden m-padding-left_16 m-padding-right_16 m-padding-top_0 m-padding-bottom_0 m-outline-style_dotted--focus m-outline-width_2 m-outline-color_teal m-outline-offset_n2 m-button m-box_border m-text-align_center m-display_inline-block m-color_secondary m-background-position_50p m-background-size_5000p m-transition-property_background-color m-transition-duration_fast m-button--secondary" href="https://blog.allegro.tech">Zobacz więcej wpisów</a></div><div class="m-width-max_1600 m-margin-left_auto m-margin-right_auto m-padding-top-24"><h2 class="m-font-family_roboto m-margin-left_0 m-margin-right_0 m-margin-top_0 m-color_text m-font-weight_300 m-font-size_27 m-font-size_36_sm m-margin-bottom_16 m-margin-bottom_24_sm m-line-height_125 m-padding-left-24 m-padding-right-24">Podcasty</h2><div class="m-display_flex m-flex-wrap_1 m-flex-grow_1 m-grid"><div class="m-grid__col m-grid__col--12 m-grid__col--6@sm m-grid__col--3@xl m-display-flex m-flex-direction_column"><article class="m-margin-bottom_16 m-display-flex m-flex-column m-flex-grow_1"><a href="https://podcast.allegro.tech/badania_i_rozwoj_ml_w_allegro" title="Badania i rozwój ML w Allegro"><img src="images/podcast.png" alt="Badania i rozwój ML w Allegro" class="m-display-block m-width-fluid"/></a><div class="m-padding-top_16 m-padding-top_24_lg m-padding-right_16 m-padding-right_24_lg m-padding-left_16 m-padding-left_24_lg m-card m-display-flex m-flex-column m-flex-grow_1 m-padding-bottom-0 m-color-bg_card"><a href="https://podcast.allegro.tech/badania_i_rozwoj_ml_w_allegro" title="Badania i rozwój ML w Allegro" class="m-text-decoration_none"><h2 class="m-font-family_roboto m-margin-left_0 m-margin-right_0 m-margin-top_0 m-color_text m-font-weight_500 m-line-height_130 m-margin-bottom_8 m-margin-bottom_16_sm m-font-size_21 m-font-size_25_sm" style="text-overflow:ellipsis;display:-webkit-box;-webkit-box-orient:vertical;-webkit-line-clamp:2;min-height:2em;overflow:hidden">Badania i rozwój ML w Allegro</h2></a><time class="m-font-family_sans m-line-height_21 m-font-size_14 m-text-size-adjust_100p m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text m-padding-bottom-16">7 miesięcy temu</time><p class="m-font-family_sans m-line-height_21 m-font-size_14 m-text-size-adjust_100p m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text m-flex-grow-1">Gdzie kryje się Machine Learning w Allegro? Jakie projekty już dzisiaj korzystają z mocy sztucznej inteligencji? Jak na codzień pracuje grupa badaczy nie tylko aplikująca, ale też rozwijająca algorytmy uczenia maszynowego? Irek, Team Manager w grupie Allegro ML Research, opowiada więcej o tym jak się tworzy AI w Polsce.</p><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-margin-top-16 m-display_block m-border-width_1 m-border-color_gray m-border-style-top_solid m-height_40 m-line-height_40 m-text-transform_uppercase m-letter-spacing_2 m-white-space_nowrap m-cursor_pointer m-overflow_hidden m-padding-left_16 m-padding-right_16 m-padding-top_0 m-padding-bottom_0 m-outline-style_dotted--focus m-outline-width_2 m-outline-color_teal m-outline-offset_n2 m-button m-box_border m-text-align_center m-display_inline-block m-color_secondary m-background-position_50p m-background-size_5000p m-transition-property_background-color m-transition-duration_fast m-button--secondary" href="https://podcast.allegro.tech/badania_i_rozwoj_ml_w_allegro">Posłuchaj odcinka</a></div></article></div><div class="m-grid__col m-grid__col--12 m-grid__col--6@sm m-grid__col--3@xl m-display-flex m-flex-direction_column"><article class="m-margin-bottom_16 m-display-flex m-flex-column m-flex-grow_1"><a href="https://podcast.allegro.tech/projektowanie_ux_w_allegro" title="Projektowanie UX w Allegro, czyli wszystko zależy od doświadczenia"><img src="images/podcast.png" alt="Projektowanie UX w Allegro, czyli wszystko zależy od doświadczenia" class="m-display-block m-width-fluid"/></a><div class="m-padding-top_16 m-padding-top_24_lg m-padding-right_16 m-padding-right_24_lg m-padding-left_16 m-padding-left_24_lg m-card m-display-flex m-flex-column m-flex-grow_1 m-padding-bottom-0 m-color-bg_card"><a href="https://podcast.allegro.tech/projektowanie_ux_w_allegro" title="Projektowanie UX w Allegro, czyli wszystko zależy od doświadczenia" class="m-text-decoration_none"><h2 class="m-font-family_roboto m-margin-left_0 m-margin-right_0 m-margin-top_0 m-color_text m-font-weight_500 m-line-height_130 m-margin-bottom_8 m-margin-bottom_16_sm m-font-size_21 m-font-size_25_sm" style="text-overflow:ellipsis;display:-webkit-box;-webkit-box-orient:vertical;-webkit-line-clamp:2;min-height:2em;overflow:hidden">Projektowanie UX w Allegro, czyli wszystko zależy od doświadczenia</h2></a><time class="m-font-family_sans m-line-height_21 m-font-size_14 m-text-size-adjust_100p m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text m-padding-bottom-16">8 miesięcy temu</time><p class="m-font-family_sans m-line-height_21 m-font-size_14 m-text-size-adjust_100p m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text m-flex-grow-1">Projektowanie na styku produktu, biznesu i potrzeb użytkownika. Budzenie ciekawości, czyli czy jesteś w stanie zmienić coś, co sprawia problem tak wielu osobom na raz? Kasia opowiada o &quot;układaniu klocków&quot;, których użytkownik wie jak użyć.</p><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-margin-top-16 m-display_block m-border-width_1 m-border-color_gray m-border-style-top_solid m-height_40 m-line-height_40 m-text-transform_uppercase m-letter-spacing_2 m-white-space_nowrap m-cursor_pointer m-overflow_hidden m-padding-left_16 m-padding-right_16 m-padding-top_0 m-padding-bottom_0 m-outline-style_dotted--focus m-outline-width_2 m-outline-color_teal m-outline-offset_n2 m-button m-box_border m-text-align_center m-display_inline-block m-color_secondary m-background-position_50p m-background-size_5000p m-transition-property_background-color m-transition-duration_fast m-button--secondary" href="https://podcast.allegro.tech/projektowanie_ux_w_allegro">Posłuchaj odcinka</a></div></article></div><div class="m-grid__col m-grid__col--12 m-grid__col--6@sm m-grid__col--3@xl m-display-flex m-flex-direction_column"><article class="m-margin-bottom_16 m-display-flex m-flex-column m-flex-grow_1"><a href="https://podcast.allegro.tech/mobile_w_allegro" title="Mobile w Allegro"><img src="images/podcast.png" alt="Mobile w Allegro" class="m-display-block m-width-fluid"/></a><div class="m-padding-top_16 m-padding-top_24_lg m-padding-right_16 m-padding-right_24_lg m-padding-left_16 m-padding-left_24_lg m-card m-display-flex m-flex-column m-flex-grow_1 m-padding-bottom-0 m-color-bg_card"><a href="https://podcast.allegro.tech/mobile_w_allegro" title="Mobile w Allegro" class="m-text-decoration_none"><h2 class="m-font-family_roboto m-margin-left_0 m-margin-right_0 m-margin-top_0 m-color_text m-font-weight_500 m-line-height_130 m-margin-bottom_8 m-margin-bottom_16_sm m-font-size_21 m-font-size_25_sm" style="text-overflow:ellipsis;display:-webkit-box;-webkit-box-orient:vertical;-webkit-line-clamp:2;min-height:2em;overflow:hidden">Mobile w Allegro</h2></a><time class="m-font-family_sans m-line-height_21 m-font-size_14 m-text-size-adjust_100p m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text m-padding-bottom-16">8 miesięcy temu</time><p class="m-font-family_sans m-line-height_21 m-font-size_14 m-text-size-adjust_100p m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text m-flex-grow-1">Cały świat obserwuje jak telefony przejmują władzę nad Internetem. Ponad połowa ruchu na Allegro pochodzi z urządzeń mobilnych, a większość tego ruchu to aplikacje natywne. Żarty się skończyły ;) Kamil opowie Wam jak specjaliści mobilni z Allegro skalują development swojej aplikacji.</p><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-margin-top-16 m-display_block m-border-width_1 m-border-color_gray m-border-style-top_solid m-height_40 m-line-height_40 m-text-transform_uppercase m-letter-spacing_2 m-white-space_nowrap m-cursor_pointer m-overflow_hidden m-padding-left_16 m-padding-right_16 m-padding-top_0 m-padding-bottom_0 m-outline-style_dotted--focus m-outline-width_2 m-outline-color_teal m-outline-offset_n2 m-button m-box_border m-text-align_center m-display_inline-block m-color_secondary m-background-position_50p m-background-size_5000p m-transition-property_background-color m-transition-duration_fast m-button--secondary" href="https://podcast.allegro.tech/mobile_w_allegro">Posłuchaj odcinka</a></div></article></div><div class="m-grid__col m-grid__col--12 m-grid__col--6@sm m-grid__col--3@xl m-display-flex m-flex-direction_column"><article class="m-margin-bottom_16 m-display-flex m-flex-column m-flex-grow_1"><a href="https://podcast.allegro.tech/misja_zmiana_branzy_zakonczona_sukcesem" title="Misja zmiana branży zakończona sukcesem! Jak wygląda dzień Product Managera"><img src="images/podcast.png" alt="Misja zmiana branży zakończona sukcesem! Jak wygląda dzień Product Managera" class="m-display-block m-width-fluid"/></a><div class="m-padding-top_16 m-padding-top_24_lg m-padding-right_16 m-padding-right_24_lg m-padding-left_16 m-padding-left_24_lg m-card m-display-flex m-flex-column m-flex-grow_1 m-padding-bottom-0 m-color-bg_card"><a href="https://podcast.allegro.tech/misja_zmiana_branzy_zakonczona_sukcesem" title="Misja zmiana branży zakończona sukcesem! Jak wygląda dzień Product Managera" class="m-text-decoration_none"><h2 class="m-font-family_roboto m-margin-left_0 m-margin-right_0 m-margin-top_0 m-color_text m-font-weight_500 m-line-height_130 m-margin-bottom_8 m-margin-bottom_16_sm m-font-size_21 m-font-size_25_sm" style="text-overflow:ellipsis;display:-webkit-box;-webkit-box-orient:vertical;-webkit-line-clamp:2;min-height:2em;overflow:hidden">Misja zmiana branży zakończona sukcesem! Jak wygląda dzień Product Managera</h2></a><time class="m-font-family_sans m-line-height_21 m-font-size_14 m-text-size-adjust_100p m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text m-padding-bottom-16">8 miesięcy temu</time><p class="m-font-family_sans m-line-height_21 m-font-size_14 m-text-size-adjust_100p m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text m-flex-grow-1">Czy pracując w IT trzeba rozmawiać z ludźmi? Budowanie relacji to mity i każdy skupia się na swoim kodzie? Gdzie w całej tej układance jest Klient, czy jest tylko odbiorcą naszych rozwiązań? Martyna odpowie na te pytania i przybliży nam jak wygląda jej dzień w roli Product Managera. Posiada ona background HRowy, dlatego jej spojrzenie na świat IT może różnić się od tego standardowego.</p><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-margin-top-16 m-display_block m-border-width_1 m-border-color_gray m-border-style-top_solid m-height_40 m-line-height_40 m-text-transform_uppercase m-letter-spacing_2 m-white-space_nowrap m-cursor_pointer m-overflow_hidden m-padding-left_16 m-padding-right_16 m-padding-top_0 m-padding-bottom_0 m-outline-style_dotted--focus m-outline-width_2 m-outline-color_teal m-outline-offset_n2 m-button m-box_border m-text-align_center m-display_inline-block m-color_secondary m-background-position_50p m-background-size_5000p m-transition-property_background-color m-transition-duration_fast m-button--secondary" href="https://podcast.allegro.tech/misja_zmiana_branzy_zakonczona_sukcesem">Posłuchaj odcinka</a></div></article></div></div><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-display_block m-margin-bottom_8 m-width_100 m-height_40 m-line-height_40 m-text-transform_uppercase m-letter-spacing_2 m-white-space_nowrap m-cursor_pointer m-overflow_hidden m-padding-left_16 m-padding-right_16 m-padding-top_0 m-padding-bottom_0 m-outline-style_dotted--focus m-outline-width_2 m-outline-color_teal m-outline-offset_n2 m-button m-box_border m-text-align_center m-display_inline-block m-color_secondary m-background-position_50p m-background-size_5000p m-transition-property_background-color m-transition-duration_fast m-button--secondary" href="https://podcast.allegro.tech">Zobacz więcej podcastów</a></div><div class="m-width-max_1600 m-margin-left_auto m-margin-right_auto m-padding-top-24"><h2 class="m-font-family_roboto m-margin-left_0 m-margin-right_0 m-margin-top_0 m-color_text m-font-weight_300 m-font-size_27 m-font-size_36_sm m-margin-bottom_16 m-margin-bottom_24_sm m-line-height_125 m-padding-left-24 m-padding-right-24">Wydarzenia</h2><div class="m-display_flex m-flex-wrap_1 m-flex-grow_1 m-grid"><div class="m-grid__col m-grid__col--12 m-grid__col--6@sm m-grid__col--6@xl m-display-flex m-flex-direction_column"><div class="m-margin-bottom_16 m-display-flex m-flex-direction_column m-flex-direction_row_sm m-padding-bottom_0"><a href="https://www.meetup.com/allegrotech/events/272249143/" title="Allegro Tech Talks #13 - Cyfrodziewczyny" class="m-display_none m-display_block_lg" style="background-color:#fd4a02"><img width="218" src="images/event.png" alt="Allegro Tech Talks #13 - Cyfrodziewczyny"/></a><article class="m-padding-top_16 m-padding-top_24_lg m-padding-right_16 m-padding-right_24_lg m-padding-left_16 m-padding-left_24_lg m-card m-display-flex m-flex-direction_column m-padding-bottom_0 m-flex_1 m-flex-justify_between m-color-bg_card"><a href="https://www.meetup.com/allegrotech/events/272249143/" title="Allegro Tech Talks #13 - Cyfrodziewczyny" class="m-text-decoration_none"><h2 class="m-font-family_roboto m-margin-left_0 m-margin-right_0 m-margin-top_0 m-color_text m-font-weight_500 m-line-height_130 m-margin-bottom_8 m-margin-bottom_16_sm m-font-size_21 m-font-size_25_sm" style="text-overflow:ellipsis;display:-webkit-box;-webkit-box-orient:vertical;-webkit-line-clamp:2;min-height:2em;overflow:hidden">Allegro Tech Talks #13 - Cyfrodziewczyny</h2></a><time class="m-font-family_sans m-line-height_21 m-font-size_14 m-text-size-adjust_100p m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text">6 miesięcy temu<!-- -->, <!-- -->Online event</time><time class="m-font-family_sans m-line-height_21 m-font-size_14 m-text-size-adjust_100p m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text m-padding-top-8">Allegro Tech Live to nowa (w 100% zdalna) odsłona naszych stacjonarnych meetupów Allegro Tech Talks. Zazwyczaj spotykaliśmy się w naszych biurach, ale tym razem to…</time><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-margin-top-16 m-display_block m-border-width_1 m-border-color_gray m-border-style-top_solid m-height_40 m-line-height_40 m-text-transform_uppercase m-letter-spacing_2 m-white-space_nowrap m-cursor_pointer m-overflow_hidden m-padding-left_16 m-padding-right_16 m-padding-top_0 m-padding-bottom_0 m-outline-style_dotted--focus m-outline-width_2 m-outline-color_teal m-outline-offset_n2 m-button m-box_border m-text-align_center m-display_inline-block m-color_secondary m-background-position_50p m-background-size_5000p m-transition-property_background-color m-transition-duration_fast m-button--secondary" href="https://www.meetup.com/allegrotech/events/272249143/">Szczegóły</a></article></div></div><div class="m-grid__col m-grid__col--12 m-grid__col--6@sm m-grid__col--6@xl m-display-flex m-flex-direction_column"><div class="m-margin-bottom_16 m-display-flex m-flex-direction_column m-flex-direction_row_sm m-padding-bottom_0"><a href="https://www.meetup.com/allegrotech/events/271624844/" title="Allegro Tech Live #12 - Wszystko o licencjach Open Source" class="m-display_none m-display_block_lg" style="background-color:#fd4a02"><img width="218" src="images/event.png" alt="Allegro Tech Live #12 - Wszystko o licencjach Open Source"/></a><article class="m-padding-top_16 m-padding-top_24_lg m-padding-right_16 m-padding-right_24_lg m-padding-left_16 m-padding-left_24_lg m-card m-display-flex m-flex-direction_column m-padding-bottom_0 m-flex_1 m-flex-justify_between m-color-bg_card"><a href="https://www.meetup.com/allegrotech/events/271624844/" title="Allegro Tech Live #12 - Wszystko o licencjach Open Source" class="m-text-decoration_none"><h2 class="m-font-family_roboto m-margin-left_0 m-margin-right_0 m-margin-top_0 m-color_text m-font-weight_500 m-line-height_130 m-margin-bottom_8 m-margin-bottom_16_sm m-font-size_21 m-font-size_25_sm" style="text-overflow:ellipsis;display:-webkit-box;-webkit-box-orient:vertical;-webkit-line-clamp:2;min-height:2em;overflow:hidden">Allegro Tech Live #12 - Wszystko o licencjach Open Source</h2></a><time class="m-font-family_sans m-line-height_21 m-font-size_14 m-text-size-adjust_100p m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text">7 miesięcy temu<!-- -->, <!-- -->Online event</time><time class="m-font-family_sans m-line-height_21 m-font-size_14 m-text-size-adjust_100p m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text m-padding-top-8">Allegro Tech Live to nowa (w 100% zdalna) odsłona naszych stacjonarnych meetupów Allegro Tech Talks. Zazwyczaj spotykaliśmy się w naszych biurach, ale tym razem to…</time><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-margin-top-16 m-display_block m-border-width_1 m-border-color_gray m-border-style-top_solid m-height_40 m-line-height_40 m-text-transform_uppercase m-letter-spacing_2 m-white-space_nowrap m-cursor_pointer m-overflow_hidden m-padding-left_16 m-padding-right_16 m-padding-top_0 m-padding-bottom_0 m-outline-style_dotted--focus m-outline-width_2 m-outline-color_teal m-outline-offset_n2 m-button m-box_border m-text-align_center m-display_inline-block m-color_secondary m-background-position_50p m-background-size_5000p m-transition-property_background-color m-transition-duration_fast m-button--secondary" href="https://www.meetup.com/allegrotech/events/271624844/">Szczegóły</a></article></div></div><div class="m-grid__col m-grid__col--12 m-grid__col--6@sm m-grid__col--6@xl m-display-flex m-flex-direction_column"><div class="m-margin-bottom_16 m-display-flex m-flex-direction_column m-flex-direction_row_sm m-padding-bottom_0"><a href="https://www.meetup.com/allegrotech/events/271396824/" title="Allegro Tech Live #11" class="m-display_none m-display_block_lg" style="background-color:#fd4a02"><img width="218" src="images/event.png" alt="Allegro Tech Live #11"/></a><article class="m-padding-top_16 m-padding-top_24_lg m-padding-right_16 m-padding-right_24_lg m-padding-left_16 m-padding-left_24_lg m-card m-display-flex m-flex-direction_column m-padding-bottom_0 m-flex_1 m-flex-justify_between m-color-bg_card"><a href="https://www.meetup.com/allegrotech/events/271396824/" title="Allegro Tech Live #11" class="m-text-decoration_none"><h2 class="m-font-family_roboto m-margin-left_0 m-margin-right_0 m-margin-top_0 m-color_text m-font-weight_500 m-line-height_130 m-margin-bottom_8 m-margin-bottom_16_sm m-font-size_21 m-font-size_25_sm" style="text-overflow:ellipsis;display:-webkit-box;-webkit-box-orient:vertical;-webkit-line-clamp:2;min-height:2em;overflow:hidden">Allegro Tech Live #11</h2></a><time class="m-font-family_sans m-line-height_21 m-font-size_14 m-text-size-adjust_100p m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text">7 miesięcy temu<!-- -->, <!-- -->Online event</time><time class="m-font-family_sans m-line-height_21 m-font-size_14 m-text-size-adjust_100p m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text m-padding-top-8">Allegro Tech Live to nowa (w 100% zdalna) odsłona naszych stacjonarnych meetupów Allegro Tech Talks. Zazwyczaj spotykaliśmy się w naszych biurach, ale tym razem to…</time><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-margin-top-16 m-display_block m-border-width_1 m-border-color_gray m-border-style-top_solid m-height_40 m-line-height_40 m-text-transform_uppercase m-letter-spacing_2 m-white-space_nowrap m-cursor_pointer m-overflow_hidden m-padding-left_16 m-padding-right_16 m-padding-top_0 m-padding-bottom_0 m-outline-style_dotted--focus m-outline-width_2 m-outline-color_teal m-outline-offset_n2 m-button m-box_border m-text-align_center m-display_inline-block m-color_secondary m-background-position_50p m-background-size_5000p m-transition-property_background-color m-transition-duration_fast m-button--secondary" href="https://www.meetup.com/allegrotech/events/271396824/">Szczegóły</a></article></div></div><div class="m-grid__col m-grid__col--12 m-grid__col--6@sm m-grid__col--6@xl m-display-flex m-flex-direction_column"><div class="m-margin-bottom_16 m-display-flex m-flex-direction_column m-flex-direction_row_sm m-padding-bottom_0"><a href="https://www.meetup.com/allegrotech/events/271201706/" title="Allegro Tech Live #10 - ML" class="m-display_none m-display_block_lg" style="background-color:#fd4a02"><img width="218" src="images/event.png" alt="Allegro Tech Live #10 - ML"/></a><article class="m-padding-top_16 m-padding-top_24_lg m-padding-right_16 m-padding-right_24_lg m-padding-left_16 m-padding-left_24_lg m-card m-display-flex m-flex-direction_column m-padding-bottom_0 m-flex_1 m-flex-justify_between m-color-bg_card"><a href="https://www.meetup.com/allegrotech/events/271201706/" title="Allegro Tech Live #10 - ML" class="m-text-decoration_none"><h2 class="m-font-family_roboto m-margin-left_0 m-margin-right_0 m-margin-top_0 m-color_text m-font-weight_500 m-line-height_130 m-margin-bottom_8 m-margin-bottom_16_sm m-font-size_21 m-font-size_25_sm" style="text-overflow:ellipsis;display:-webkit-box;-webkit-box-orient:vertical;-webkit-line-clamp:2;min-height:2em;overflow:hidden">Allegro Tech Live #10 - ML</h2></a><time class="m-font-family_sans m-line-height_21 m-font-size_14 m-text-size-adjust_100p m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text">7 miesięcy temu<!-- -->, <!-- -->Online event</time><time class="m-font-family_sans m-line-height_21 m-font-size_14 m-text-size-adjust_100p m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text m-padding-top-8">Allegro Tech Live to nowa (w 100% zdalna) odsłona naszych stacjonarnych meetupów Allegro Tech Talks. Zazwyczaj spotykaliśmy się w naszych biurach, ale tym razem to…</time><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-margin-top-16 m-display_block m-border-width_1 m-border-color_gray m-border-style-top_solid m-height_40 m-line-height_40 m-text-transform_uppercase m-letter-spacing_2 m-white-space_nowrap m-cursor_pointer m-overflow_hidden m-padding-left_16 m-padding-right_16 m-padding-top_0 m-padding-bottom_0 m-outline-style_dotted--focus m-outline-width_2 m-outline-color_teal m-outline-offset_n2 m-button m-box_border m-text-align_center m-display_inline-block m-color_secondary m-background-position_50p m-background-size_5000p m-transition-property_background-color m-transition-duration_fast m-button--secondary" href="https://www.meetup.com/allegrotech/events/271201706/">Szczegóły</a></article></div></div></div><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-display_block m-margin-bottom_8 m-width_100 m-height_40 m-line-height_40 m-text-transform_uppercase m-letter-spacing_2 m-white-space_nowrap m-cursor_pointer m-overflow_hidden m-padding-left_16 m-padding-right_16 m-padding-top_0 m-padding-bottom_0 m-outline-style_dotted--focus m-outline-width_2 m-outline-color_teal m-outline-offset_n2 m-button m-box_border m-text-align_center m-display_inline-block m-color_secondary m-background-position_50p m-background-size_5000p m-transition-property_background-color m-transition-duration_fast m-button--secondary" href="https://www.meetup.com/allegrotech/events/">Zobacz więcej wydarzeń</a></div><div class="m-width-max_1600 m-margin-left_auto m-margin-right_auto m-padding-top-24"><h2 class="m-font-family_roboto m-margin-left_0 m-margin-right_0 m-margin-top_0 m-color_text m-font-weight_300 m-font-size_27 m-font-size_36_sm m-margin-bottom_16 m-margin-bottom_24_sm m-line-height_125 m-padding-left-24 m-padding-right-24">Oferty pracy</h2><div class="m-width-max_1600 m-margin-left_auto m-margin-right_auto"><article class="m-padding-top_16 m-padding-top_24_lg m-padding-right_16 m-padding-right_24_lg m-padding-bottom_16 m-padding-bottom_24_lg m-padding-left_16 m-padding-left_24_lg m-card m-margin-bottom_16 m-display-flex m-flex-justify_between m-flex-items-center m-flex-direction_column m-flex-direction_row_md m-color-bg_card"><h2 class="m-font-family_roboto m-margin-left_0 m-margin-right_0 m-margin-top_0 m-color_text m-font-weight_500 m-line-height_130 m-margin-bottom_8 m-margin-bottom_16_sm m-font-size_21 m-font-size_25_sm m-margin-bottom_0_sm m-flex-grow_1">Senior Software Engineer</h2><p class="m-font-family_sans m-line-height_21 m-font-size_14 m-text-size-adjust_100p m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text m-font-size_21 m-font-size_25_sm m-font-weight_300 m-line-height_normal m-margin-top_16 m-text-align_left m-padding-right_16">Kraków</p><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-height_40 m-line-height_40 m-text-transform_uppercase m-letter-spacing_2 m-white-space_nowrap m-cursor_pointer m-overflow_hidden m-padding-left_16 m-padding-right_16 m-padding-top_0 m-padding-bottom_0 m-outline-style_dotted--focus m-outline-width_2 m-outline-color_teal m-outline-offset_n2 m-button m-box_border m-text-align_center m-display_inline-block m-color_secondary m-background-position_50p m-background-size_5000p m-transition-property_background-color m-transition-duration_fast m-button--secondary" href="https://www.smartrecruiters.com/Allegro/743999732620819-senior-software-engineer?trid=de9dfdf3-7f9e-4ebf-8a64-49f6c69ad640">Sprawdź</a></article><article class="m-padding-top_16 m-padding-top_24_lg m-padding-right_16 m-padding-right_24_lg m-padding-bottom_16 m-padding-bottom_24_lg m-padding-left_16 m-padding-left_24_lg m-card m-margin-bottom_16 m-display-flex m-flex-justify_between m-flex-items-center m-flex-direction_column m-flex-direction_row_md m-color-bg_card"><h2 class="m-font-family_roboto m-margin-left_0 m-margin-right_0 m-margin-top_0 m-color_text m-font-weight_500 m-line-height_130 m-margin-bottom_8 m-margin-bottom_16_sm m-font-size_21 m-font-size_25_sm m-margin-bottom_0_sm m-flex-grow_1">Senior Software Engineer</h2><p class="m-font-family_sans m-line-height_21 m-font-size_14 m-text-size-adjust_100p m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text m-font-size_21 m-font-size_25_sm m-font-weight_300 m-line-height_normal m-margin-top_16 m-text-align_left m-padding-right_16">Warszawa</p><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-height_40 m-line-height_40 m-text-transform_uppercase m-letter-spacing_2 m-white-space_nowrap m-cursor_pointer m-overflow_hidden m-padding-left_16 m-padding-right_16 m-padding-top_0 m-padding-bottom_0 m-outline-style_dotted--focus m-outline-width_2 m-outline-color_teal m-outline-offset_n2 m-button m-box_border m-text-align_center m-display_inline-block m-color_secondary m-background-position_50p m-background-size_5000p m-transition-property_background-color m-transition-duration_fast m-button--secondary" href="https://www.smartrecruiters.com/Allegro/743999732620056-senior-software-engineer?trid=de9dfdf3-7f9e-4ebf-8a64-49f6c69ad640">Sprawdź</a></article><article class="m-padding-top_16 m-padding-top_24_lg m-padding-right_16 m-padding-right_24_lg m-padding-bottom_16 m-padding-bottom_24_lg m-padding-left_16 m-padding-left_24_lg m-card m-margin-bottom_16 m-display-flex m-flex-justify_between m-flex-items-center m-flex-direction_column m-flex-direction_row_md m-color-bg_card"><h2 class="m-font-family_roboto m-margin-left_0 m-margin-right_0 m-margin-top_0 m-color_text m-font-weight_500 m-line-height_130 m-margin-bottom_8 m-margin-bottom_16_sm m-font-size_21 m-font-size_25_sm m-margin-bottom_0_sm m-flex-grow_1">Software Engineer (Big Data Team)</h2><p class="m-font-family_sans m-line-height_21 m-font-size_14 m-text-size-adjust_100p m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text m-font-size_21 m-font-size_25_sm m-font-weight_300 m-line-height_normal m-margin-top_16 m-text-align_left m-padding-right_16">Warszawa, Kraków, Poznań, Toruń</p><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-height_40 m-line-height_40 m-text-transform_uppercase m-letter-spacing_2 m-white-space_nowrap m-cursor_pointer m-overflow_hidden m-padding-left_16 m-padding-right_16 m-padding-top_0 m-padding-bottom_0 m-outline-style_dotted--focus m-outline-width_2 m-outline-color_teal m-outline-offset_n2 m-button m-box_border m-text-align_center m-display_inline-block m-color_secondary m-background-position_50p m-background-size_5000p m-transition-property_background-color m-transition-duration_fast m-button--secondary" href="https://www.smartrecruiters.com/Allegro/743999732606435-software-engineer-big-data-team?trid=de9dfdf3-7f9e-4ebf-8a64-49f6c69ad640">Sprawdź</a></article><article class="m-padding-top_16 m-padding-top_24_lg m-padding-right_16 m-padding-right_24_lg m-padding-bottom_16 m-padding-bottom_24_lg m-padding-left_16 m-padding-left_24_lg m-card m-margin-bottom_16 m-display-flex m-flex-justify_between m-flex-items-center m-flex-direction_column m-flex-direction_row_md m-color-bg_card"><h2 class="m-font-family_roboto m-margin-left_0 m-margin-right_0 m-margin-top_0 m-color_text m-font-weight_500 m-line-height_130 m-margin-bottom_8 m-margin-bottom_16_sm m-font-size_21 m-font-size_25_sm m-margin-bottom_0_sm m-flex-grow_1">Consumer Strategy Team Leader</h2><p class="m-font-family_sans m-line-height_21 m-font-size_14 m-text-size-adjust_100p m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text m-font-size_21 m-font-size_25_sm m-font-weight_300 m-line-height_normal m-margin-top_16 m-text-align_left m-padding-right_16">Warszawa, Poznań</p><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-height_40 m-line-height_40 m-text-transform_uppercase m-letter-spacing_2 m-white-space_nowrap m-cursor_pointer m-overflow_hidden m-padding-left_16 m-padding-right_16 m-padding-top_0 m-padding-bottom_0 m-outline-style_dotted--focus m-outline-width_2 m-outline-color_teal m-outline-offset_n2 m-button m-box_border m-text-align_center m-display_inline-block m-color_secondary m-background-position_50p m-background-size_5000p m-transition-property_background-color m-transition-duration_fast m-button--secondary" href="https://www.smartrecruiters.com/Allegro/743999732498611-consumer-strategy-team-leader?trid=de9dfdf3-7f9e-4ebf-8a64-49f6c69ad640">Sprawdź</a></article><article class="m-padding-top_16 m-padding-top_24_lg m-padding-right_16 m-padding-right_24_lg m-padding-bottom_16 m-padding-bottom_24_lg m-padding-left_16 m-padding-left_24_lg m-card m-margin-bottom_16 m-display-flex m-flex-justify_between m-flex-items-center m-flex-direction_column m-flex-direction_row_md m-color-bg_card"><h2 class="m-font-family_roboto m-margin-left_0 m-margin-right_0 m-margin-top_0 m-color_text m-font-weight_500 m-line-height_130 m-margin-bottom_8 m-margin-bottom_16_sm m-font-size_21 m-font-size_25_sm m-margin-bottom_0_sm m-flex-grow_1">Big Data Engineer</h2><p class="m-font-family_sans m-line-height_21 m-font-size_14 m-text-size-adjust_100p m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text m-font-size_21 m-font-size_25_sm m-font-weight_300 m-line-height_normal m-margin-top_16 m-text-align_left m-padding-right_16">Poznań, Warszawa, Kraków, Toruń</p><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-height_40 m-line-height_40 m-text-transform_uppercase m-letter-spacing_2 m-white-space_nowrap m-cursor_pointer m-overflow_hidden m-padding-left_16 m-padding-right_16 m-padding-top_0 m-padding-bottom_0 m-outline-style_dotted--focus m-outline-width_2 m-outline-color_teal m-outline-offset_n2 m-button m-box_border m-text-align_center m-display_inline-block m-color_secondary m-background-position_50p m-background-size_5000p m-transition-property_background-color m-transition-duration_fast m-button--secondary" href="https://www.smartrecruiters.com/Allegro/743999732293504-big-data-engineer?trid=de9dfdf3-7f9e-4ebf-8a64-49f6c69ad640">Sprawdź</a></article></div><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-display_block m-margin-bottom_8 m-width_100 m-height_40 m-line-height_40 m-text-transform_uppercase m-letter-spacing_2 m-white-space_nowrap m-cursor_pointer m-overflow_hidden m-padding-left_16 m-padding-right_16 m-padding-top_0 m-padding-bottom_0 m-outline-style_dotted--focus m-outline-width_2 m-outline-color_teal m-outline-offset_n2 m-button m-box_border m-text-align_center m-display_inline-block m-color_secondary m-background-position_50p m-background-size_5000p m-transition-property_background-color m-transition-duration_fast m-button--secondary" href="https://allegro.pl/praca">Zobacz więcej ofert</a></div><footer class="m-color-bg_navy m-margin-top-32"><div class="m-width-max_1600 m-margin-left_auto m-margin-right_auto m-padding-top-24 m-padding-bottom-24 m-display-flex@sm m-flex-justify-between m-flex-items-center m-text-align_center"><p class="m-font-family_sans m-line-height_21 m-font-size_14 m-text-size-adjust_100p m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text m-color_white m-padding-left-24@sm">Proudly built by Allegro Tech engineers</p><ul class="m-font-family_sans m-font-size_14 m-line-height_21 m-color_text m-text-size-adjust_100p m-list-style-type_none m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-padding-top_0 m-padding-right_0 m-padding-bottom_0 m-padding-left_0 m-display-flex m-flex-justify-center"><li style="filter:brightness(0) invert(1);opacity:0.5" class="m-margin-right-8 m-margin-top-16 m-margin-top-0@sm m-color_white"><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-line-height_21" href="https://github.com/allegro"><img src="https://assets.allegrostatic.com/metrum/icon/github-6a18df1729.svg" alt="Github" class="m-icon"/></a></li><li style="filter:brightness(0) invert(1);opacity:0.5" class="m-margin-right-8 m-margin-top-16 m-margin-top-0@sm m-color_white"><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-line-height_21" href="https://www.facebook.com/allegro.tech/"><img src="https://assets.allegrostatic.com/metrum/icon/facebook-a2b92f9dcb.svg" alt="Facebook" class="m-icon"/></a></li><li style="filter:brightness(0) invert(1);opacity:0.5" class="m-margin-right-8 m-margin-top-16 m-margin-top-0@sm m-color_white"><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-line-height_21" href="https://twitter.com/allegrotech"><img src="https://assets.allegrostatic.com/metrum/icon/twitter-25164a58aa.svg" alt="Twitter" class="m-icon"/></a></li></ul></div></footer><div style="visibility:hidden;height:0;overflow:hidden;position:relative"><img alt="doubleclick" width="1" height="1" style="position:absolute" src="https://pubads.g.doubleclick.net/activity;dc_iu=/21612525419/DFPAudiencePixel;ord=6390948650556.645;dc_seg=507368552?"/><img alt="fb" height="1" width="1" style="position:absolute" src="https://www.facebook.com/tr?id=1650870088530325&amp;ev=PageView&amp;noscript=1"/></div></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"posts":[{"title":"Impact of data model on MongoDB database size","link":"https://blog.allegro.tech/2021/01/impact-of-the-data-model-on-the-MongoDB-database-size.html","pubDate":"Thu, 14 Jan 2021 00:00:00 +0100","authors":{"author":[{"name":["Michał Knasiecki"],"photo":["https://blog.allegro.tech/img/authors/michal.knasiecki.jpg"],"url":["https://blog.allegro.tech/authors/michal.knasiecki"]}]},"content":"\u003cp\u003eSo I was tuning one of our services in order to speed up some MongoDB queries. Incidentally, my attention was caught by\nthe size of one of the collections that contains archived objects and therefore is rarely used. Unfortunately I wasn’t\nable to reduce the size of the documents stored there, but I started to wonder: is it possible to store the same data\nin a more compact way? Mongo stores \u003ccode class=\"language-plaintext highlighter-rouge\"\u003eJSON\u003c/code\u003e that allows many different ways of expressing similar data, so there seems\nto be room for improvements.\u003c/p\u003e\n\n\u003cp\u003eIt is worth asking: why make such an effort in the era of Big Data and unlimited resources? There are several reasons.\u003c/p\u003e\n\n\u003cp\u003eFirst of all, the resources are not unlimited and at the end we have physical drives that cost money to buy, replace,\nmaintain, and be supplied with power.\u003c/p\u003e\n\n\u003cp\u003eSecondly, less stored data results in less time to access it. Moreover, less data means that more of it will fit into\ncache, so the next data access will be an order of magnitude faster.\u003c/p\u003e\n\n\u003cp\u003eI decided to do some experiments and check how the model design affects the size of database files.\u003c/p\u003e\n\n\u003cp\u003eI used a local MongoDB Community Edition 4.4 installation and I initially tested collections containing 1\nmillion and 10 million documents. One of the variants contained up to 100 million, but the results were proportional\n(nearly linear). Therefore, in the end I decided to stop at 1M collections, because loading the data was simply much faster.\u003c/p\u003e\n\n\u003cp\u003eHaving access to local database files, I could easily check the size of the files storing individual collections.\nHowever, it turned out to be unnecessary, because the same data can be obtained with the command:\u003c/p\u003e\n\n\u003cdiv class=\"language-sql highlighter-rouge\"\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre class=\"highlight\"\u003e\u003ccode\u003e\u003cspan class=\"n\"\u003edb\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003egetCollection\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s1\"\u003e'COLLECTION_NAME'\u003c/span\u003e\u003cspan class=\"p\"\u003e).\u003c/span\u003e\u003cspan class=\"n\"\u003estats\u003c/span\u003e\u003cspan class=\"p\"\u003e()\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\n\n\u003cp\u003e\u003cimg src=\"/img/articles/2021-01-14-impact-of-the-data-model-on-the-MongoDB-database-size/collection-stats.png\" alt=\"Collection stats\" /\u003e\u003c/p\u003e\n\n\u003cp\u003eThe following fields are expressed in bytes:\u003c/p\u003e\n\n\u003cul\u003e\n  \u003cli\u003e\u003ccode class=\"language-plaintext highlighter-rouge\"\u003esize\u003c/code\u003e: size of all collection documents, before compression,\u003c/li\u003e\n  \u003cli\u003e\u003ccode class=\"language-plaintext highlighter-rouge\"\u003eavgObjSize\u003c/code\u003e: average document size, before compression,\u003c/li\u003e\n  \u003cli\u003e\u003ccode class=\"language-plaintext highlighter-rouge\"\u003estorageSize\u003c/code\u003e: file size on the disk; this is the value after the data is compressed, the same value is returned by\n\u003ccode class=\"language-plaintext highlighter-rouge\"\u003els\u003c/code\u003e command,\u003c/li\u003e\n  \u003cli\u003e\u003ccode class=\"language-plaintext highlighter-rouge\"\u003efreeStorageSize\u003c/code\u003e: size of unused space allocated for the data; Mongo does not increase the file size byte-by-byte,\nbut allocates a percentage of the current file size and this value indicates how much data will still fit into the file.\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003eTo present the results I used (storageSize - freeStorageSize) value that indicates the actual space occupied by the data.\u003c/p\u003e\n\n\u003cp\u003eMy local MongoDB instance had compression enabled. Not every storage engine has this option enabled by default, so when\nyou start your own analysis it is worth checking how it is in your particular case.\u003c/p\u003e\n\n\u003ch3 id=\"id-field-type\"\u003eId field type\u003c/h3\u003e\n\n\u003cp\u003eIn the beginning I decided to check \u003ccode class=\"language-plaintext highlighter-rouge\"\u003eID\u003c/code\u003e fields. Not the collection primary key, mind you – it is ‘ObjectId’ type by\ndefault and generally shouldn’t be changed. I decided to focus on user\nand offers identifiers which, although numerical, are often saved as String in Mongo. I believe it comes partly due to\nthe contract of our services - identifiers in \u003ccode class=\"language-plaintext highlighter-rouge\"\u003eJSON\u003c/code\u003e most often come as Strings and in this form they are later stored\nin our databases.\u003c/p\u003e\n\n\u003cp\u003eLet’s start with some theory: the number of \u003ccode class=\"language-plaintext highlighter-rouge\"\u003eint32\u003c/code\u003e type in Mongo has a fixed size of 4 bytes. The same number\nwritten as a \u003ccode class=\"language-plaintext highlighter-rouge\"\u003eString\u003c/code\u003e of characters has a size dependent on the number of digits; additionally it contains the length of\nthe text (4 bytes) and a terminator character. For example, the text “0” is 12 bytes long and “1234567890” is 25 bytes\nlong. So in theory we should get interesting results, but what does it look like in reality?\u003c/p\u003e\n\n\u003cp\u003eI prepared 2 collections, one million documents each, containing the following documents:\u003c/p\u003e\n\n\u003cdiv class=\"language-json highlighter-rouge\"\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre class=\"highlight\"\u003e\u003ccode\u003e\u003cspan class=\"p\"\u003e{\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"nl\"\u003e\"_id\"\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"mi\"\u003e1\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"p\"\u003e}\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\n\n\u003cp\u003eand\u003c/p\u003e\n\n\u003cdiv class=\"language-json highlighter-rouge\"\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre class=\"highlight\"\u003e\u003ccode\u003e\u003cspan class=\"p\"\u003e{\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"nl\"\u003e\"_id\"\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"s2\"\u003e\"1\"\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"p\"\u003e}\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\n\n\u003cp\u003eThe values of identifiers were consecutive natural numbers. Here is the comparison of results:\u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"/img/articles/2021-01-14-impact-of-the-data-model-on-the-MongoDB-database-size/chart-id.png\" alt=\"String vs int32 size\" /\u003e\u003c/p\u003e\n\n\u003cp\u003eAs you can see the difference is significant since the size on disk decreased by half. Additionally, it is worth\nnoting here that my data is synthetic and the identifiers start from 1. The advantage of a numerical identifier over a\n\u003ccode class=\"language-plaintext highlighter-rouge\"\u003eString\u003c/code\u003e increases the more digits a number has, so benefits should be even better for the real life data.\u003c/p\u003e\n\n\u003cp\u003eEncouraged by the success I decided to check if field type had any influence on the size of an index created on it. In\nthis case, unfortunately, I got disappointed: the sizes were similar. This is due to the fact that MongoDB\nuses a hash function when creating indexes, so physically both indexes are composed of numerical values.\nHowever, if we are dealing with hashing, maybe at least searching by index in a numerical field is faster?\u003c/p\u003e\n\n\u003cp\u003eI made a comparison of searching for a million and 10 million documents by indexed key in a random but the same order\nfor both collections. Again, a missed shot: both tests ended up with a similar result, so the conclusion is this:\nit is worth using numerical identifiers, because they require less disk space, but we will not get additional benefits\nassociated with indexes on these fields.\u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"/img/articles/2021-01-14-impact-of-the-data-model-on-the-MongoDB-database-size/chart-search-1M.png\" alt=\"String vs int32 search time 1M\" /\u003e\u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"/img/articles/2021-01-14-impact-of-the-data-model-on-the-MongoDB-database-size/chart-search-10M.png\" alt=\"String vs int32 search time 10M\" /\u003e\u003c/p\u003e\n\n\u003ch3 id=\"simple-and-complex-structures\"\u003eSimple and complex structures\u003c/h3\u003e\n\n\u003cp\u003eLet’s move on to more complex data. Our models are often built from smaller structures grouping data such as user,\norder or offer. I decided to compare the size of documents storing such structures and their flat counterparts:\u003c/p\u003e\n\n\u003cdiv class=\"language-json highlighter-rouge\"\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre class=\"highlight\"\u003e\u003ccode\u003e\u003cspan class=\"p\"\u003e{\u003c/span\u003e\u003cspan class=\"nl\"\u003e\"_id\"\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"mi\"\u003e1\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"nl\"\u003e\"user\"\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"p\"\u003e{\u003c/span\u003e\u003cspan class=\"nl\"\u003e\"name\"\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"s2\"\u003e\"Jan\"\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"nl\"\u003e\"surname\"\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"s2\"\u003e\"Nowak\"\u003c/span\u003e\u003cspan class=\"p\"\u003e}}\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\n\n\u003cp\u003eand\u003c/p\u003e\n\n\u003cdiv class=\"language-json highlighter-rouge\"\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre class=\"highlight\"\u003e\u003ccode\u003e\u003cspan class=\"p\"\u003e{\u003c/span\u003e\u003cspan class=\"nl\"\u003e\"_id\"\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"mi\"\u003e1\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"nl\"\u003e\"userName\"\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"s2\"\u003e\"Jan\"\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"nl\"\u003e\"userSurname\"\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"s2\"\u003e\"Nowak\"\u003c/span\u003e\u003cspan class=\"p\"\u003e}\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\n\n\u003cp\u003eIn both cases we store identical data, the documents differ only in the schema. Take a look at the result:\u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"/img/articles/2021-01-14-impact-of-the-data-model-on-the-MongoDB-database-size/chart-struct-1.png\" alt=\"complex vs simple size\" /\u003e\u003c/p\u003e\n\n\u003cp\u003eThere is a slight reduction by 0.4MB. It may seem not much compared to the effect we achieved for the field\ncontaining an ID. However, we have to bear in mind that in this case we were dealing with a more complex document. It\ncontained – in addition to the compared fields – a numerical type identifier that, as we remember from previous\nexperiments, takes up about 5MB of disk space.\u003c/p\u003e\n\n\u003cp\u003eTaking this into account in the above results we are talking about a decrease from 3.4MB to 3MB. It actually looks\nbetter as percentage - we saved 12% of the space needed to store personal data.\u003c/p\u003e\n\n\u003cp\u003eLet’s go back to the discussed documents for a moment:\u003c/p\u003e\n\n\u003cdiv class=\"language-json highlighter-rouge\"\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre class=\"highlight\"\u003e\u003ccode\u003e\u003cspan class=\"p\"\u003e{\u003c/span\u003e\u003cspan class=\"nl\"\u003e\"_id\"\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"mi\"\u003e1\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"nl\"\u003e\"user\"\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"p\"\u003e{\u003c/span\u003e\u003cspan class=\"nl\"\u003e\"name\"\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"s2\"\u003e\"Jan\"\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"nl\"\u003e\"surname\"\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"s2\"\u003e\"Nowak\"\u003c/span\u003e\u003cspan class=\"p\"\u003e}}\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\n\n\u003cp\u003eand\u003c/p\u003e\n\n\u003cdiv class=\"language-json highlighter-rouge\"\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre class=\"highlight\"\u003e\u003ccode\u003e\u003cspan class=\"p\"\u003e{\u003c/span\u003e\u003cspan class=\"nl\"\u003e\"_id\"\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"mi\"\u003e1\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"nl\"\u003e\"userName\"\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"s2\"\u003e\"Jan\"\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"nl\"\u003e\"userSurname\"\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"s2\"\u003e\"Nowak\"\u003c/span\u003e\u003cspan class=\"p\"\u003e}\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\n\n\u003cp\u003eA watchful eye will notice that I used longer field names in the document after flattening. So instead of \u003ccode class=\"language-plaintext highlighter-rouge\"\u003euser.name\u003c/code\u003e\nand \u003ccode class=\"language-plaintext highlighter-rouge\"\u003euser.surname\u003c/code\u003e I made \u003ccode class=\"language-plaintext highlighter-rouge\"\u003euserName\u003c/code\u003e and \u003ccode class=\"language-plaintext highlighter-rouge\"\u003euserSurname\u003c/code\u003e. I did it automatically, a bit unconsciously, to make the\nresulting \u003ccode class=\"language-plaintext highlighter-rouge\"\u003eJSON\u003c/code\u003e more readable.  However, if by changing only the schema of the document from compound to flat we managed\nto reduce the size of the data, maybe it is worth to go a step further and shorten the field names?\u003c/p\u003e\n\n\u003cp\u003eI decided to add a third document for comparison, flattened and with shorter field names:\u003c/p\u003e\n\n\u003cdiv class=\"language-json highlighter-rouge\"\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre class=\"highlight\"\u003e\u003ccode\u003e\u003cspan class=\"p\"\u003e{\u003c/span\u003e\u003cspan class=\"nl\"\u003e\"_id\"\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"mi\"\u003e1\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"nl\"\u003e\"name\"\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"s2\"\u003e\"Jan\"\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"nl\"\u003e\"surname\"\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"s2\"\u003e\"Nowak\"\u003c/span\u003e\u003cspan class=\"p\"\u003e}\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\n\n\u003cp\u003eThe results are shown in the chart below:\u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"/img/articles/2021-01-14-impact-of-the-data-model-on-the-MongoDB-database-size/chart-struct-2.png\" alt=\"complex vs simple vs short size\" /\u003e\u003c/p\u003e\n\n\u003cp\u003eThe result is even better than just flattening. Apart from the document’s key size, we achieved a decrease from\n3.4MB to 2MB. Why does this happen even though we store exactly the same data?\u003c/p\u003e\n\n\u003cp\u003eThe reason for the decrease is the nature of NoSQL databases that, unlike the relational ones, do not have a schema\ndefined at the level of the entire collection. If someone is very stubborn, they can store user data, offers, orders and\npayments in one collection. It would still be possible to read, index and search that data. This is because the\ndatabase, in addition to the data itself, stores its schema with each document. Thus, the total size of a document\nconsists of its data and its schema. And that explains the whole puzzle. By reducing the size of the schema, we also\nreduce the size of each document, i.e. the size of the final file with the collection data. It is worth taking this into\naccount when designing a collection schema in order not to blow it up too much. Of course, you cannot go to extremes, because\nthat would lead us to fields named \u003ccode class=\"language-plaintext highlighter-rouge\"\u003ea\u003c/code\u003e, \u003ccode class=\"language-plaintext highlighter-rouge\"\u003eb\u003c/code\u003e and \u003ccode class=\"language-plaintext highlighter-rouge\"\u003ec\u003c/code\u003e, what would make the collection completely illegible for a human.\u003c/p\u003e\n\n\u003cp\u003eFor very large collections, however, this approach is used, an example of which is the MongoDB operation log\nthat contains fields called:\u003c/p\u003e\n\n\u003cul\u003e\n  \u003cli\u003eh\u003c/li\u003e\n  \u003cli\u003eop\u003c/li\u003e\n  \u003cli\u003ens\u003c/li\u003e\n  \u003cli\u003eo\u003c/li\u003e\n  \u003cli\u003eo2\u003c/li\u003e\n  \u003cli\u003eb\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003ch3 id=\"empty-fields\"\u003eEmpty fields\u003c/h3\u003e\n\n\u003cp\u003eSince we are at the document’s schema, it is still worth looking at the problem of blank fields. In the case of\n\u003ccode class=\"language-plaintext highlighter-rouge\"\u003eJSON\u003c/code\u003e the lack of value in a certain field can be written in two ways, either directly - by writing null in its value -\nor by not serializing the field at all. I prepared a comparison of documents with the following structure:\u003c/p\u003e\n\n\u003cdiv class=\"language-json highlighter-rouge\"\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre class=\"highlight\"\u003e\u003ccode\u003e\u003cspan class=\"p\"\u003e{\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"nl\"\u003e\"id\"\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"mi\"\u003e1\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"p\"\u003e}\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\n\n\u003cp\u003eand\u003c/p\u003e\n\n\u003cdiv class=\"language-json highlighter-rouge\"\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre class=\"highlight\"\u003e\u003ccode\u003e\u003cspan class=\"p\"\u003e{\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"nl\"\u003e\"id\"\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"mi\"\u003e1\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"nl\"\u003e\"phone\"\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"kc\"\u003enull\u003c/span\u003e\u003cspan class=\"p\"\u003e}\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\n\n\u003cp\u003eThe meaning of the data in both documents is identical - the user has no phone number. However, the schema of the second\ndocument is different from the first one because it contains two fields.\u003c/p\u003e\n\n\u003cp\u003eHere are the results:\u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"/img/articles/2021-01-14-impact-of-the-data-model-on-the-MongoDB-database-size/chart-null.png\" alt=\"null vs empty size\" /\u003e\u003c/p\u003e\n\n\u003cp\u003eThe results are quite surprising: saving a million null’s is quite expensive as it takes more than 1MB on a disk.\u003c/p\u003e\n\n\u003ch3 id=\"enumerated-types\"\u003eEnumerated types\u003c/h3\u003e\n\n\u003cp\u003eNow, let’s also look at the enumerated types. You can store them in two ways, either by using a label:\u003c/p\u003e\n\n\u003cdiv class=\"language-json highlighter-rouge\"\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre class=\"highlight\"\u003e\u003ccode\u003e\u003cspan class=\"p\"\u003e{\u003c/span\u003e\u003cspan class=\"nl\"\u003e\"_id\"\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"mi\"\u003e1\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"nl\"\u003e\"source\"\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"s2\"\u003e\"ALLEGRO\"\u003c/span\u003e\u003cspan class=\"p\"\u003e}\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\n\n\u003cp\u003eor by ordinal value:\u003c/p\u003e\n\n\u003cdiv class=\"language-json highlighter-rouge\"\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre class=\"highlight\"\u003e\u003ccode\u003e\u003cspan class=\"p\"\u003e{\u003c/span\u003e\u003cspan class=\"nl\"\u003e\"_id\"\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"mi\"\u003e1\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"nl\"\u003e\"source\"\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"mi\"\u003e1\u003c/span\u003e\u003cspan class=\"p\"\u003e}\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\n\n\u003cp\u003eHere an analogy with the first experiment can be found: again we replace a character string with a numerical value.\nSince we got a great result the first time, maybe we could repeat it here?\u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"/img/articles/2021-01-14-impact-of-the-data-model-on-the-MongoDB-database-size/chart-enums-1.png\" alt=\"label vs index size\" /\u003e\u003c/p\u003e\n\n\u003cp\u003eUnfortunately, the result is disappointing and the reduction in size is small. However, if we think more deeply, we will\ncome to the conclusion that it could not have been otherwise. The enumerated types are not unique identifiers. We are\ndealing only with a few possible values, appearing many times in the whole collection, so it’s a great chance for\nMongoDB data compression to prove its worth. Most likely the values from the first collection have been automatically replaced by the\nengine to its corresponding ordinal values. Additionally, we don’t have any profit on the schema here, because they are\nthe same in both cases. The situation might be different for collections that do not have data compression enabled.\u003c/p\u003e\n\n\u003cp\u003eThis is a good time to take a closer look at how \u003ca href=\"https://www.mongodb.com/blog/post/new-compression-options-mongodb-30\"\u003esnappy\u003c/a\u003e\ncompression works in MongoDB. I’ve prepared two more collections, with identical data, but with data compression\nturned off. The results are shown in the chart below, compiled together with the collections with data compression turned on.\u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"/img/articles/2021-01-14-impact-of-the-data-model-on-the-MongoDB-database-size/chart-enums-2.png\" alt=\"snappy vs plain size\" /\u003e\u003c/p\u003e\n\n\u003cp\u003eIt is clear that the use of an ordinal value instead of a label of the enumerated type brings considerable profit for\ncollections with data compression disabled. In case of lack of compression it is definitely worth considering using numerical\nvalues.\u003c/p\u003e\n\n\u003ch3 id=\"useless-_class-field\"\u003eUseless _class field\u003c/h3\u003e\n\n\u003cp\u003eIf we use \u003ccode class=\"language-plaintext highlighter-rouge\"\u003espring-data\u003c/code\u003e, each of our collections additionally contains a \u003ccode class=\"language-plaintext highlighter-rouge\"\u003e_class\u003c/code\u003e field storing the package and the\nname of the entity class containing the mapping for documents from this collection. This mechanism is used to support\ninheritance of model classes, that is not widely used. In most cases this field stores exactly the same values for each\ndocument what makes it useless. And how much does it cost? Let’s compare the following documents:\u003c/p\u003e\n\n\u003cdiv class=\"language-json highlighter-rouge\"\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre class=\"highlight\"\u003e\u003ccode\u003e\u003cspan class=\"p\"\u003e{\u003c/span\u003e\u003cspan class=\"nl\"\u003e\"_id\"\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"mi\"\u003e1\u003c/span\u003e\u003cspan class=\"p\"\u003e}\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\n\n\u003cp\u003eand\u003c/p\u003e\n\n\u003cdiv class=\"language-json highlighter-rouge\"\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre class=\"highlight\"\u003e\u003ccode\u003e\u003cspan class=\"p\"\u003e{\u003c/span\u003e\u003cspan class=\"nl\"\u003e\"_id\"\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"mi\"\u003e1\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"nl\"\u003e\"_class\"\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"s2\"\u003e\"pl.allegro.some.project.domain.sample\"\u003c/span\u003e\u003cspan class=\"p\"\u003e}\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\n\n\u003cp\u003e\u003cimg src=\"/img/articles/2021-01-14-impact-of-the-data-model-on-the-MongoDB-database-size/chart-class.png\" alt=\"_class field size\" /\u003e\u003c/p\u003e\n\n\u003cp\u003eThe difference is considerable, over 50%. Bearing in mind that the compression is enabled, I believe that the impact of\nthe data itself is small and the result is caused by the schema of the collection containing only the key being half as small\nas the one with the key (1 field vs 2 fields). After removal of the \u003ccode class=\"language-plaintext highlighter-rouge\"\u003e_class\u003c/code\u003e field from a document with more fields,\nthe difference expressed in percent will be much smaller of course. However, storing useless data does not make sense.\u003c/p\u003e\n\n\u003ch3 id=\"useless-indexes\"\u003eUseless indexes\u003c/h3\u003e\n\n\u003cp\u003eWhen investigating the case with identifiers stored as strings, I checked whether manipulating the data type could reduce\nthe index size. Unfortunately I did not succeed, so I decided to focus on the problem of redundant indexes.\u003c/p\u003e\n\n\u003cp\u003eIt is usually a good practice to cover each query with an index so that the number of \u003ccode class=\"language-plaintext highlighter-rouge\"\u003ecollscan\u003c/code\u003e operations is as small as\npossible. This often leads to situations where we have too many indexes. We add more queries, create new indexes for\nthem, and often it turns out that this newly created index takes over the queries of another one.\nAs a result, we have to maintain many unnecessary indexes, wasting disk space and CPU time.\u003c/p\u003e\n\n\u003cp\u003eFortunately, it is possible to check the number of index uses with the query:\u003c/p\u003e\n\n\u003cdiv class=\"language-sql highlighter-rouge\"\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre class=\"highlight\"\u003e\u003ccode\u003e\u003cspan class=\"n\"\u003edb\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eCOLLECTION_NAME\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"k\"\u003eaggregate\u003c/span\u003e\u003cspan class=\"p\"\u003e([\u003c/span\u003e\u003cspan class=\"err\"\u003e{$\u003c/span\u003e\u003cspan class=\"n\"\u003eindexStats\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\u003cspan class=\"err\"\u003e{}}\u003c/span\u003e\u003cspan class=\"p\"\u003e])\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\n\n\u003cp\u003eIt allows you to find indexes that are not used so you can safely delete them.\u003c/p\u003e\n\n\u003cp\u003eAnd finally, one more thing. Indexes can be removed quickly, but they take much longer to rebuild. This means that the\nconsequences of removal of an index by mistake can be severe. Therefore it is better to make sure that the index to be\ndeleted is definitely not used by any query. The latest MongoDB 4.4 provides a command that helps to avoid errors:\u003c/p\u003e\n\n\u003cdiv class=\"language-sql highlighter-rouge\"\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre class=\"highlight\"\u003e\u003ccode\u003e\u003cspan class=\"n\"\u003edb\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eCOLLECTION_NAME\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003ehideIndex\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"o\"\u003e\u0026lt;\u003c/span\u003e\u003cspan class=\"k\"\u003eindex\u003c/span\u003e\u003cspan class=\"o\"\u003e\u0026gt;\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\n\n\u003cp\u003eThe above-mentioned command hides the index from the query optimizer. It does not take this index into account when\nbuilding the query execution plan, but the index is still updated when modifying documents. Thanks to that, if it turns\nout that it was needed, we are able to restore it immediately and it will still be up-to-date.\u003c/p\u003e\n\n\u003ch3 id=\"conclusion\"\u003eConclusion\u003c/h3\u003e\n\n\u003cp\u003eUsing a few simple techniques, preparing several versions of the schema and using stats() command we are able to design\na model which does not overload our infrastructure. I encourage you to experiment with your own databases. Maybe you too\ncan save some disk space and CPU time.\u003c/p\u003e\n","contentSnippet":"So I was tuning one of our services in order to speed up some MongoDB queries. Incidentally, my attention was caught by\nthe size of one of the collections that contains archived objects and therefore is rarely used. Unfortunately I wasn’t\nable to reduce the size of the documents stored there, but I started to wonder: is it possible to store the same data\nin a more compact way? Mongo stores JSON that allows many different ways of expressing similar data, so there seems\nto be room for improvements.\nIt is worth asking: why make such an effort in the era of Big Data and unlimited resources? There are several reasons.\nFirst of all, the resources are not unlimited and at the end we have physical drives that cost money to buy, replace,\nmaintain, and be supplied with power.\nSecondly, less stored data results in less time to access it. Moreover, less data means that more of it will fit into\ncache, so the next data access will be an order of magnitude faster.\nI decided to do some experiments and check how the model design affects the size of database files.\nI used a local MongoDB Community Edition 4.4 installation and I initially tested collections containing 1\nmillion and 10 million documents. One of the variants contained up to 100 million, but the results were proportional\n(nearly linear). Therefore, in the end I decided to stop at 1M collections, because loading the data was simply much faster.\nHaving access to local database files, I could easily check the size of the files storing individual collections.\nHowever, it turned out to be unnecessary, because the same data can be obtained with the command:\n\ndb.getCollection('COLLECTION_NAME').stats()\n\n\n\nThe following fields are expressed in bytes:\nsize: size of all collection documents, before compression,\navgObjSize: average document size, before compression,\nstorageSize: file size on the disk; this is the value after the data is compressed, the same value is returned by\nls command,\nfreeStorageSize: size of unused space allocated for the data; Mongo does not increase the file size byte-by-byte,\nbut allocates a percentage of the current file size and this value indicates how much data will still fit into the file.\nTo present the results I used (storageSize - freeStorageSize) value that indicates the actual space occupied by the data.\nMy local MongoDB instance had compression enabled. Not every storage engine has this option enabled by default, so when\nyou start your own analysis it is worth checking how it is in your particular case.\nId field type\nIn the beginning I decided to check ID fields. Not the collection primary key, mind you – it is ‘ObjectId’ type by\ndefault and generally shouldn’t be changed. I decided to focus on user\nand offers identifiers which, although numerical, are often saved as String in Mongo. I believe it comes partly due to\nthe contract of our services - identifiers in JSON most often come as Strings and in this form they are later stored\nin our databases.\nLet’s start with some theory: the number of int32 type in Mongo has a fixed size of 4 bytes. The same number\nwritten as a String of characters has a size dependent on the number of digits; additionally it contains the length of\nthe text (4 bytes) and a terminator character. For example, the text “0” is 12 bytes long and “1234567890” is 25 bytes\nlong. So in theory we should get interesting results, but what does it look like in reality?\nI prepared 2 collections, one million documents each, containing the following documents:\n\n{ \"_id\" : 1 }\n\n\nand\n\n{ \"_id\" : \"1\" }\n\n\nThe values of identifiers were consecutive natural numbers. Here is the comparison of results:\n\nAs you can see the difference is significant since the size on disk decreased by half. Additionally, it is worth\nnoting here that my data is synthetic and the identifiers start from 1. The advantage of a numerical identifier over a\nString increases the more digits a number has, so benefits should be even better for the real life data.\nEncouraged by the success I decided to check if field type had any influence on the size of an index created on it. In\nthis case, unfortunately, I got disappointed: the sizes were similar. This is due to the fact that MongoDB\nuses a hash function when creating indexes, so physically both indexes are composed of numerical values.\nHowever, if we are dealing with hashing, maybe at least searching by index in a numerical field is faster?\nI made a comparison of searching for a million and 10 million documents by indexed key in a random but the same order\nfor both collections. Again, a missed shot: both tests ended up with a similar result, so the conclusion is this:\nit is worth using numerical identifiers, because they require less disk space, but we will not get additional benefits\nassociated with indexes on these fields.\n\n\nSimple and complex structures\nLet’s move on to more complex data. Our models are often built from smaller structures grouping data such as user,\norder or offer. I decided to compare the size of documents storing such structures and their flat counterparts:\n\n{\"_id\": 1, \"user\": {\"name\": \"Jan\", \"surname\": \"Nowak\"}}\n\n\nand\n\n{\"_id\": 1, \"userName\": \"Jan\", \"userSurname\": \"Nowak\"}\n\n\nIn both cases we store identical data, the documents differ only in the schema. Take a look at the result:\n\nThere is a slight reduction by 0.4MB. It may seem not much compared to the effect we achieved for the field\ncontaining an ID. However, we have to bear in mind that in this case we were dealing with a more complex document. It\ncontained – in addition to the compared fields – a numerical type identifier that, as we remember from previous\nexperiments, takes up about 5MB of disk space.\nTaking this into account in the above results we are talking about a decrease from 3.4MB to 3MB. It actually looks\nbetter as percentage - we saved 12% of the space needed to store personal data.\nLet’s go back to the discussed documents for a moment:\n\n{\"_id\": 1, \"user\": {\"name\": \"Jan\", \"surname\": \"Nowak\"}}\n\n\nand\n\n{\"_id\": 1, \"userName\": \"Jan\", \"userSurname\": \"Nowak\"}\n\n\nA watchful eye will notice that I used longer field names in the document after flattening. So instead of user.name\nand user.surname I made userName and userSurname. I did it automatically, a bit unconsciously, to make the\nresulting JSON more readable.  However, if by changing only the schema of the document from compound to flat we managed\nto reduce the size of the data, maybe it is worth to go a step further and shorten the field names?\nI decided to add a third document for comparison, flattened and with shorter field names:\n\n{\"_id\": 1, \"name\": \"Jan\", \"surname\": \"Nowak\"}\n\n\nThe results are shown in the chart below:\n\nThe result is even better than just flattening. Apart from the document’s key size, we achieved a decrease from\n3.4MB to 2MB. Why does this happen even though we store exactly the same data?\nThe reason for the decrease is the nature of NoSQL databases that, unlike the relational ones, do not have a schema\ndefined at the level of the entire collection. If someone is very stubborn, they can store user data, offers, orders and\npayments in one collection. It would still be possible to read, index and search that data. This is because the\ndatabase, in addition to the data itself, stores its schema with each document. Thus, the total size of a document\nconsists of its data and its schema. And that explains the whole puzzle. By reducing the size of the schema, we also\nreduce the size of each document, i.e. the size of the final file with the collection data. It is worth taking this into\naccount when designing a collection schema in order not to blow it up too much. Of course, you cannot go to extremes, because\nthat would lead us to fields named a, b and c, what would make the collection completely illegible for a human.\nFor very large collections, however, this approach is used, an example of which is the MongoDB operation log\nthat contains fields called:\nh\n  \nop\nns\no\n  \no2\nb\n\n\nEmpty fields\nSince we are at the document’s schema, it is still worth looking at the problem of blank fields. In the case of\nJSON the lack of value in a certain field can be written in two ways, either directly - by writing null in its value -\nor by not serializing the field at all. I prepared a comparison of documents with the following structure:\n\n{ \"id\" : 1 }\n\n\nand\n\n{ \"id\" : 1, \"phone\" : null}\n\n\nThe meaning of the data in both documents is identical - the user has no phone number. However, the schema of the second\ndocument is different from the first one because it contains two fields.\nHere are the results:\n\nThe results are quite surprising: saving a million null’s is quite expensive as it takes more than 1MB on a disk.\nEnumerated types\nNow, let’s also look at the enumerated types. You can store them in two ways, either by using a label:\n\n{\"_id\": 1, \"source\": \"ALLEGRO\"}\n\n\nor by ordinal value:\n\n{\"_id\": 1, \"source\": 1}\n\n\nHere an analogy with the first experiment can be found: again we replace a character string with a numerical value.\nSince we got a great result the first time, maybe we could repeat it here?\n\nUnfortunately, the result is disappointing and the reduction in size is small. However, if we think more deeply, we will\ncome to the conclusion that it could not have been otherwise. The enumerated types are not unique identifiers. We are\ndealing only with a few possible values, appearing many times in the whole collection, so it’s a great chance for\nMongoDB data compression to prove its worth. Most likely the values from the first collection have been automatically replaced by the\nengine to its corresponding ordinal values. Additionally, we don’t have any profit on the schema here, because they are\nthe same in both cases. The situation might be different for collections that do not have data compression enabled.\nThis is a good time to take a closer look at how snappy\ncompression works in MongoDB. I’ve prepared two more collections, with identical data, but with data compression\nturned off. The results are shown in the chart below, compiled together with the collections with data compression turned on.\n\nIt is clear that the use of an ordinal value instead of a label of the enumerated type brings considerable profit for\ncollections with data compression disabled. In case of lack of compression it is definitely worth considering using numerical\nvalues.\nUseless _class field\nIf we use spring-data, each of our collections additionally contains a _class field storing the package and the\nname of the entity class containing the mapping for documents from this collection. This mechanism is used to support\ninheritance of model classes, that is not widely used. In most cases this field stores exactly the same values for each\ndocument what makes it useless. And how much does it cost? Let’s compare the following documents:\n\n{\"_id\": 1}\n\n\nand\n\n{\"_id\": 1, \"_class\": \"pl.allegro.some.project.domain.sample\"}\n\n\n\nThe difference is considerable, over 50%. Bearing in mind that the compression is enabled, I believe that the impact of\nthe data itself is small and the result is caused by the schema of the collection containing only the key being half as small\nas the one with the key (1 field vs 2 fields). After removal of the _class field from a document with more fields,\nthe difference expressed in percent will be much smaller of course. However, storing useless data does not make sense.\nUseless indexes\nWhen investigating the case with identifiers stored as strings, I checked whether manipulating the data type could reduce\nthe index size. Unfortunately I did not succeed, so I decided to focus on the problem of redundant indexes.\nIt is usually a good practice to cover each query with an index so that the number of collscan operations is as small as\npossible. This often leads to situations where we have too many indexes. We add more queries, create new indexes for\nthem, and often it turns out that this newly created index takes over the queries of another one.\nAs a result, we have to maintain many unnecessary indexes, wasting disk space and CPU time.\nFortunately, it is possible to check the number of index uses with the query:\n\ndb.COLLECTION_NAME.aggregate([{$indexStats:{}}])\n\n\nIt allows you to find indexes that are not used so you can safely delete them.\nAnd finally, one more thing. Indexes can be removed quickly, but they take much longer to rebuild. This means that the\nconsequences of removal of an index by mistake can be severe. Therefore it is better to make sure that the index to be\ndeleted is definitely not used by any query. The latest MongoDB 4.4 provides a command that helps to avoid errors:\n\ndb.COLLECTION_NAME.hideIndex(\u003cindex\u003e)\n\n\nThe above-mentioned command hides the index from the query optimizer. It does not take this index into account when\nbuilding the query execution plan, but the index is still updated when modifying documents. Thanks to that, if it turns\nout that it was needed, we are able to restore it immediately and it will still be up-to-date.\nConclusion\nUsing a few simple techniques, preparing several versions of the schema and using stats() command we are able to design\na model which does not overload our infrastructure. I encourage you to experiment with your own databases. Maybe you too\ncan save some disk space and CPU time.","guid":"https://blog.allegro.tech/2021/01/impact-of-the-data-model-on-the-MongoDB-database-size.html","categories":["mongodb"],"isoDate":"2021-01-13T23:00:00.000Z","thumbnail":"images/post-headers/mongodb.png"},{"title":"Speeding up warm builds in Xcode","link":"https://blog.allegro.tech/2020/12/speeding-up-warm-builds.html","pubDate":"Mon, 28 Dec 2020 00:00:00 +0100","authors":{"author":[{"name":["Maciej Piotrowski"],"photo":["https://blog.allegro.tech/img/authors/maciej.piotrowski.jpg"],"url":["https://blog.allegro.tech/authors/maciej.piotrowski"]}]},"content":"\u003cp\u003eProgrammers who have ever developed software for Apple platforms in the early days of \u003cstrong\u003eSwift\u003c/strong\u003e language might remember ridiculous\ntimes it took to compile the whole project. For large and complicated codebase times used to range from 10 up to 40 minutes.\nOver the years our toolset has improved alongside with compilation times, but slow build times of source code can still be a nightmare.\u003c/p\u003e\n\n\u003cp\u003eWhen we wait a few minutes for a build, we navigate ourselves towards different activities and start e.g. watching funny animal pictures or\nYouTube videos, easily \u003cstrong\u003eloosing context\u003c/strong\u003e of the task at hand. What becomes annoying for us is \u003cstrong\u003eslow feedback\u003c/strong\u003e of code correctness.\u003c/p\u003e\n\n\u003cp\u003eIn the \u003ca href=\"https://allegro.tech/2020/12/speeding-up-ios-builds-with-bazel.html\"\u003epast issue\u003c/a\u003e my colleague has written about a solution\nto slow \u003cstrong\u003eclean\u003c/strong\u003e builds.In this post I will focus on \u003cstrong\u003ewarm\u003c/strong\u003e builds improvement.\u003c/p\u003e\n\n\u003ch2 id=\"clean-and-incremental-builds\"\u003eClean and incremental builds\u003c/h2\u003e\n\n\u003cp\u003eThere are two terms used in the realm of \u003ca href=\"https://developer.apple.com/xcode/\"\u003eXcode\u003c/a\u003e when it comes to distinguishing types of\ncompilation: \u003cstrong\u003eclean\u003c/strong\u003e and \u003cstrong\u003eincremental\u003c/strong\u003e build. The first refers to the time it takes to build a project from scratch. The latter is the time\nit takes to build only whatever changed since the last build and to integrate the changes into a build product.\u003c/p\u003e\n\n\u003cp\u003eYou might also be familiar with the term \u003cstrong\u003ewarm\u003c/strong\u003e build. It’s used interchangeably with \u003cstrong\u003eincremental\u003c/strong\u003e build term, but for the sake of this\npost I will be using it to refer to \u003cem\u003ethe time it takes to build a product since the last clean build without introducing any source code\nchanges\u003c/em\u003e.\u003c/p\u003e\n\n\u003ch2 id=\"why-and-what-for\"\u003eWhy and what for\u003c/h2\u003e\n\n\u003cp\u003eWhy bothering with improving \u003cstrong\u003ewarm\u003c/strong\u003e build times? Well, for a small projects built on super fast workstations it might take a fraction of a\nsecond to do a warm build, but as projects grow and multiple \u003cem\u003eBuild Phases\u003c/em\u003e get added to a target so grow the build times. These times\nare noticeable especially when one builds the target without an introduction of changes to the source code.\u003c/p\u003e\n\n\u003cp\u003eBefore we started improving the warm build of the Allegro app for iOS platform it took 18 seconds to perform the build on our\nContinuous Integration (CI) servers (Mac Mini, 6-Core 3.2 GHz CPU, 32 GB Ram).\u003c/p\u003e\n\n\u003cp\u003eIs 18 seconds too much? When you put it into a perspective of 1 year:\u003c/p\u003e\n\n\u003cblockquote\u003e\n  \u003cp\u003e18 seconds × 6 builds per hour × 8 hours per day × 20 days per month × 12 months per year = 207360 seconds = 3456 minutes = 57\nhours 36 minutes\u003c/p\u003e\n\u003c/blockquote\u003e\n\n\u003cp\u003eIt means that on average a programmer spends around 57 hours 36 minutes yearly to wait for a feedback if their code is correct. Is it\nmuch? I leave the answer to you, but it definitely hinders developer’s experience and distracts the developer from their job.\u003c/p\u003e\n\n\u003cp\u003eTo make the developer’s experience better, we, the iOS Mobile Core Team at Allegro, have set the goal to minimize the time developers\nspend between hitting the build button and getting the feedback on their code as quickly as possible.\u003c/p\u003e\n\n\u003cp\u003eHow could the goal be achieved? Well, before I answer that, let’s put some light onto how to actually measure build times.\u003c/p\u003e\n\n\u003ch2 id=\"measurements\"\u003eMeasurements\u003c/h2\u003e\n\n\u003cp\u003eDevelopers building software for Apple platforms use the \u003ca href=\"https://developer.apple.com/xcode/\"\u003eXcode\u003c/a\u003e application which has a command\nline interface called \u003ccode class=\"language-plaintext highlighter-rouge\"\u003excodebuild\u003c/code\u003e. The Xcode has an option to output times for build phases from the menu\n\u003ccode class=\"language-plaintext highlighter-rouge\"\u003eProduct \u0026gt; Perform Action \u0026gt; Build With Timing Summary\u003c/code\u003e (doesn’t seem to work on Xcode 12.2 at the time of writing this\nblog post). To get build times with \u003ccode class=\"language-plaintext highlighter-rouge\"\u003excodebuild\u003c/code\u003e for our Allegro app for each build phase of the main target the following command\ncan be used:\u003c/p\u003e\n\n\u003cdiv class=\"language-sh highlighter-rouge\"\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre class=\"highlight\"\u003e\u003ccode\u003excodebuild \u003cspan class=\"nt\"\u003e-workspace\u003c/span\u003e \u003cspan class=\"s1\"\u003e'Allegro/Allegro.xcworkspace'\u003c/span\u003e \u003cspan class=\"se\"\u003e\\\u003c/span\u003e\n\u003cspan class=\"nt\"\u003e-scheme\u003c/span\u003e \u003cspan class=\"s1\"\u003e'Allegro'\u003c/span\u003e \u003cspan class=\"se\"\u003e\\\u003c/span\u003e\n\u003cspan class=\"nt\"\u003e-configuration\u003c/span\u003e \u003cspan class=\"s1\"\u003e'Debug'\u003c/span\u003e \u003cspan class=\"se\"\u003e\\\u003c/span\u003e\n\u003cspan class=\"nt\"\u003e-sdk\u003c/span\u003e \u003cspan class=\"s1\"\u003e'iphonesimulator'\u003c/span\u003e \u003cspan class=\"se\"\u003e\\\u003c/span\u003e\n\u003cspan class=\"nt\"\u003e-arch\u003c/span\u003e \u003cspan class=\"s1\"\u003e'x86_64'\u003c/span\u003e \u003cspan class=\"se\"\u003e\\\u003c/span\u003e\n\u003cspan class=\"nt\"\u003e-showBuildTimingSummary\u003c/span\u003e \u003cspan class=\"se\"\u003e\\\u003c/span\u003e\nbuild \u003cspan class=\"se\"\u003e\\\u003c/span\u003e\n| \u003cspan class=\"nb\"\u003esed\u003c/span\u003e \u003cspan class=\"nt\"\u003e-n\u003c/span\u003e \u003cspan class=\"nt\"\u003e-e\u003c/span\u003e \u003cspan class=\"s1\"\u003e'/Build Timing Summary/,$p'\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\n\n\u003cp\u003eIn the case of the Allegro app it outputs the following lines when I do a \u003cstrong\u003eclean\u003c/strong\u003e build with Xcode 12.2’s \u003ccode class=\"language-plaintext highlighter-rouge\"\u003excodebuild\u003c/code\u003e\n(Mac Book Pro 2.2 GHz 6-Core Intel Core i7 CPU, 32 GB RAM):\u003c/p\u003e\n\n\u003cdiv class=\"language-sh highlighter-rouge\"\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre class=\"highlight\"\u003e\u003ccode\u003eBuild Timing Summary\n\nCompileC \u003cspan class=\"o\"\u003e(\u003c/span\u003e49 tasks\u003cspan class=\"o\"\u003e)\u003c/span\u003e | 174.459 seconds\n\nCompileSwiftSources \u003cspan class=\"o\"\u003e(\u003c/span\u003e3 tasks\u003cspan class=\"o\"\u003e)\u003c/span\u003e | 31.747 seconds\n\nCompileStoryboard \u003cspan class=\"o\"\u003e(\u003c/span\u003e6 tasks\u003cspan class=\"o\"\u003e)\u003c/span\u003e | 29.057 seconds\n\nPhaseScriptExecution \u003cspan class=\"o\"\u003e(\u003c/span\u003e8 tasks\u003cspan class=\"o\"\u003e)\u003c/span\u003e | 22.320 seconds\n\nDitto \u003cspan class=\"o\"\u003e(\u003c/span\u003e21 tasks\u003cspan class=\"o\"\u003e)\u003c/span\u003e | 22.282 seconds\n\nLd \u003cspan class=\"o\"\u003e(\u003c/span\u003e3 tasks\u003cspan class=\"o\"\u003e)\u003c/span\u003e | 13.432 seconds\n\nCompileAssetCatalog \u003cspan class=\"o\"\u003e(\u003c/span\u003e1 task\u003cspan class=\"o\"\u003e)\u003c/span\u003e | 6.620 seconds\n\nValidateEmbeddedBinary \u003cspan class=\"o\"\u003e(\u003c/span\u003e2 tasks\u003cspan class=\"o\"\u003e)\u003c/span\u003e | 6.528 seconds\n\nCompileXIB \u003cspan class=\"o\"\u003e(\u003c/span\u003e1 task\u003cspan class=\"o\"\u003e)\u003c/span\u003e | 5.000 seconds\n\nCodeSign \u003cspan class=\"o\"\u003e(\u003c/span\u003e3 tasks\u003cspan class=\"o\"\u003e)\u003c/span\u003e | 1.419 seconds\n\nCopyPNGFile \u003cspan class=\"o\"\u003e(\u003c/span\u003e3 tasks\u003cspan class=\"o\"\u003e)\u003c/span\u003e | 1.236 seconds\n\nTouch \u003cspan class=\"o\"\u003e(\u003c/span\u003e4 tasks\u003cspan class=\"o\"\u003e)\u003c/span\u003e | 0.318 seconds\n\nLibtool \u003cspan class=\"o\"\u003e(\u003c/span\u003e1 task\u003cspan class=\"o\"\u003e)\u003c/span\u003e | 0.241 seconds\n\nLinkStoryboards \u003cspan class=\"o\"\u003e(\u003c/span\u003e2 tasks\u003cspan class=\"o\"\u003e)\u003c/span\u003e | 0.108 seconds\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\n\n\u003cp\u003eThere’s a lot of tasks. When it comes to source code compilation process, we can lower down build times by splitting the source code\ninto modular frameworks, using binary caching techniques and adding explicit types for expressions in Swift.\u003c/p\u003e\n\n\u003cp\u003eIn the case of a \u003cstrong\u003ewarm build\u003c/strong\u003e the only phases listed are:\u003c/p\u003e\n\n\u003cdiv class=\"language-sh highlighter-rouge\"\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre class=\"highlight\"\u003e\u003ccode\u003eBuild Timing Summary\n\nPhaseScriptExecution \u003cspan class=\"o\"\u003e(\u003c/span\u003e6 tasks\u003cspan class=\"o\"\u003e)\u003c/span\u003e | 23.350 seconds\n\nValidateEmbeddedBinary \u003cspan class=\"o\"\u003e(\u003c/span\u003e2 tasks\u003cspan class=\"o\"\u003e)\u003c/span\u003e | 2.424 seconds\n\n\u003cspan class=\"k\"\u003e**\u003c/span\u003e BUILD SUCCEEDED \u003cspan class=\"k\"\u003e**\u003c/span\u003e \u003cspan class=\"o\"\u003e[\u003c/span\u003e27.238 sec]\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\n\n\u003cp\u003eThanks to performing the \u003cstrong\u003ewarm build\u003c/strong\u003e it can be easily noticed that there’s a room for improvement when it comes to\n\u003ccode class=\"language-plaintext highlighter-rouge\"\u003ePhaseScriptExecution\u003c/code\u003e part of the build process. This is actually the part over which we have the control of. Let’s see, what we can\ndo in order to speed up the build time by playing with what and how scripts get executed.\u003c/p\u003e\n\n\u003ch2 id=\"cleaning-up-run-scripts\"\u003eCleaning up run scripts\u003c/h2\u003e\n\n\u003cp\u003eFirst thing we did with for our iOS application target was selecting scripts which can be run only for Release builds. There’s an easy way\nin Xcode to mark them as runnable for such builds only - just select \u003ccode class=\"language-plaintext highlighter-rouge\"\u003eFor install builds only\u003c/code\u003e checkbox.\u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"/img/articles/2020-12-28-speeding-up-warm-builds/xcode-run-for-release.png\" alt=\"Run script: For install builds only - checkbox in Xcode\" /\u003e\u003c/p\u003e\n\n\u003cp\u003eWhat jobs are great for running only for Release builds? We selected a few:\u003c/p\u003e\n\n\u003cul\u003e\n  \u003cli\u003euploading debug symbols to 3rd party monitoring services\u003c/li\u003e\n  \u003cli\u003esetting endpoints or enabling Apple Transport Security (ATS) for Debug/Release builds\u003c/li\u003e\n  \u003cli\u003eselecting proper \u003ccode class=\"language-plaintext highlighter-rouge\"\u003e.plist\u003c/code\u003e files for Debug/Release builds\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003eNot all tasks can be selected as Release - only. Some of them need to be run for Debug and Release builds, but they don’t have to be\nrun for every build. Xcode 12 introduced a neat feature - running the script based on dependency analysis.\u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"/img/articles/2020-12-28-speeding-up-warm-builds/xcode-dependency-analysis.png\" alt=\"Run script: Based on dependency analysis - checkbox in Xcode\" /\u003e\u003c/p\u003e\n\n\u003cp\u003eSelecting the checkbox isn’t enough to benefit from dependency analysis. Xcode analyses dependencies of a script, i.e. it verifies if the\ninputs of the script have changed since the last run and if the outputs of the script exist. The potential problem occurred for scripts in our\nproject - they didn’t have explicit inputs and outputs defined so we couldn’t tap into the brand new feature of Xcode.\u003c/p\u003e\n\n\u003ch2 id=\"defining-inputs-and-outputs-for-scripts\"\u003eDefining inputs and outputs for scripts\u003c/h2\u003e\n\n\u003cp\u003eOne of the scripts in our project which is time-consuming copies bundles with resources of each module. Our Xcode workspace consists\nof multiple projects. The main project contains the application target which depends on modules built by other projects. The projects\ncontain static frameworks with resources. The resources for each framework are wrapped in \u003ccode class=\"language-plaintext highlighter-rouge\"\u003e.bundle\u003c/code\u003e wrapper and are embedded in\nthe framework. All frameworks are linked statically to the application and their bundles are copied by the script to the application\nwrapper (\u003ccode class=\"language-plaintext highlighter-rouge\"\u003e.app\u003c/code\u003e).\u003c/p\u003e\n\n\u003cp\u003eThe list with \u003ccode class=\"language-plaintext highlighter-rouge\"\u003e.bundle\u003c/code\u003e files to be copied became an input to our script. We also created a list with paths to which bundles are copied.\nXcode uses a \u003ccode class=\"language-plaintext highlighter-rouge\"\u003e.xcfilelist\u003c/code\u003e format for such lists, but it’s just a file with newline-separated values. The\n\u003ccode class=\"language-plaintext highlighter-rouge\"\u003ecopy-bundles-input.xcfilelist\u003c/code\u003e input to our script looks as such:\u003c/p\u003e\n\n\u003cdiv class=\"language-sh highlighter-rouge\"\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre class=\"highlight\"\u003e\u003ccode\u003e\u003cspan class=\"si\"\u003e$(\u003c/span\u003eBUILT_PRODUCTS_DIR\u003cspan class=\"si\"\u003e)\u003c/span\u003e/ModuleX.framework/ModuleX.bundle\n\u003cspan class=\"si\"\u003e$(\u003c/span\u003eBUILT_PRODUCTS_DIR\u003cspan class=\"si\"\u003e)\u003c/span\u003e/ModuleY.framework/ModuleY.bundle\n\u003cspan class=\"si\"\u003e$(\u003c/span\u003eBUILT_PRODUCTS_DIR\u003cspan class=\"si\"\u003e)\u003c/span\u003e/ModuleZ.framework/ModuleZ.bundle\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\n\n\u003cp\u003eand the \u003ccode class=\"language-plaintext highlighter-rouge\"\u003ecopy-bundles-output.xcfilelist\u003c/code\u003e output:\u003c/p\u003e\n\n\u003cdiv class=\"language-sh highlighter-rouge\"\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre class=\"highlight\"\u003e\u003ccode\u003e\u003cspan class=\"si\"\u003e$(\u003c/span\u003eTARGET_BUILD_DIR\u003cspan class=\"si\"\u003e)\u003c/span\u003e/\u003cspan class=\"si\"\u003e$(\u003c/span\u003eEXECUTABLE_FOLDER_PATH\u003cspan class=\"si\"\u003e)\u003c/span\u003e/ModuleX.bundle\n\u003cspan class=\"si\"\u003e$(\u003c/span\u003eTARGET_BUILD_DIR\u003cspan class=\"si\"\u003e)\u003c/span\u003e/\u003cspan class=\"si\"\u003e$(\u003c/span\u003eEXECUTABLE_FOLDER_PATH\u003cspan class=\"si\"\u003e)\u003c/span\u003e/ModuleY.bundle\n\u003cspan class=\"si\"\u003e$(\u003c/span\u003eTARGET_BUILD_DIR\u003cspan class=\"si\"\u003e)\u003c/span\u003e/\u003cspan class=\"si\"\u003e$(\u003c/span\u003eEXECUTABLE_FOLDER_PATH\u003cspan class=\"si\"\u003e)\u003c/span\u003e/ModuleZ.bundle\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\n\n\u003cp\u003eFile lists can be accessed in a script through environment variables. Each script can have many of them and they are indexed from 0:\u003c/p\u003e\n\n\u003cul\u003e\n  \u003cli\u003e\u003ccode class=\"language-plaintext highlighter-rouge\"\u003eSCRIPT_INPUT_FILE_LIST_0\u003c/code\u003e\u003c/li\u003e\n  \u003cli\u003e…\u003c/li\u003e\n  \u003cli\u003e\u003ccode class=\"language-plaintext highlighter-rouge\"\u003eSCRIPT_INPUT_FILE_LIST_1024\u003c/code\u003e\u003c/li\u003e\n  \u003cli\u003e\u003ccode class=\"language-plaintext highlighter-rouge\"\u003eSCRIPT_OUTPUT_FILE_LIST_0\u003c/code\u003e\u003c/li\u003e\n  \u003cli\u003e…\u003c/li\u003e\n  \u003cli\u003e\u003ccode class=\"language-plaintext highlighter-rouge\"\u003eSCRIPT_OUTPUT_FILE_LIST_1024\u003c/code\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003eThere is also a possibility to use input and output files instead of a list (not shown on the screens):\u003c/p\u003e\n\n\u003cul\u003e\n  \u003cli\u003e\u003ccode class=\"language-plaintext highlighter-rouge\"\u003eSCRIPT_INPUT_FILE_0\u003c/code\u003e\u003c/li\u003e\n  \u003cli\u003e…\u003c/li\u003e\n  \u003cli\u003e\u003ccode class=\"language-plaintext highlighter-rouge\"\u003eSCRIPT_INPUT_FILE_1024\u003c/code\u003e\u003c/li\u003e\n  \u003cli\u003e\u003ccode class=\"language-plaintext highlighter-rouge\"\u003eSCRIPT_OUTPUT_FILE_0\u003c/code\u003e\u003c/li\u003e\n  \u003cli\u003e…\u003c/li\u003e\n  \u003cli\u003e\u003ccode class=\"language-plaintext highlighter-rouge\"\u003eSCRIPT_OUTPUT_FILE_1024\u003c/code\u003e\u003c/li\u003e\n  \u003cli\u003eand additionally the \u003ccode class=\"language-plaintext highlighter-rouge\"\u003eSCRIPT_INPUT_FILE_COUNT\u003c/code\u003e and \u003ccode class=\"language-plaintext highlighter-rouge\"\u003eSCRIPT_OUTPUT_FILE_COUNT\u003c/code\u003e can be used\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003eWe based our script copying resource bundles only on file lists and it’s actually quite simple - it just copies files from the\ninput file list to the destination which is the path to the executable.\u003c/p\u003e\n\n\u003cdiv class=\"language-sh highlighter-rouge\"\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre class=\"highlight\"\u003e\u003ccode\u003e\u003cspan class=\"nv\"\u003edestination\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"s2\"\u003e\"\u003c/span\u003e\u003cspan class=\"k\"\u003e${\u003c/span\u003e\u003cspan class=\"nv\"\u003eTARGET_BUILD_DIR\u003c/span\u003e\u003cspan class=\"k\"\u003e}\u003c/span\u003e\u003cspan class=\"s2\"\u003e/\u003c/span\u003e\u003cspan class=\"k\"\u003e${\u003c/span\u003e\u003cspan class=\"nv\"\u003eEXECUTABLE_FOLDER_PATH\u003c/span\u003e\u003cspan class=\"k\"\u003e}\u003c/span\u003e\u003cspan class=\"s2\"\u003e\"\u003c/span\u003e\n\u003cspan class=\"nb\"\u003egrep\u003c/span\u003e \u003cspan class=\"nt\"\u003e-v\u003c/span\u003e \u003cspan class=\"s1\"\u003e'^ *#'\u003c/span\u003e \u0026lt; \u003cspan class=\"s2\"\u003e\"\u003c/span\u003e\u003cspan class=\"k\"\u003e${\u003c/span\u003e\u003cspan class=\"nv\"\u003eSCRIPT_INPUT_FILE_LIST_0\u003c/span\u003e\u003cspan class=\"k\"\u003e}\u003c/span\u003e\u003cspan class=\"s2\"\u003e\"\u003c/span\u003e | \u003cspan class=\"k\"\u003ewhile \u003c/span\u003e\u003cspan class=\"nv\"\u003eIFS\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"nb\"\u003eread\u003c/span\u003e \u003cspan class=\"nt\"\u003e-r\u003c/span\u003e bundle_path\n\u003cspan class=\"k\"\u003edo\n    if\u003c/span\u003e \u003cspan class=\"o\"\u003e[\u003c/span\u003e \u003cspan class=\"nt\"\u003e-d\u003c/span\u003e \u003cspan class=\"s2\"\u003e\"\u003c/span\u003e\u003cspan class=\"nv\"\u003e$bundle_path\u003c/span\u003e\u003cspan class=\"s2\"\u003e\"\u003c/span\u003e \u003cspan class=\"o\"\u003e]\u003c/span\u003e\u003cspan class=\"p\"\u003e;\u003c/span\u003e \u003cspan class=\"k\"\u003ethen\n        \u003c/span\u003ersync \u003cspan class=\"nt\"\u003e-auv\u003c/span\u003e \u003cspan class=\"s2\"\u003e\"\u003c/span\u003e\u003cspan class=\"k\"\u003e${\u003c/span\u003e\u003cspan class=\"nv\"\u003ebundle_path\u003c/span\u003e\u003cspan class=\"k\"\u003e}\u003c/span\u003e\u003cspan class=\"s2\"\u003e\"\u003c/span\u003e \u003cspan class=\"s2\"\u003e\"\u003c/span\u003e\u003cspan class=\"k\"\u003e${\u003c/span\u003e\u003cspan class=\"nv\"\u003edestination\u003c/span\u003e\u003cspan class=\"k\"\u003e}\u003c/span\u003e\u003cspan class=\"s2\"\u003e\"\u003c/span\u003e \u003cspan class=\"o\"\u003e||\u003c/span\u003e \u003cspan class=\"nb\"\u003eexit \u003c/span\u003e1\n    \u003cspan class=\"k\"\u003efi\ndone\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\n\n\u003cp\u003eIn the end we tapped into using Xcode’s dependency analysis for a few run scripts and it allowed us to improve warm build time.\u003c/p\u003e\n\n\u003cdiv class=\"language-sh highlighter-rouge\"\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre class=\"highlight\"\u003e\u003ccode\u003eBuild Timing Summary\n\nPhaseScriptExecution \u003cspan class=\"o\"\u003e(\u003c/span\u003e6 tasks\u003cspan class=\"o\"\u003e)\u003c/span\u003e | 3.666 seconds\n\nValidateEmbeddedBinary \u003cspan class=\"o\"\u003e(\u003c/span\u003e2 tasks\u003cspan class=\"o\"\u003e)\u003c/span\u003e | 2.314 seconds\n\n\u003cspan class=\"k\"\u003e**\u003c/span\u003e BUILD SUCCEEDED \u003cspan class=\"k\"\u003e**\u003c/span\u003e \u003cspan class=\"o\"\u003e[\u003c/span\u003e7.500 sec]\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\n\n\u003cp\u003e\u003cimg src=\"/img/articles/2020-12-28-speeding-up-warm-builds/warm-build-graph.png\" alt=\"Allegro iOS - graph depicting Warm Build Time change over months\" /\u003e\u003c/p\u003e\n\n\u003cp\u003eAt the time of writing the \u003cstrong\u003ewarm build time\u003c/strong\u003e on our CI machines takes \u003cstrong\u003e4 seconds\u003c/strong\u003e. The overall goal of speeding up builds is so that\nthe \u003cstrong\u003eclean build\u003c/strong\u003e time becomes equal to \u003cstrong\u003ewarm build\u003c/strong\u003e.\u003c/p\u003e\n\n\u003ch2 id=\"links\"\u003eLinks\u003c/h2\u003e\n\n\u003cp\u003eSome useful links related to improving compilation times for Xcode projects:\u003c/p\u003e\n\n\u003cul\u003e\n  \u003cli\u003e\u003ca href=\"https://www.onswiftwings.com/posts/build-time-optimization-part1/\"\u003eXcode Build Time Optimization - Part 1\u003c/a\u003e\u003c/li\u003e\n  \u003cli\u003e\u003ca href=\"https://www.onswiftwings.com/posts/build-time-optimization-part2/\"\u003eXcode Build Time Optimization - Part 2\u003c/a\u003e\u003c/li\u003e\n  \u003cli\u003e\u003ca href=\"https://eisel.me/signing\"\u003eDisabling code signing for Debug builds\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n","contentSnippet":"Programmers who have ever developed software for Apple platforms in the early days of Swift language might remember ridiculous\ntimes it took to compile the whole project. For large and complicated codebase times used to range from 10 up to 40 minutes.\nOver the years our toolset has improved alongside with compilation times, but slow build times of source code can still be a nightmare.\nWhen we wait a few minutes for a build, we navigate ourselves towards different activities and start e.g. watching funny animal pictures or\nYouTube videos, easily loosing context of the task at hand. What becomes annoying for us is slow feedback of code correctness.\nIn the past issue my colleague has written about a solution\nto slow clean builds.In this post I will focus on warm builds improvement.\nClean and incremental builds\nThere are two terms used in the realm of Xcode when it comes to distinguishing types of\ncompilation: clean and incremental build. The first refers to the time it takes to build a project from scratch. The latter is the time\nit takes to build only whatever changed since the last build and to integrate the changes into a build product.\nYou might also be familiar with the term warm build. It’s used interchangeably with incremental build term, but for the sake of this\npost I will be using it to refer to the time it takes to build a product since the last clean build without introducing any source code\nchanges.\nWhy and what for\nWhy bothering with improving warm build times? Well, for a small projects built on super fast workstations it might take a fraction of a\nsecond to do a warm build, but as projects grow and multiple Build Phases get added to a target so grow the build times. These times\nare noticeable especially when one builds the target without an introduction of changes to the source code.\nBefore we started improving the warm build of the Allegro app for iOS platform it took 18 seconds to perform the build on our\nContinuous Integration (CI) servers (Mac Mini, 6-Core 3.2 GHz CPU, 32 GB Ram).\nIs 18 seconds too much? When you put it into a perspective of 1 year:\n18 seconds × 6 builds per hour × 8 hours per day × 20 days per month × 12 months per year = 207360 seconds = 3456 minutes = 57\nhours 36 minutes\nIt means that on average a programmer spends around 57 hours 36 minutes yearly to wait for a feedback if their code is correct. Is it\nmuch? I leave the answer to you, but it definitely hinders developer’s experience and distracts the developer from their job.\nTo make the developer’s experience better, we, the iOS Mobile Core Team at Allegro, have set the goal to minimize the time developers\nspend between hitting the build button and getting the feedback on their code as quickly as possible.\nHow could the goal be achieved? Well, before I answer that, let’s put some light onto how to actually measure build times.\nMeasurements\nDevelopers building software for Apple platforms use the Xcode application which has a command\nline interface called xcodebuild. The Xcode has an option to output times for build phases from the menu\nProduct \u003e Perform Action \u003e Build With Timing Summary (doesn’t seem to work on Xcode 12.2 at the time of writing this\nblog post). To get build times with xcodebuild for our Allegro app for each build phase of the main target the following command\ncan be used:\n\nxcodebuild -workspace 'Allegro/Allegro.xcworkspace' \\\n-scheme 'Allegro' \\\n-configuration 'Debug' \\\n-sdk 'iphonesimulator' \\\n-arch 'x86_64' \\\n-showBuildTimingSummary \\\nbuild \\\n| sed -n -e '/Build Timing Summary/,$p'\n\n\nIn the case of the Allegro app it outputs the following lines when I do a clean build with Xcode 12.2’s xcodebuild\n(Mac Book Pro 2.2 GHz 6-Core Intel Core i7 CPU, 32 GB RAM):\n\nBuild Timing Summary\n\nCompileC (49 tasks) | 174.459 seconds\n\nCompileSwiftSources (3 tasks) | 31.747 seconds\n\nCompileStoryboard (6 tasks) | 29.057 seconds\n\nPhaseScriptExecution (8 tasks) | 22.320 seconds\n\nDitto (21 tasks) | 22.282 seconds\n\nLd (3 tasks) | 13.432 seconds\n\nCompileAssetCatalog (1 task) | 6.620 seconds\n\nValidateEmbeddedBinary (2 tasks) | 6.528 seconds\n\nCompileXIB (1 task) | 5.000 seconds\n\nCodeSign (3 tasks) | 1.419 seconds\n\nCopyPNGFile (3 tasks) | 1.236 seconds\n\nTouch (4 tasks) | 0.318 seconds\n\nLibtool (1 task) | 0.241 seconds\n\nLinkStoryboards (2 tasks) | 0.108 seconds\n\n\nThere’s a lot of tasks. When it comes to source code compilation process, we can lower down build times by splitting the source code\ninto modular frameworks, using binary caching techniques and adding explicit types for expressions in Swift.\nIn the case of a warm build the only phases listed are:\n\nBuild Timing Summary\n\nPhaseScriptExecution (6 tasks) | 23.350 seconds\n\nValidateEmbeddedBinary (2 tasks) | 2.424 seconds\n\n** BUILD SUCCEEDED ** [27.238 sec]\n\n\nThanks to performing the warm build it can be easily noticed that there’s a room for improvement when it comes to\nPhaseScriptExecution part of the build process. This is actually the part over which we have the control of. Let’s see, what we can\ndo in order to speed up the build time by playing with what and how scripts get executed.\nCleaning up run scripts\nFirst thing we did with for our iOS application target was selecting scripts which can be run only for Release builds. There’s an easy way\nin Xcode to mark them as runnable for such builds only - just select For install builds only checkbox.\n\nWhat jobs are great for running only for Release builds? We selected a few:\nuploading debug symbols to 3rd party monitoring services\nsetting endpoints or enabling Apple Transport Security (ATS) for Debug/Release builds\nselecting proper .plist files for Debug/Release builds\nNot all tasks can be selected as Release - only. Some of them need to be run for Debug and Release builds, but they don’t have to be\nrun for every build. Xcode 12 introduced a neat feature - running the script based on dependency analysis.\n\nSelecting the checkbox isn’t enough to benefit from dependency analysis. Xcode analyses dependencies of a script, i.e. it verifies if the\ninputs of the script have changed since the last run and if the outputs of the script exist. The potential problem occurred for scripts in our\nproject - they didn’t have explicit inputs and outputs defined so we couldn’t tap into the brand new feature of Xcode.\nDefining inputs and outputs for scripts\nOne of the scripts in our project which is time-consuming copies bundles with resources of each module. Our Xcode workspace consists\nof multiple projects. The main project contains the application target which depends on modules built by other projects. The projects\ncontain static frameworks with resources. The resources for each framework are wrapped in .bundle wrapper and are embedded in\nthe framework. All frameworks are linked statically to the application and their bundles are copied by the script to the application\nwrapper (.app).\nThe list with .bundle files to be copied became an input to our script. We also created a list with paths to which bundles are copied.\nXcode uses a .xcfilelist format for such lists, but it’s just a file with newline-separated values. The\ncopy-bundles-input.xcfilelist input to our script looks as such:\n\n$(BUILT_PRODUCTS_DIR)/ModuleX.framework/ModuleX.bundle\n$(BUILT_PRODUCTS_DIR)/ModuleY.framework/ModuleY.bundle\n$(BUILT_PRODUCTS_DIR)/ModuleZ.framework/ModuleZ.bundle\n\n\nand the copy-bundles-output.xcfilelist output:\n\n$(TARGET_BUILD_DIR)/$(EXECUTABLE_FOLDER_PATH)/ModuleX.bundle\n$(TARGET_BUILD_DIR)/$(EXECUTABLE_FOLDER_PATH)/ModuleY.bundle\n$(TARGET_BUILD_DIR)/$(EXECUTABLE_FOLDER_PATH)/ModuleZ.bundle\n\n\nFile lists can be accessed in a script through environment variables. Each script can have many of them and they are indexed from 0:\nSCRIPT_INPUT_FILE_LIST_0\n…\n  \nSCRIPT_INPUT_FILE_LIST_1024\nSCRIPT_OUTPUT_FILE_LIST_0\n…\n  \nSCRIPT_OUTPUT_FILE_LIST_1024\nThere is also a possibility to use input and output files instead of a list (not shown on the screens):\nSCRIPT_INPUT_FILE_0\n…\n  \nSCRIPT_INPUT_FILE_1024\nSCRIPT_OUTPUT_FILE_0\n…\n  \nSCRIPT_OUTPUT_FILE_1024\nand additionally the SCRIPT_INPUT_FILE_COUNT and SCRIPT_OUTPUT_FILE_COUNT can be used\nWe based our script copying resource bundles only on file lists and it’s actually quite simple - it just copies files from the\ninput file list to the destination which is the path to the executable.\n\ndestination=\"${TARGET_BUILD_DIR}/${EXECUTABLE_FOLDER_PATH}\"\ngrep -v '^ *#' \u003c \"${SCRIPT_INPUT_FILE_LIST_0}\" | while IFS= read -r bundle_path\ndo\n    if [ -d \"$bundle_path\" ]; then\n        rsync -auv \"${bundle_path}\" \"${destination}\" || exit 1\n    fi\ndone\n\n\nIn the end we tapped into using Xcode’s dependency analysis for a few run scripts and it allowed us to improve warm build time.\n\nBuild Timing Summary\n\nPhaseScriptExecution (6 tasks) | 3.666 seconds\n\nValidateEmbeddedBinary (2 tasks) | 2.314 seconds\n\n** BUILD SUCCEEDED ** [7.500 sec]\n\n\n\nAt the time of writing the warm build time on our CI machines takes 4 seconds. The overall goal of speeding up builds is so that\nthe clean build time becomes equal to warm build.\nLinks\nSome useful links related to improving compilation times for Xcode projects:\nXcode Build Time Optimization - Part 1\nXcode Build Time Optimization - Part 2\nDisabling code signing for Debug builds","guid":"https://blog.allegro.tech/2020/12/speeding-up-warm-builds.html","categories":["ios","xcode","swift","objectivec"],"isoDate":"2020-12-27T23:00:00.000Z","thumbnail":"images/post-headers/xcode.png"},{"title":"Speeding up iOS builds with Bazel","link":"https://blog.allegro.tech/2020/12/speeding-up-ios-builds-with-bazel.html","pubDate":"Thu, 17 Dec 2020 00:00:00 +0100","authors":{"author":[{"name":["Kamil Pyć"],"photo":["https://blog.allegro.tech/img/authors/kamil.pyc.jpg"],"url":["https://blog.allegro.tech/authors/kamil.pyc"]}]},"content":"\u003cp\u003eWhen we developed our Allegro iOS app adding new features and with more people contributing to the codebase, we noticed\nthat build times began to grow. In order to have precise metrics, we started to track clean build time as well as the\namount of code we had. Do these two metrics grow at the same pace?\u003c/p\u003e\n\n\u003ch3 id=\"slowing-down\"\u003eSlowing down\u003c/h3\u003e\n\n\u003cp\u003e\u003cimg src=\"/img/articles/2020-12-17-speeding-up-ios-builds-with-bazel/build_time_chart.png\" alt=\"Build time chart\" /\u003e\u003c/p\u003e\n\n\u003cp\u003eOur measurements started in May 2019 with combined 300k lines of Objective-C and Swift code that took around\n~177 seconds to compile. One year later we increased code size by 33% but compilation time grew by 50%.\nIt’s worth noting that this time is measured on our CI machine which is more powerful than a laptop machine —\nbuild times are about 50% slower on our work Macbooks. To put it into perspective - on average developers do 8\nclean builds each day and they will now take about 40 minutes of their work. As we have 25 developers contributing\nto the project, this will add up to 16 hours each day and over 300 hours monthly!\nWe had to make some changes in order to not spend most of our time waiting for the app to compile.\nEven if we have split application into smaller projects it all needs to be built into one single application.\nSince it’s a monolith that needs to be linked together, one “service” cannot be changed in a running app as you would\nin microservice backend infrastructure.\u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"/img/articles/2020-12-17-speeding-up-ios-builds-with-bazel/build_details.png\" alt=\"Build details\" /\u003e\u003c/p\u003e\n\n\u003cp\u003eAt first we tried to speed things up with building our 3rd party dependencies with Carthage. However, this was not very\nefficient being only a small fraction of our code base. Any improvement was quickly eaten up by adding new code that\nneeded time to compile. The direction of not compiling the same code over and over was what we were aiming for.\u003c/p\u003e\n\n\u003ch3 id=\"bazel\"\u003eBazel\u003c/h3\u003e\n\n\u003cp\u003eBefore we started looking for a solution we outlined what was important for us:\u003c/p\u003e\n\n\u003cul\u003e\n  \u003cli\u003eIdeally it should be transparent to our developers - they should only notice an increase in speed\u003c/li\u003e\n  \u003cli\u003eIt should work with modules that mix Obj-C and Swift\u003c/li\u003e\n  \u003cli\u003eIt should be easy to turn off and switch to standard building Xcode if something goes sideways\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003eBasically, this meant we wanted to keep our current setup while adding a mechanism letting us to share compiled\nartifacts(preferably without any special developer integration). Our eyes turned to the open source build\nsystems - \u003ca href=\"https://bazel.build\"\u003eBazel\u003c/a\u003e and \u003ca href=\"https://buck.build\"\u003eBuck\u003c/a\u003e. Comparing these two, we chose Bazel since it\nprovides better support for custom actions\nwith its Starlak language and it’s more popular in the iOS community.\u003c/p\u003e\n\n\u003cp\u003eBazel is Google’s build system that supports C++, Android, iOS, Go and a wide variety of other\nlanguage platforms on Windows, macOS, and Linux. One of its key features is its caching mechanism - both local and\nremote.\u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"/img/articles/2020-12-17-speeding-up-ios-builds-with-bazel/bazel_logo.png\" alt=\"Bazel logo\" /\u003e\u003c/p\u003e\n\n\u003cp\u003eBazel already provides sets of Apple rules that can build a complete application but it didn’t meet our requirements\nsince mixing Swift and Obj-C is not possible. Another problem is that we would need to do the whole transition at once\nsince you cannot simply migrate only a part of the project. We decided to create a custom rule that would use\nxcodebuild to build frameworks - this means we would use the same build system we currently use in everyday development\nand we wouldn’t have to change our current project.\u003c/p\u003e\n\n\u003cp\u003eCustom rules can be written in Starlak language. In our case we needed to wrap\n\u003ccode class=\"language-plaintext highlighter-rouge\"\u003excodebuild\u003c/code\u003e into a \u003ccode class=\"language-plaintext highlighter-rouge\"\u003esh_binary\u003c/code\u003e action:\u003c/p\u003e\n\n\u003cdiv class=\"language-python highlighter-rouge\"\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre class=\"highlight\"\u003e\u003ccode\u003e\u003cspan class=\"n\"\u003esh_binary\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\n    \u003cspan class=\"n\"\u003ename\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"s\"\u003e\"xcodebuild\"\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\n    \u003cspan class=\"n\"\u003esrcs\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"p\"\u003e[\u003c/span\u003e\u003cspan class=\"s\"\u003e\"/usr/bin/xcodebuild\"\u003c/span\u003e\u003cspan class=\"p\"\u003e],\u003c/span\u003e\n    \u003cspan class=\"n\"\u003evisibility\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"p\"\u003e[\u003c/span\u003e\u003cspan class=\"s\"\u003e\"//visibility:public\"\u003c/span\u003e\u003cspan class=\"p\"\u003e]\u003c/span\u003e\n\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\n\n\u003cp\u003eThen, we can create a rule that will call \u003ccode class=\"language-plaintext highlighter-rouge\"\u003excodebuild\u003c/code\u003e and produce \u003ccode class=\"language-plaintext highlighter-rouge\"\u003etarget.framework\u003c/code\u003e:\u003c/p\u003e\n\n\u003cdiv class=\"language-python highlighter-rouge\"\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre class=\"highlight\"\u003e\u003ccode\u003e\u003cspan class=\"k\"\u003edef\u003c/span\u003e \u003cspan class=\"nf\"\u003e_impl\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003ectx\u003c/span\u003e\u003cspan class=\"p\"\u003e):\u003c/span\u003e\n  \u003cspan class=\"n\"\u003ename\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003ectx\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003elabel\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003ename\u003c/span\u003e\n  \u003cspan class=\"n\"\u003epbxProj\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003ectx\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"nb\"\u003efile\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eproject\u003c/span\u003e\n  \u003cspan class=\"n\"\u003eoutput_config\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"s\"\u003e\"CONFIGURATION_BUILD_DIR=../%s\"\u003c/span\u003e \u003cspan class=\"o\"\u003e%\u003c/span\u003e \u003cspan class=\"n\"\u003ectx\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eoutputs\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eframework\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003edirname\u003c/span\u003e\n\n  \u003cspan class=\"n\"\u003ectx\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eactions\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003erun\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\n        \u003cspan class=\"n\"\u003einputs\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"p\"\u003e[\u003c/span\u003e\u003cspan class=\"n\"\u003epbxProj\u003c/span\u003e\u003cspan class=\"p\"\u003e]\u003c/span\u003e \u003cspan class=\"o\"\u003e+\u003c/span\u003e \u003cspan class=\"n\"\u003ectx\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003efiles\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003esrcs\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\n        \u003cspan class=\"n\"\u003eoutputs\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"p\"\u003e[\u003c/span\u003e\u003cspan class=\"n\"\u003ectx\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eoutputs\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eframework\u003c/span\u003e\u003cspan class=\"p\"\u003e],\u003c/span\u003e\n        \u003cspan class=\"n\"\u003earguments\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"p\"\u003e[\u003c/span\u003e\u003cspan class=\"s\"\u003e\"build\"\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"s\"\u003e\"-project\"\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003epbxProj\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003epath\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"s\"\u003e\"-scheme\"\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003ename\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003eoutput_config\u003c/span\u003e\u003cspan class=\"p\"\u003e],\u003c/span\u003e\n        \u003cspan class=\"n\"\u003eprogress_message\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"s\"\u003e\"Building framework %s\"\u003c/span\u003e \u003cspan class=\"o\"\u003e%\u003c/span\u003e \u003cspan class=\"n\"\u003ename\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\n        \u003cspan class=\"n\"\u003eexecutable\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003ectx\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eexecutable\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003excodebuild\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\n    \u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\n\u003cspan class=\"n\"\u003eframework\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003erule\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\n    \u003cspan class=\"n\"\u003eimplementation\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003e_impl\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\n    \u003cspan class=\"n\"\u003eattrs\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"p\"\u003e{\u003c/span\u003e\n        \u003cspan class=\"s\"\u003e\"srcs\"\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"n\"\u003eattr\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003elabel_list\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003eallow_files\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"bp\"\u003eTrue\u003c/span\u003e\u003cspan class=\"p\"\u003e),\u003c/span\u003e\n        \u003cspan class=\"s\"\u003e\"project\"\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"n\"\u003eattr\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003elabel\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\n            \u003cspan class=\"n\"\u003eallow_single_file\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"bp\"\u003eTrue\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\n            \u003cspan class=\"n\"\u003emandatory\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"bp\"\u003eTrue\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\n        \u003cspan class=\"p\"\u003e),\u003c/span\u003e\n        \u003cspan class=\"s\"\u003e\"xcodebuild\"\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"n\"\u003eattr\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003elabel\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\n            \u003cspan class=\"n\"\u003eexecutable\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"bp\"\u003eTrue\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\n            \u003cspan class=\"n\"\u003ecfg\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"s\"\u003e\"host\"\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\n            \u003cspan class=\"n\"\u003eallow_files\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"bp\"\u003eTrue\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\n            \u003cspan class=\"n\"\u003edefault\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003eLabel\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s\"\u003e\"//bazel/xcodebuild\"\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n        \u003cspan class=\"p\"\u003e),\u003c/span\u003e\n      \u003cspan class=\"p\"\u003e},\u003c/span\u003e\n      \u003cspan class=\"n\"\u003eoutputs\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"p\"\u003e{\u003c/span\u003e\u003cspan class=\"s\"\u003e\"framework\"\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"s\"\u003e\"%{name}.framework\"\u003c/span\u003e\u003cspan class=\"p\"\u003e},\u003c/span\u003e\n\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\n\n\u003cp\u003eWith that we can now build any project we want to, in this case AFNetworking library:\u003c/p\u003e\n\n\u003cdiv class=\"language-python highlighter-rouge\"\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre class=\"highlight\"\u003e\u003ccode\u003e\n\u003cspan class=\"n\"\u003eload\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s\"\u003e\"//bazel:xcodebuild.bzl\"\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"s\"\u003e\"framework\"\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\n\u003cspan class=\"n\"\u003eframework\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\n   \u003cspan class=\"n\"\u003ename\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"s\"\u003e\"AFNetworking\"\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\n   \u003cspan class=\"n\"\u003eproject\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"s\"\u003e\"Pods/AFNetworking.xcodeproj\"\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\n   \u003cspan class=\"n\"\u003esrcs\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003eglob\u003c/span\u003e\u003cspan class=\"p\"\u003e([\u003c/span\u003e\u003cspan class=\"s\"\u003e\"Pods/AFNetworking/**/*\"\u003c/span\u003e\u003cspan class=\"p\"\u003e]),\u003c/span\u003e\n\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\n\n\u003cp\u003eThen we can call:\u003c/p\u003e\n\n\u003cdiv class=\"language-shell highlighter-rouge\"\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre class=\"highlight\"\u003e\u003ccode\u003e./bazel/bazelisk build //:AFNetworking\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\n\n\u003cp\u003eand this should be given as an output:\u003c/p\u003e\n\n\u003cdiv class=\"language-shell highlighter-rouge\"\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre class=\"highlight\"\u003e\u003ccode\u003e\u003cspan class=\"k\"\u003e**\u003c/span\u003e BUILD SUCCEEDED \u003cspan class=\"k\"\u003e**\u003c/span\u003e \u003cspan class=\"o\"\u003e[\u003c/span\u003e11.279 sec]\n\nTarget //:AFNetworking:\n  bazel-bin/AFNetworking.framework\nINFO: Elapsed \u003cspan class=\"nb\"\u003etime\u003c/span\u003e: 12.427s, Critical Path: 12.28s\nINFO: 1 process: 1 local.\nINFO: Build completed successfully, 2 total actions\u003cspan class=\"sb\"\u003e`\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\n\n\u003cp\u003eThanks to Bazel, build will only be performed once and rebuild only when any of the target files change.\nOnce we point to a remote cache with –remote_http_cache we can share this artefact in a shared remote cache.\nIt’s amazing how easy it is to set up a remote cache.\u003c/p\u003e\n\n\u003cp\u003eHow can we use Bazel from Xcode, though? Unfortunately, Xcode is not known for great support of external build systems\nand there is no way of doing it ourselves since it’s closed source. The only way of extending it are plugins whose\ncapabilities are very limited. Fortunately, there is a way: we can use Build Phases that are run each time a project is\nbuilt. It’s a simple Run Script phase that invokes Bazel and copies created frameworks to BUILT_PRODUCTS_DIR.\nWhen developers are not working on a given module, we use our special tool that will generate a workspace without it\nand this target will be built with Bazel in this Build Phase. Thanks to shared remote cache, most of the time instead\nof compiling it we would just download precompiled frameworks.\u003c/p\u003e\n\n\u003cp\u003eAfter migrating all of our modules to Bazel we were able to significantly reduce our clean build time. It dropped over\nthreefold, going from 260s to just 85s. Developers’ experience improved as well, because Xcode is a lot more responsive\nthan before because of reducing the number of projects included in the workspace.\u003c/p\u003e\n\n\u003cp\u003eIt’s worth noting that if any of our scripts or build artefacts contain e.g. local paths they will cause misses in\nour cache. To prevent this we monitor our local and CI builds times and cache hits to detect such situations.\u003c/p\u003e\n\n\u003ch3 id=\"tests\"\u003eTests\u003c/h3\u003e\n\n\u003cp\u003eA couple years ago we moved all of our iOS projects to a single monorepo.\nThis has drastically simplified development since we don’t have to maintain a pyramid of hundreds of dependencies\nbetween dozens of repositories anymore. One downside is that all projects combined produce over 15.000 unit tests that\ntake over an hour to build and run. We didn’t want to wait that long in each PR, so we decided to run only a selected\nportion of tests affected by introduced changes. To achieve this we had to maintain a list of dependencies between\ndifferent projects and that was obviously very error prone.\nThe chart below shows just a small portion of our dependency tree (generated in Bazel).\u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"/img/articles/2020-12-17-speeding-up-ios-builds-with-bazel/dependency_graph.png\" alt=\"Dependency graph\" /\u003e\u003c/p\u003e\n\n\u003cp\u003eAfter the migration to Bazel we can query our dependency graph to get a list of targets that a given file affects\nand run unit tests for that target. That improved our experience since we used to manually maintain list of\ndependencies beetwen our module which was error prone and time consuming.\u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"/img/articles/2020-12-17-speeding-up-ios-builds-with-bazel/query.png\" alt=\"Sample query\" /\u003e\u003c/p\u003e\n\n\u003cp\u003eBuild results can be cached the same way as build artifacts.\nThis has dramatically reduced test times of our master branch test plan, as we can run \u003ccode class=\"language-plaintext highlighter-rouge\"\u003ebazel test //...\u003c/code\u003e and only test\ntargets that have not been run previously. Take a look at the below chart to see how good our result are:\u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"/img/articles/2020-12-17-speeding-up-ios-builds-with-bazel/tests_time_chart.png\" alt=\"Tests time chart\" /\u003e\u003c/p\u003e\n\n\u003ch3 id=\"conclusion\"\u003eConclusion\u003c/h3\u003e\n\n\u003cp\u003eIntegrating Bazel into an iOS project requires some effort, but in our opinion it’s worth it, especially in large scale\nprojects. We observe more and more companies struggling with fast and scalable builds. Some of the key tech players,\nincluding Lyft, Pinterest and LinkedIn, switched to Bazel for building iOS apps as well. It’s worth watching Keith\nSmiley \u0026amp; Dave Lee \u003ca href=\"https://www.youtube.com/watch?v=NAPeWoimGx8\"\u003etalk\u003c/a\u003e from Bazel Conf about migration of Lyft app to\nBazel.\u003c/p\u003e\n\n\u003cp\u003eWe still have the main app target with a large amount of source code that always needs to be compiled.\nCurrently we are working on splitting the app target into modules, so we can cache this code as well and reduce build\ntime even further. In the future we will try to make the same Bazel migration with our Android application to achieve\nthe same build speed improvement and have single build tool for both platforms. We will also try out try another\npromising feature, called Remote Execution - so we can use remote workers to perform remote builds. We estimate that\nafter completion of these plans, we can further reduce our build times to about 10 seconds.\u003c/p\u003e\n","contentSnippet":"When we developed our Allegro iOS app adding new features and with more people contributing to the codebase, we noticed\nthat build times began to grow. In order to have precise metrics, we started to track clean build time as well as the\namount of code we had. Do these two metrics grow at the same pace?\nSlowing down\n\nOur measurements started in May 2019 with combined 300k lines of Objective-C and Swift code that took around\n~177 seconds to compile. One year later we increased code size by 33% but compilation time grew by 50%.\nIt’s worth noting that this time is measured on our CI machine which is more powerful than a laptop machine —\nbuild times are about 50% slower on our work Macbooks. To put it into perspective - on average developers do 8\nclean builds each day and they will now take about 40 minutes of their work. As we have 25 developers contributing\nto the project, this will add up to 16 hours each day and over 300 hours monthly!\nWe had to make some changes in order to not spend most of our time waiting for the app to compile.\nEven if we have split application into smaller projects it all needs to be built into one single application.\nSince it’s a monolith that needs to be linked together, one “service” cannot be changed in a running app as you would\nin microservice backend infrastructure.\n\nAt first we tried to speed things up with building our 3rd party dependencies with Carthage. However, this was not very\nefficient being only a small fraction of our code base. Any improvement was quickly eaten up by adding new code that\nneeded time to compile. The direction of not compiling the same code over and over was what we were aiming for.\nBazel\nBefore we started looking for a solution we outlined what was important for us:\nIdeally it should be transparent to our developers - they should only notice an increase in speed\nIt should work with modules that mix Obj-C and Swift\nIt should be easy to turn off and switch to standard building Xcode if something goes sideways\nBasically, this meant we wanted to keep our current setup while adding a mechanism letting us to share compiled\nartifacts(preferably without any special developer integration). Our eyes turned to the open source build\nsystems - Bazel and Buck. Comparing these two, we chose Bazel since it\nprovides better support for custom actions\nwith its Starlak language and it’s more popular in the iOS community.\nBazel is Google’s build system that supports C++, Android, iOS, Go and a wide variety of other\nlanguage platforms on Windows, macOS, and Linux. One of its key features is its caching mechanism - both local and\nremote.\n\nBazel already provides sets of Apple rules that can build a complete application but it didn’t meet our requirements\nsince mixing Swift and Obj-C is not possible. Another problem is that we would need to do the whole transition at once\nsince you cannot simply migrate only a part of the project. We decided to create a custom rule that would use\nxcodebuild to build frameworks - this means we would use the same build system we currently use in everyday development\nand we wouldn’t have to change our current project.\nCustom rules can be written in Starlak language. In our case we needed to wrap\nxcodebuild into a sh_binary action:\n\nsh_binary(\n    name = \"xcodebuild\",\n    srcs = [\"/usr/bin/xcodebuild\"],\n    visibility = [\"//visibility:public\"]\n)\n\n\nThen, we can create a rule that will call xcodebuild and produce target.framework:\n\ndef _impl(ctx):\n  name = ctx.label.name\n  pbxProj = ctx.file.project\n  output_config = \"CONFIGURATION_BUILD_DIR=../%s\" % ctx.outputs.framework.dirname\n\n  ctx.actions.run(\n        inputs = [pbxProj] + ctx.files.srcs,\n        outputs = [ctx.outputs.framework],\n        arguments = [\"build\", \"-project\", pbxProj.path, \"-scheme\", name, output_config],\n        progress_message = \"Building framework %s\" % name,\n        executable = ctx.executable.xcodebuild,\n    )\n\nframework = rule(\n    implementation = _impl,\n    attrs = {\n        \"srcs\": attr.label_list(allow_files = True),\n        \"project\": attr.label(\n            allow_single_file = True,\n            mandatory = True,\n        ),\n        \"xcodebuild\": attr.label(\n            executable = True,\n            cfg = \"host\",\n            allow_files = True,\n            default = Label(\"//bazel/xcodebuild\")\n        ),\n      },\n      outputs = {\"framework\": \"%{name}.framework\"},\n)\n\n\nWith that we can now build any project we want to, in this case AFNetworking library:\n\n\nload(\"//bazel:xcodebuild.bzl\", \"framework\")\n\nframework(\n   name = \"AFNetworking\",\n   project = \"Pods/AFNetworking.xcodeproj\",\n   srcs = glob([\"Pods/AFNetworking/**/*\"]),\n)\n\n\n\nThen we can call:\n\n./bazel/bazelisk build //:AFNetworking\n\n\nand this should be given as an output:\n\n** BUILD SUCCEEDED ** [11.279 sec]\n\nTarget //:AFNetworking:\n  bazel-bin/AFNetworking.framework\nINFO: Elapsed time: 12.427s, Critical Path: 12.28s\nINFO: 1 process: 1 local.\nINFO: Build completed successfully, 2 total actions`\n\n\nThanks to Bazel, build will only be performed once and rebuild only when any of the target files change.\nOnce we point to a remote cache with –remote_http_cache we can share this artefact in a shared remote cache.\nIt’s amazing how easy it is to set up a remote cache.\nHow can we use Bazel from Xcode, though? Unfortunately, Xcode is not known for great support of external build systems\nand there is no way of doing it ourselves since it’s closed source. The only way of extending it are plugins whose\ncapabilities are very limited. Fortunately, there is a way: we can use Build Phases that are run each time a project is\nbuilt. It’s a simple Run Script phase that invokes Bazel and copies created frameworks to BUILT_PRODUCTS_DIR.\nWhen developers are not working on a given module, we use our special tool that will generate a workspace without it\nand this target will be built with Bazel in this Build Phase. Thanks to shared remote cache, most of the time instead\nof compiling it we would just download precompiled frameworks.\nAfter migrating all of our modules to Bazel we were able to significantly reduce our clean build time. It dropped over\nthreefold, going from 260s to just 85s. Developers’ experience improved as well, because Xcode is a lot more responsive\nthan before because of reducing the number of projects included in the workspace.\nIt’s worth noting that if any of our scripts or build artefacts contain e.g. local paths they will cause misses in\nour cache. To prevent this we monitor our local and CI builds times and cache hits to detect such situations.\nTests\nA couple years ago we moved all of our iOS projects to a single monorepo.\nThis has drastically simplified development since we don’t have to maintain a pyramid of hundreds of dependencies\nbetween dozens of repositories anymore. One downside is that all projects combined produce over 15.000 unit tests that\ntake over an hour to build and run. We didn’t want to wait that long in each PR, so we decided to run only a selected\nportion of tests affected by introduced changes. To achieve this we had to maintain a list of dependencies between\ndifferent projects and that was obviously very error prone.\nThe chart below shows just a small portion of our dependency tree (generated in Bazel).\n\nAfter the migration to Bazel we can query our dependency graph to get a list of targets that a given file affects\nand run unit tests for that target. That improved our experience since we used to manually maintain list of\ndependencies beetwen our module which was error prone and time consuming.\n\nBuild results can be cached the same way as build artifacts.\nThis has dramatically reduced test times of our master branch test plan, as we can run bazel test //... and only test\ntargets that have not been run previously. Take a look at the below chart to see how good our result are:\n\nConclusion\nIntegrating Bazel into an iOS project requires some effort, but in our opinion it’s worth it, especially in large scale\nprojects. We observe more and more companies struggling with fast and scalable builds. Some of the key tech players,\nincluding Lyft, Pinterest and LinkedIn, switched to Bazel for building iOS apps as well. It’s worth watching Keith\nSmiley \u0026 Dave Lee talk from Bazel Conf about migration of Lyft app to\nBazel.\nWe still have the main app target with a large amount of source code that always needs to be compiled.\nCurrently we are working on splitting the app target into modules, so we can cache this code as well and reduce build\ntime even further. In the future we will try to make the same Bazel migration with our Android application to achieve\nthe same build speed improvement and have single build tool for both platforms. We will also try out try another\npromising feature, called Remote Execution - so we can use remote workers to perform remote builds. We estimate that\nafter completion of these plans, we can further reduce our build times to about 10 seconds.","guid":"https://blog.allegro.tech/2020/12/speeding-up-ios-builds-with-bazel.html","categories":["ios","bazel"],"isoDate":"2020-12-16T23:00:00.000Z","thumbnail":"images/post-headers/bazel.png"},{"title":"Big data marketing. The story of how technology behind Allegro marketing works.","link":"https://blog.allegro.tech/2020/12/bigdata-marketing.html","pubDate":"Mon, 07 Dec 2020 00:00:00 +0100","authors":{"author":[{"name":["Filip Błaszczyk"],"photo":["https://blog.allegro.tech/img/authors/filip.blaszczyk.jpg"],"url":["https://blog.allegro.tech/authors/filip.blaszczyk"]},{"name":["Piotr Góralczyk"],"photo":["https://blog.allegro.tech/img/authors/piotr.goralczyk.jpg"],"url":["https://blog.allegro.tech/authors/piotr.goralczyk"]},{"name":["Grzegorz Kaczmarczyk"],"photo":["https://blog.allegro.tech/img/authors/grzegorz.kaczmarczyk.jpg"],"url":["https://blog.allegro.tech/authors/grzegorz.kaczmarczyk"]}]},"content":"\u003cp\u003eMarketing is a very important department in every company. In case of Allegro,\nmarketing is especially difficult because you have so many products to promote.\nIn this post we will tell the story of a platform we built for marketing\npurposes.\u003c/p\u003e\n\n\u003ch2 id=\"background\"\u003eBackground\u003c/h2\u003e\n\n\u003cp\u003eAllegro is the biggest e-commerce platform in Poland and one of top 10 largest e-commerce platforms worldwide.\nOur catalog holds almost 200 million offers at this moment (December 2020), and the number is still growing.\nThe marketing team uses tools such as\n\u003ca href=\"https://www.google.com/retail/solutions/merchant-center/\"\u003eGoogle Merchant Center\u003c/a\u003e\nand \u003ca href=\"https://www.facebook.com/business/ads\"\u003eFacebook Ads\u003c/a\u003e in order to advertise\nAllegro offers and get more traffic to the platform. To integrate with\nthem we need to prepare an XML file containing information about our offers.\nSuch XML files are called \u003cem\u003e“feed”\u003c/em\u003e . We will use this name later on.\u003c/p\u003e\n\n\u003cp\u003eSince we need to find specific offers, using a search engine may seem natural.\nHowever, Allegro’s search engine is not suited for this task. Feed could contain\nmillions of offers, what would result in deep pagination issues. A decision\nwas made to simply generate static files using batch jobs run in our \u003ca href=\"https://hadoop.apache.org/\"\u003eHadoop\u003c/a\u003e\necosystem. The ability to handle large volumes of data, powerful query\ncapabilities as well as access to various datasets across the platform were\nmajor advantages. \u003ca href=\"https://spark.apache.org/\"\u003eApache Spark\u003c/a\u003e, an already tried and tested tool, was an\nobvious choice.\u003c/p\u003e\n\n\u003cp\u003eSince we didn’t expect the number of feeds to exceed a few dozen, every\nfeed was calculated in a separate (but executed in parallel) Spark job.\nBusiness users created every feed definition by providing predicates that offers must\nsatisfy to be included in feed, as well as expected XML format and recipient.\nYou can see that architecture in the diagram below. \u003ccode class=\"language-plaintext highlighter-rouge\"\u003eAggregateGeneratorJob\u003c/code\u003e and\n\u003ccode class=\"language-plaintext highlighter-rouge\"\u003eFeedGeneratorJob\u003c/code\u003e were batch jobs written in Apache Spark. First one collected\ndata from different sources on Hive and HDFS, then assembled them into a single\nParquet-based file called simply “aggregate” (we will use this name later on).\nSecond job, \u003ccode class=\"language-plaintext highlighter-rouge\"\u003eFeedGeneratorJob\u003c/code\u003e generated and uploaded a single feed (XML file)\nto S3. All jobs were run in parallel.\u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"/img/articles/2020-12-07-bigdata-marketing/old_arch.svg\" alt=\"Old architecture diagram\" /\u003e\u003c/p\u003e\n\n\u003cp\u003eBut soon, against initial assumptions, a number of feeds exploded. Eventually,\nwe encountered as much as… 1300 feeds! Updating all of them, to\npresent current data in advertisements, took more than 24 hours.\nWe managed to improve this situation a little by vertical scaling and\nremoving some of unused/poor performing feeds. However, it was just a temporary\nimprovement, since it still took as much as 13 hours to refresh all the feeds.\u003c/p\u003e\n\n\u003cp\u003eWe were yet to find out that poor performance was just the tip of the iceberg. Much bigger\nproblem was the architecture that no longer suited our needs and made\nimplementing new features time-consuming. Codebase used a then acclaimed\n\u003ca href=\"https://medium.com/rahasak/scala-cake-pattern-e0cd894dae4e\"\u003ecake (anti)pattern\u003c/a\u003e\nthat turned out to work poorly in connection with Spark. It caused serious serialization issues.\nAdd to that leaky monitoring and handwritten scheduler, and you will\nget a full picture of our despair. Besides, the tool itself became very\nimportant. It handled more and more integrations and was crucial for the\ncompany.\u003c/p\u003e\n\n\u003ch2 id=\"brave-new-world-solution\"\u003eBrave new \u003cdel\u003eworld\u003c/del\u003e solution\u003c/h2\u003e\n\n\u003cp\u003eAt that moment we knew that we needed a new solution.\nWe decided that our target solution should let us:\u003c/p\u003e\n\n\u003cul\u003e\n  \u003cli\u003eReduce the execution time to 1h (2h at most) while keeping same amount of resources\u003c/li\u003e\n  \u003cli\u003eQuery over any offer property (in old solution we had only some predefined predicates)\u003c/li\u003e\n  \u003cli\u003eChoose offer by a key (or in programmers language: group by + aggregate)\u003c/li\u003e\n  \u003cli\u003eIntroduce new data sources quickly\u003c/li\u003e\n  \u003cli\u003eCreate feed with arbitrary size: from just a few offers to whole catalog\u003c/li\u003e\n  \u003cli\u003eScale horizontally\u003c/li\u003e\n  \u003cli\u003eLast but not least: integrate with our partners not only by files\nbut also using an event-based approach (streaming API)\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003eThese requirements would be easy to comply with in case of a “normal” shop with\na few thousands products. However, Allegro operates on a much larger scale of:\u003c/p\u003e\n\n\u003cul\u003e\n  \u003cli\u003ealmost 200M offers (and still growing)\u003c/li\u003e\n  \u003cli\u003e~25-40M changes in offers per day\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003eAlso, Allegro is based on microservices architecture and we don’t have a single\nDB with full information about the system. This leaves us with yet another\nproblem: how to gather all needed data. We have to use information\non offers, sellers, campaigns, ratings, products and few others.\nSo the first item on our TODO list was to find a solution for collecting\nthe data. In Allegro most of the services use\n\u003ca href=\"http://hermes.allegro.tech/\"\u003eHermes\u003c/a\u003e as a message broker. Also, all of the\ndata that is sent by Hermes is dumped to HDFS in near real-time manner. To\nmake this clearer, let me show you that on diagram:\u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"/img/articles/2020-12-07-bigdata-marketing/hermes.svg\" alt=\"Event flow in Allegro\" /\u003e\u003c/p\u003e\n\n\u003cp\u003eAt that moment, we wondered which approach would suit our requirements\nbest. We saw three options here:\u003c/p\u003e\n\n\u003cul\u003e\n  \u003cli\u003eFind some existing solution in our platform and customize it for our needs\u003c/li\u003e\n  \u003cli\u003eUse Hermes topics directly (online)\u003c/li\u003e\n  \u003cli\u003eCollect data from HDFS (offline)\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003eFirst option would be nice, but there was one problem… we haven’t found any\nsuitable source. So basically, we had to choose between collecting all data\nonline vs offline. Beside the most obvious difference, latency, what else\ndifferentiates these solutions?\u003c/p\u003e\n\n\u003cp\u003eIt is always more difficult to join data online. We would need to maintain a\ndatabase with the whole state and we would be prone to all kinds of\nconcurrency-related bugs. In case of any detected problem we would have to\nrecover using historical data.\u003c/p\u003e\n\n\u003cp\u003eOffline solution would be similar to what we had in the old platform\n(\u003ccode class=\"language-plaintext highlighter-rouge\"\u003eAggregateGeneratorJob\u003c/code\u003e described before). Joins between various data sources\nwould be straightforward. We wouldn’t have any problems with concurrency.\nRecreating data is easy, since basically it is done on every job execution,\nalthough we pay for that with latency. The question though was how long would it take\nto create such aggregate and how much latency we would get at that stage.\nConsidering it was easy to implement we decided to simply measure it. In the end it\nturned out not that bad: in typical cases we were able to maintain\nlatency of about 30 minutes.\u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"/img/articles/2020-12-07-bigdata-marketing/aggregate-job.svg\" alt=\"Aggregate job\" /\u003e\u003c/p\u003e\n\n\u003cp\u003eThat was acceptable for a start. In case of it being not enough, we could always transform\nit later into delta architecture and read the newest data (or at least some subset of it,\nfor example products prices) from Hermes to bring it up-to-date.\u003c/p\u003e\n\n\u003cp\u003eOnce we had a data source, we had to find a way to generate feeds based on\nit. We were a bit biased against Apache Spark, because of poor performance\nand hard maintainability of the old platform. Back then we didn’t know that\nproblem was in our solution, not in Spark itself. That’s why we decided to\nspend a few weeks on research. We made a few prototypes based on Spark\nStreaming, Kafka Streams and on databases. We even had an idea of writing our\nown engine for computation. During that research we came up with the idea of generating\nfeeds in an efficient way and… we realized that it will be\npretty easy to implement in Spark! \u003cstrong\u003eWe also made an important decision: we\nwill focus on generating files, and get back to streaming API later\u003c/strong\u003e.\u003c/p\u003e\n\n\u003ch2 id=\"i-am-speed\"\u003eI am speed\u003c/h2\u003e\n\n\u003cp\u003eBasically, in order to generate feed we need to look through offers catalog and find\nall offers matching defined criteria. In a database you typically use indexes\non a subset of fields to simplify searching. Since we need the possibility of\nmaking predicates on all fields as well as of integrating all offers with our\npartner, we decided to go for linear scanning. Is it bad? Well, it depends on next steps.\nSince we decided on linear scanning, we knew that complexity of our process\nwould be at least O(N). We could handle that, but only as long as we would be\nable to make complexity independent of the number of feeds (integrations). Even more\nimportantly, we had to take care of scalability. It would be best to partition data and\ncalculate it concurrently, while sharing as little common state as possible.\u003c/p\u003e\n\n\u003cp\u003eIn the old process every feed was calculated by a separate job. Although we had\nhundreds of integrations, lots of them use the same XML schema (later called\n“template”). Also, lots of feeds use similar predicates, so their results can\nbe cached and reused. Therefore, our first priority was to calculate everything in one\ngo. In order to achieve that we simply divide our aggregate into partitions and for each partition we\nevaluate predicates to figure out which feed includes which offer. When we\nknow that, we also know in which templates we need to render an offer. At the\nend of the process we pivot and write data to appropriate partition-files on\nS3. From S3 we serve partitioned files using our file server that knows how to\nassemble parts and serve them as a single piece.\u003c/p\u003e\n\n\u003cp\u003eOk, so how much speedup we gained thanks to that approach? After rewriting we\nwere able to recalculate all feeds in a little over 1h (comparing to 13h previously).\nIt wasn’t all, though. Not only have we sped the process up 13 times, we also\nreduced memory usage twofold! And well, in the end we used the same tools, but in a better way.\u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"/img/articles/2020-12-07-bigdata-marketing/engine.svg\" alt=\"How engine works\" /\u003e\u003c/p\u003e\n\n\u003ch2 id=\"streaming-api\"\u003eStreaming API\u003c/h2\u003e\n\n\u003cp\u003eAfter we drank our champagne, and cooled down a little bit, we had to return to\nthe problem with providing streaming API. Following ambitious targets of our\nmarketing business, we wanted to integrate Allegro’s offers in a more effective\nway. This type of integration results in smaller latency of products’ updates\nand also fewer resources are required on the partner’s side.\u003c/p\u003e\n\n\u003cp\u003eAt that moment we returned to the problem stated before: \u003cstrong\u003ewhat should\nwe use as a source of data?\u003c/strong\u003e Catching every event in the moment that it was\nproduced would be very difficult due to the scale and resources required\nto handle such traffic.\u003c/p\u003e\n\n\u003cp\u003eMoreover, being a constant listener to all events emitted in our platform and\nsending them instantly to various partners’ APIs brings no benefits in terms\nof data freshness. This is due to the fact that updates are not applied\nimmediately by partners’ sides - even though the latency is lower than in the\nXML solution, it still occurs and can take up to a couple of hours.\u003c/p\u003e\n\n\u003cp\u003eWe decided that we can start with taking data from offers’ aggregate built for\nfile-based feeds. We didn’t want to send all offers that should be integrated\nwith a partner at every job’s run because in most cases we would generate\nredundant events. Some offers didn’t change between two successive runs at all,\nsome of them were newly added or removed from the feed, but in most\ncases they had only partial updates e.g. price change. So we had the idea of \u003cstrong\u003esending\njust the difference between the previous and current event\nfeed state\u003c/strong\u003e. How? Here’s a simplified version of algorithm for this approach:\u003c/p\u003e\n\n\u003cul\u003e\n  \u003cli\u003eload latest state of all Allegro’s offers from HDFS - let’s call it aggregate,\u003c/li\u003e\n  \u003cli\u003eextract from aggregate and save on HDFS only the offers that should be\n included in event feed - let’s call it snapshot X,\u003c/li\u003e\n  \u003cli\u003eload the previous state of the event feed (from the previous run) - snapshot\n Y,\u003c/li\u003e\n  \u003cli\u003emake a full join on X and Y using offer’s unique key - dataset Z of type\n \u003ccode class=\"language-plaintext highlighter-rouge\"\u003eTuple(OfferStateX, OfferStateY)\u003c/code\u003e,\u003c/li\u003e\n  \u003cli\u003edecide to generate appropriate events based on dataset Z:\n    \u003cul\u003e\n      \u003cli\u003eif both values are non-empty, generate an event with the calculated difference between state X and Y,\u003c/li\u003e\n      \u003cli\u003eif the value of X is empty, generate an event on removal from the feed,\u003c/li\u003e\n      \u003cli\u003eif the Y value is empty, generate an event on addition of a new offer to the event feed,\u003c/li\u003e\n    \u003c/ul\u003e\n  \u003c/li\u003e\n  \u003cli\u003esend generated events to Kafka topic that is constantly consumed by the\n service (connector) responsible for sending offers to a marketing partner,\u003c/li\u003e\n  \u003cli\u003esave snapshot X on HDFS (in the next run it will act as a snapshot Y)\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003e\u003cimg src=\"/img/articles/2020-12-07-bigdata-marketing/streaming-api.svg\" alt=\"Streaming API architecture\" /\u003e\u003c/p\u003e\n\n\u003cp\u003eI’m sure you’re wondering how much latency this solution adds.\nWell, it turned out to be only 20 minutes and in our case it is totally acceptable.\nIt is also worth mentioning that our Kafka topic is scalable in case a new partnership appears.\nThis is because the event model contains information about its destinations.\nThanks to this approach, we reduce the amount of data sent, thus limiting the\ntraffic of millions of sent events to just tens of thousands.\u003c/p\u003e\n\n\u003ch2 id=\"self-healing-system\"\u003eSelf-healing system\u003c/h2\u003e\n\n\u003cp\u003eEvery complex system is prone to inconsistencies. Especially when this\ncomplexity increases and it is hard to stop that - as it was before our big\nrefactor. New architecture let us create a self-healing and fully controllable\nsystem that is convenient to maintain even taking into account the scale we\nface everyday. When designing the architecture we focused mainly on two things:\navailability and control.\u003c/p\u003e\n\n\u003ch3 id=\"availability\"\u003eAvailability\u003c/h3\u003e\n\n\u003cp\u003eThe first step that should be considered is: \u003cstrong\u003ehow to properly\ndefine the responsibilities of individual system components?\u003c/strong\u003e\u003c/p\u003e\n\n\u003cp\u003eSystem works as a sum of cooperating elements. But it is easier to maintain\nthese elements when they have very specific tasks to handle. There are three\nmain components in our system (introduced earlier):\u003c/p\u003e\n\n\u003col\u003e\n  \u003cli\u003eAggregator - knows everything about every offer’s current state. It gathers\nall needed data and saves them to HDFS,\u003c/li\u003e\n  \u003cli\u003eGenerator - it takes data generated by Aggregate, filters it and prepares\nfor delivery to a partner,\u003c/li\u003e\n  \u003cli\u003eConnector (possibility of having many connectors) - holds integration with\npartner, acts as a data mapper and sender.\u003c/li\u003e\n\u003c/ol\u003e\n\n\u003cp\u003e\u003cem\u003eAggregator\u003c/em\u003e and \u003cem\u003eGenerator\u003c/em\u003e are \u003cstrong\u003ebased on a complementary set of information\nabout the whole system and offers’ state at a certain point in time\u003c/strong\u003e.\nSo in case the aggregate contains damaged offers and the generator already\ntook it to prepare for sending, in the next cycle it will get overwritten by fixed ones.\nThis happens because each cycle run’s results overwrite the previous ones.\nAdditionally, both Aggregate and Generator stage results are persisted on HDFS.\nThanks to this we can run the whole computation for any period of time and go back to\nany system state. Also, the Generator stage can be based on data generated at\nany time. In case Aggregate is failing while generating new data, Generator\nworks properly using earlier data.\u003c/p\u003e\n\n\u003cp\u003eThen, we have a Connector. It consumes events from Kafka and pushes them,\nin appropriate form, on partner’s API. It has no responsibility for checking\ndata or state correctness. It simply gets what the Generator prepared and\ntries to deliver it to a partner. Thanks to this separation of responsibility,\nConnector is not dependent on Generator - even if Generator has a breakdown,\nthe Connector at worst may have nothing to do.\u003c/p\u003e\n\n\u003ch3 id=\"control\"\u003eControl\u003c/h3\u003e\n\n\u003cp\u003eIn the previous paragraph we mentioned a few processing issues we are\nstruggling with. However, we also proved that despite this our system can still\nwork in such conditions - maybe not as effectively as in standard scenarios,\nbut it still does. To react faster, we’ve managed to make quite “garbage\ndata”-resistant, notifications-based alerting system that will alarm about\nanomalies occuring during computation. In short, if the difference\nbetween states of previous and current Generator run is significant (experience\nbased numbers), the system will stop and inform us about it so that we can\ndecide if this change is acceptable or not. (By difference between states I\nmean difference between parameters such as feed’s offer count, number of\noffers’ parameters changes etc.) Once the change is approved, the system returns\nto its work. Otherwise, data is not propagated from Generator to Kafka, resulting\nin lack of data to be consumed by Connector. Even if we pass some incorrect\ndata to a partner and it will be too late to retreat, we have a\nspecial mechanism refreshing any offer that was updated more than 28 days\nago. So if an offer wasn’t updated for such a long time, it doesn’t matter if\nit is damaged or not – it will be refreshed eventually.\u003c/p\u003e\n\n\u003ch2 id=\"summary\"\u003eSummary\u003c/h2\u003e\n\n\u003cp\u003eKey takeaway points:\u003c/p\u003e\n\n\u003cul\u003e\n  \u003cli\u003eJust because something does not work well it doesn’t mean the tool is bad.\nMaybe there is something wrong with the way you are using it?\u003c/li\u003e\n  \u003cli\u003eIdeas can be complex but it doesn’t mean that they have to be complicated!\u003c/li\u003e\n  \u003cli\u003eResearch is key. Even if your business tells you there is no time for it, insist on it.\nOtherwise you will end up spending even more time on fixes.\u003c/li\u003e\n  \u003cli\u003eApache Spark is a beast. It can simplify your computation dramatically and\ngive amazing results with it, but at the same time you need to\nthink more about how your data will be calculated. One small problem may result\nin slow computation. Unfortunately lots of them are hard to notice.\u003c/li\u003e\n  \u003cli\u003e\u003ca href=\"https://allegro.pl/praca\"\u003eJoin us\u003c/a\u003e if you like such challenges :-)\u003c/li\u003e\n\u003c/ul\u003e\n","contentSnippet":"Marketing is a very important department in every company. In case of Allegro,\nmarketing is especially difficult because you have so many products to promote.\nIn this post we will tell the story of a platform we built for marketing\npurposes.\nBackground\nAllegro is the biggest e-commerce platform in Poland and one of top 10 largest e-commerce platforms worldwide.\nOur catalog holds almost 200 million offers at this moment (December 2020), and the number is still growing.\nThe marketing team uses tools such as\nGoogle Merchant Center\nand Facebook Ads in order to advertise\nAllegro offers and get more traffic to the platform. To integrate with\nthem we need to prepare an XML file containing information about our offers.\nSuch XML files are called “feed” . We will use this name later on.\nSince we need to find specific offers, using a search engine may seem natural.\nHowever, Allegro’s search engine is not suited for this task. Feed could contain\nmillions of offers, what would result in deep pagination issues. A decision\nwas made to simply generate static files using batch jobs run in our Hadoop\necosystem. The ability to handle large volumes of data, powerful query\ncapabilities as well as access to various datasets across the platform were\nmajor advantages. Apache Spark, an already tried and tested tool, was an\nobvious choice.\nSince we didn’t expect the number of feeds to exceed a few dozen, every\nfeed was calculated in a separate (but executed in parallel) Spark job.\nBusiness users created every feed definition by providing predicates that offers must\nsatisfy to be included in feed, as well as expected XML format and recipient.\nYou can see that architecture in the diagram below. AggregateGeneratorJob and\nFeedGeneratorJob were batch jobs written in Apache Spark. First one collected\ndata from different sources on Hive and HDFS, then assembled them into a single\nParquet-based file called simply “aggregate” (we will use this name later on).\nSecond job, FeedGeneratorJob generated and uploaded a single feed (XML file)\nto S3. All jobs were run in parallel.\n\nBut soon, against initial assumptions, a number of feeds exploded. Eventually,\nwe encountered as much as… 1300 feeds! Updating all of them, to\npresent current data in advertisements, took more than 24 hours.\nWe managed to improve this situation a little by vertical scaling and\nremoving some of unused/poor performing feeds. However, it was just a temporary\nimprovement, since it still took as much as 13 hours to refresh all the feeds.\nWe were yet to find out that poor performance was just the tip of the iceberg. Much bigger\nproblem was the architecture that no longer suited our needs and made\nimplementing new features time-consuming. Codebase used a then acclaimed\ncake (anti)pattern\nthat turned out to work poorly in connection with Spark. It caused serious serialization issues.\nAdd to that leaky monitoring and handwritten scheduler, and you will\nget a full picture of our despair. Besides, the tool itself became very\nimportant. It handled more and more integrations and was crucial for the\ncompany.\nBrave new world solution\nAt that moment we knew that we needed a new solution.\nWe decided that our target solution should let us:\nReduce the execution time to 1h (2h at most) while keeping same amount of resources\nQuery over any offer property (in old solution we had only some predefined predicates)\nChoose offer by a key (or in programmers language: group by + aggregate)\nIntroduce new data sources quickly\nCreate feed with arbitrary size: from just a few offers to whole catalog\nScale horizontally\nLast but not least: integrate with our partners not only by files\nbut also using an event-based approach (streaming API)\nThese requirements would be easy to comply with in case of a “normal” shop with\na few thousands products. However, Allegro operates on a much larger scale of:\nalmost 200M offers (and still growing)\n~25-40M changes in offers per day\nAlso, Allegro is based on microservices architecture and we don’t have a single\nDB with full information about the system. This leaves us with yet another\nproblem: how to gather all needed data. We have to use information\non offers, sellers, campaigns, ratings, products and few others.\nSo the first item on our TODO list was to find a solution for collecting\nthe data. In Allegro most of the services use\nHermes as a message broker. Also, all of the\ndata that is sent by Hermes is dumped to HDFS in near real-time manner. To\nmake this clearer, let me show you that on diagram:\n\nAt that moment, we wondered which approach would suit our requirements\nbest. We saw three options here:\nFind some existing solution in our platform and customize it for our needs\nUse Hermes topics directly (online)\nCollect data from HDFS (offline)\nFirst option would be nice, but there was one problem… we haven’t found any\nsuitable source. So basically, we had to choose between collecting all data\nonline vs offline. Beside the most obvious difference, latency, what else\ndifferentiates these solutions?\nIt is always more difficult to join data online. We would need to maintain a\ndatabase with the whole state and we would be prone to all kinds of\nconcurrency-related bugs. In case of any detected problem we would have to\nrecover using historical data.\nOffline solution would be similar to what we had in the old platform\n(AggregateGeneratorJob described before). Joins between various data sources\nwould be straightforward. We wouldn’t have any problems with concurrency.\nRecreating data is easy, since basically it is done on every job execution,\nalthough we pay for that with latency. The question though was how long would it take\nto create such aggregate and how much latency we would get at that stage.\nConsidering it was easy to implement we decided to simply measure it. In the end it\nturned out not that bad: in typical cases we were able to maintain\nlatency of about 30 minutes.\n\nThat was acceptable for a start. In case of it being not enough, we could always transform\nit later into delta architecture and read the newest data (or at least some subset of it,\nfor example products prices) from Hermes to bring it up-to-date.\nOnce we had a data source, we had to find a way to generate feeds based on\nit. We were a bit biased against Apache Spark, because of poor performance\nand hard maintainability of the old platform. Back then we didn’t know that\nproblem was in our solution, not in Spark itself. That’s why we decided to\nspend a few weeks on research. We made a few prototypes based on Spark\nStreaming, Kafka Streams and on databases. We even had an idea of writing our\nown engine for computation. During that research we came up with the idea of generating\nfeeds in an efficient way and… we realized that it will be\npretty easy to implement in Spark! We also made an important decision: we\nwill focus on generating files, and get back to streaming API later.\nI am speed\nBasically, in order to generate feed we need to look through offers catalog and find\nall offers matching defined criteria. In a database you typically use indexes\non a subset of fields to simplify searching. Since we need the possibility of\nmaking predicates on all fields as well as of integrating all offers with our\npartner, we decided to go for linear scanning. Is it bad? Well, it depends on next steps.\nSince we decided on linear scanning, we knew that complexity of our process\nwould be at least O(N). We could handle that, but only as long as we would be\nable to make complexity independent of the number of feeds (integrations). Even more\nimportantly, we had to take care of scalability. It would be best to partition data and\ncalculate it concurrently, while sharing as little common state as possible.\nIn the old process every feed was calculated by a separate job. Although we had\nhundreds of integrations, lots of them use the same XML schema (later called\n“template”). Also, lots of feeds use similar predicates, so their results can\nbe cached and reused. Therefore, our first priority was to calculate everything in one\ngo. In order to achieve that we simply divide our aggregate into partitions and for each partition we\nevaluate predicates to figure out which feed includes which offer. When we\nknow that, we also know in which templates we need to render an offer. At the\nend of the process we pivot and write data to appropriate partition-files on\nS3. From S3 we serve partitioned files using our file server that knows how to\nassemble parts and serve them as a single piece.\nOk, so how much speedup we gained thanks to that approach? After rewriting we\nwere able to recalculate all feeds in a little over 1h (comparing to 13h previously).\nIt wasn’t all, though. Not only have we sped the process up 13 times, we also\nreduced memory usage twofold! And well, in the end we used the same tools, but in a better way.\n\nStreaming API\nAfter we drank our champagne, and cooled down a little bit, we had to return to\nthe problem with providing streaming API. Following ambitious targets of our\nmarketing business, we wanted to integrate Allegro’s offers in a more effective\nway. This type of integration results in smaller latency of products’ updates\nand also fewer resources are required on the partner’s side.\nAt that moment we returned to the problem stated before: what should\nwe use as a source of data? Catching every event in the moment that it was\nproduced would be very difficult due to the scale and resources required\nto handle such traffic.\nMoreover, being a constant listener to all events emitted in our platform and\nsending them instantly to various partners’ APIs brings no benefits in terms\nof data freshness. This is due to the fact that updates are not applied\nimmediately by partners’ sides - even though the latency is lower than in the\nXML solution, it still occurs and can take up to a couple of hours.\nWe decided that we can start with taking data from offers’ aggregate built for\nfile-based feeds. We didn’t want to send all offers that should be integrated\nwith a partner at every job’s run because in most cases we would generate\nredundant events. Some offers didn’t change between two successive runs at all,\nsome of them were newly added or removed from the feed, but in most\ncases they had only partial updates e.g. price change. So we had the idea of sending\njust the difference between the previous and current event\nfeed state. How? Here’s a simplified version of algorithm for this approach:\nload latest state of all Allegro’s offers from HDFS - let’s call it aggregate,\nextract from aggregate and save on HDFS only the offers that should be\n included in event feed - let’s call it snapshot X,\nload the previous state of the event feed (from the previous run) - snapshot\n Y,\nmake a full join on X and Y using offer’s unique key - dataset Z of type\n Tuple(OfferStateX, OfferStateY),\ndecide to generate appropriate events based on dataset Z:\n    \nif both values are non-empty, generate an event with the calculated difference between state X and Y,\nif the value of X is empty, generate an event on removal from the feed,\nif the Y value is empty, generate an event on addition of a new offer to the event feed,\nsend generated events to Kafka topic that is constantly consumed by the\n service (connector) responsible for sending offers to a marketing partner,\nsave snapshot X on HDFS (in the next run it will act as a snapshot Y)\n\nI’m sure you’re wondering how much latency this solution adds.\nWell, it turned out to be only 20 minutes and in our case it is totally acceptable.\nIt is also worth mentioning that our Kafka topic is scalable in case a new partnership appears.\nThis is because the event model contains information about its destinations.\nThanks to this approach, we reduce the amount of data sent, thus limiting the\ntraffic of millions of sent events to just tens of thousands.\nSelf-healing system\nEvery complex system is prone to inconsistencies. Especially when this\ncomplexity increases and it is hard to stop that - as it was before our big\nrefactor. New architecture let us create a self-healing and fully controllable\nsystem that is convenient to maintain even taking into account the scale we\nface everyday. When designing the architecture we focused mainly on two things:\navailability and control.\nAvailability\nThe first step that should be considered is: how to properly\ndefine the responsibilities of individual system components?\nSystem works as a sum of cooperating elements. But it is easier to maintain\nthese elements when they have very specific tasks to handle. There are three\nmain components in our system (introduced earlier):\nAggregator - knows everything about every offer’s current state. It gathers\nall needed data and saves them to HDFS,\nGenerator - it takes data generated by Aggregate, filters it and prepares\nfor delivery to a partner,\nConnector (possibility of having many connectors) - holds integration with\npartner, acts as a data mapper and sender.\nAggregator and Generator are based on a complementary set of information\nabout the whole system and offers’ state at a certain point in time.\nSo in case the aggregate contains damaged offers and the generator already\ntook it to prepare for sending, in the next cycle it will get overwritten by fixed ones.\nThis happens because each cycle run’s results overwrite the previous ones.\nAdditionally, both Aggregate and Generator stage results are persisted on HDFS.\nThanks to this we can run the whole computation for any period of time and go back to\nany system state. Also, the Generator stage can be based on data generated at\nany time. In case Aggregate is failing while generating new data, Generator\nworks properly using earlier data.\nThen, we have a Connector. It consumes events from Kafka and pushes them,\nin appropriate form, on partner’s API. It has no responsibility for checking\ndata or state correctness. It simply gets what the Generator prepared and\ntries to deliver it to a partner. Thanks to this separation of responsibility,\nConnector is not dependent on Generator - even if Generator has a breakdown,\nthe Connector at worst may have nothing to do.\nControl\nIn the previous paragraph we mentioned a few processing issues we are\nstruggling with. However, we also proved that despite this our system can still\nwork in such conditions - maybe not as effectively as in standard scenarios,\nbut it still does. To react faster, we’ve managed to make quite “garbage\ndata”-resistant, notifications-based alerting system that will alarm about\nanomalies occuring during computation. In short, if the difference\nbetween states of previous and current Generator run is significant (experience\nbased numbers), the system will stop and inform us about it so that we can\ndecide if this change is acceptable or not. (By difference between states I\nmean difference between parameters such as feed’s offer count, number of\noffers’ parameters changes etc.) Once the change is approved, the system returns\nto its work. Otherwise, data is not propagated from Generator to Kafka, resulting\nin lack of data to be consumed by Connector. Even if we pass some incorrect\ndata to a partner and it will be too late to retreat, we have a\nspecial mechanism refreshing any offer that was updated more than 28 days\nago. So if an offer wasn’t updated for such a long time, it doesn’t matter if\nit is damaged or not – it will be refreshed eventually.\nSummary\nKey takeaway points:\nJust because something does not work well it doesn’t mean the tool is bad.\nMaybe there is something wrong with the way you are using it?\nIdeas can be complex but it doesn’t mean that they have to be complicated!\nResearch is key. Even if your business tells you there is no time for it, insist on it.\nOtherwise you will end up spending even more time on fixes.\nApache Spark is a beast. It can simplify your computation dramatically and\ngive amazing results with it, but at the same time you need to\nthink more about how your data will be calculated. One small problem may result\nin slow computation. Unfortunately lots of them are hard to notice.\nJoin us if you like such challenges :-)","guid":"https://blog.allegro.tech/2020/12/bigdata-marketing.html","categories":["architecture","bigdata","spark"],"isoDate":"2020-12-06T23:00:00.000Z","thumbnail":"images/post-headers/spark.png"}],"jobs":[{"id":"743999732620819","name":"Senior Software Engineer","uuid":"54116ab3-33fa-44db-b6f0-46bf7b03fbaa","refNumber":"REF1898H","company":{"identifier":"Allegro","name":"Allegro"},"releasedDate":"2021-01-26T13:29:05.000Z","location":{"city":"Kraków","region":"Lesser Poland Voivodeship","country":"pl","remote":false},"industry":{"id":"internet","label":"Internet"},"department":{"id":"1013462","label":"Technology - Product Development"},"function":{"id":"information_technology","label":"Information Technology"},"typeOfEmployment":{"label":"Full-time"},"experienceLevel":{"id":"mid_senior_level","label":"Mid-Senior Level"},"customField":[{"fieldId":"58c15608e4b01d4b19ddf790","fieldLabel":"Proces rekrutacji","valueId":"c807eec2-8a53-4b55-b7c5-c03180f2059b","valueLabel":"IT Allegro"},{"fieldId":"COUNTRY","fieldLabel":"Country","valueId":"pl","valueLabel":"Poland"},{"fieldId":"58c13159e4b01d4b19ddf729","fieldLabel":"Department","valueId":"1013462","valueLabel":"Technology - Product Development"},{"fieldId":"58c158e9e4b0614667d5973e","fieldLabel":"Więcej lokalizacji","valueId":"fa9e2c82-b44f-40df-a699-4dea59cb0c13","valueLabel":"Nie"},{"fieldId":"58c13159e4b01d4b19ddf728","fieldLabel":"Brands","valueId":"4ccb4fab-6c3f-4ed0-9140-8533fe17447f","valueLabel":"Allegro"}],"ref":"https://api.smartrecruiters.com/v1/companies/allegro/postings/743999732620819","creator":{"name":"Jagoda Rusiniak"},"language":{"code":"pl","label":"Polish","labelNative":"polski"}},{"id":"743999732620056","name":"Senior Software Engineer","uuid":"8db5abff-5e5b-4feb-a06b-c164dd337c1f","refNumber":"REF1903G","company":{"identifier":"Allegro","name":"Allegro"},"releasedDate":"2021-01-26T13:21:46.000Z","location":{"city":"Warszawa","country":"pl","remote":false},"industry":{"id":"internet","label":"Internet"},"department":{"id":"1013462","label":"Technology - Product Development"},"function":{"id":"information_technology","label":"Information Technology"},"typeOfEmployment":{"label":"Full-time"},"experienceLevel":{"id":"mid_senior_level","label":"Mid-Senior Level"},"customField":[{"fieldId":"58c15608e4b01d4b19ddf790","fieldLabel":"Proces rekrutacji","valueId":"c807eec2-8a53-4b55-b7c5-c03180f2059b","valueLabel":"IT Allegro"},{"fieldId":"COUNTRY","fieldLabel":"Country","valueId":"pl","valueLabel":"Poland"},{"fieldId":"58c13159e4b01d4b19ddf729","fieldLabel":"Department","valueId":"1013462","valueLabel":"Technology - Product Development"},{"fieldId":"58c13159e4b01d4b19ddf728","fieldLabel":"Brands","valueId":"4ccb4fab-6c3f-4ed0-9140-8533fe17447f","valueLabel":"Allegro"},{"fieldId":"5cdab2c84cedfd0006e7758c","fieldLabel":"Key words","valueLabel":"java, scala, kotlin, developer, programista, inżynier, architekt, architecture, architect"}],"ref":"https://api.smartrecruiters.com/v1/companies/allegro/postings/743999732620056","creator":{"name":"Jagoda Rusiniak"},"language":{"code":"pl","label":"Polish","labelNative":"polski"}},{"id":"743999732606435","name":"Software Engineer (Big Data Team)","uuid":"ca7c0429-f844-4933-acac-1a6910b14da9","refNumber":"REF2348I","company":{"identifier":"Allegro","name":"Allegro"},"releasedDate":"2021-01-26T10:07:45.000Z","location":{"city":"Warszawa, Kraków, Poznań, Toruń","region":"Masovian Voivodeship","country":"pl","remote":false},"industry":{"id":"internet","label":"Internet"},"department":{"id":"1013462","label":"Technology - Product Development"},"function":{"id":"information_technology","label":"Information Technology"},"typeOfEmployment":{"label":"Full-time"},"experienceLevel":{"id":"mid_senior_level","label":"Mid-Senior Level"},"customField":[{"fieldId":"58c1575ee4b01d4b19ddf797","fieldLabel":"Kraków","valueId":"2cfcfcc1-da44-43f7-8eaa-3907faf2797c","valueLabel":"Tak"},{"fieldId":"58c158e9e4b0614667d5973e","fieldLabel":"Więcej lokalizacji","valueId":"3a186e8d-a82c-4955-af81-fcef9a63928a","valueLabel":"Tak"},{"fieldId":"58c1576ee4b0614667d59732","fieldLabel":"Toruń","valueId":"a6765624-e047-4a26-9481-9621086d8b96","valueLabel":"Nie"},{"fieldId":"58c156e6e4b01d4b19ddf793","fieldLabel":"Warszawa","valueId":"6a428533-1586-4372-89ec-65fb45366363","valueLabel":"Tak"},{"fieldId":"58c15608e4b01d4b19ddf790","fieldLabel":"Proces rekrutacji","valueId":"c807eec2-8a53-4b55-b7c5-c03180f2059b","valueLabel":"IT Allegro"},{"fieldId":"COUNTRY","fieldLabel":"Country","valueId":"pl","valueLabel":"Poland"},{"fieldId":"58c15788e4b0614667d59733","fieldLabel":"Błonie","valueId":"25f6cb8c-81b3-434a-93ec-6dc851d5808d","valueLabel":"Nie"},{"fieldId":"58c15798e4b01d4b19ddf79b","fieldLabel":"Wrocław","valueId":"31873284-1e97-427d-8918-6ce504344351","valueLabel":"Nie"},{"fieldId":"58c13159e4b01d4b19ddf729","fieldLabel":"Department","valueId":"1013462","valueLabel":"Technology - Product Development"},{"fieldId":"58c13159e4b01d4b19ddf728","fieldLabel":"Brands","valueId":"4ccb4fab-6c3f-4ed0-9140-8533fe17447f","valueLabel":"Allegro"},{"fieldId":"58c156b9e4b01d4b19ddf792","fieldLabel":"Poznań","valueId":"92fb81b1-a3eb-4b30-8571-0dfdfcb911d9","valueLabel":"Nie"},{"fieldId":"5cdab2c84cedfd0006e7758c","fieldLabel":"Key words","valueLabel":"Software Engineer, Developer, Programista, Java, Scala, Kotlin, Big Data, Hadoop, Spark"}],"ref":"https://api.smartrecruiters.com/v1/companies/allegro/postings/743999732606435","creator":{"name":"Angelika Szymkiewicz"},"language":{"code":"pl","label":"Polish","labelNative":"polski"}},{"id":"743999732498611","name":"Consumer Strategy Team Leader","uuid":"9140e17b-f9dc-49ba-a951-d6f11fb212ec","refNumber":"REF2515K","company":{"identifier":"Allegro","name":"Allegro"},"releasedDate":"2021-01-25T15:09:15.000Z","location":{"city":"Warszawa, Poznań","region":"Masovian Voivodeship","country":"pl","remote":false},"industry":{"id":"internet","label":"Internet"},"department":{"id":"1013462","label":"Technology - Product Development"},"function":{"id":"analyst","label":"Analyst"},"typeOfEmployment":{"label":"Full-time"},"experienceLevel":{"id":"mid_senior_level","label":"Mid-Senior Level"},"customField":[{"fieldId":"58c1575ee4b01d4b19ddf797","fieldLabel":"Kraków","valueId":"bcab9d4f-8624-4d85-8e3d-dbf12239b972","valueLabel":"Nie"},{"fieldId":"58c158e9e4b0614667d5973e","fieldLabel":"Więcej lokalizacji","valueId":"3a186e8d-a82c-4955-af81-fcef9a63928a","valueLabel":"Tak"},{"fieldId":"58c1576ee4b0614667d59732","fieldLabel":"Toruń","valueId":"a6765624-e047-4a26-9481-9621086d8b96","valueLabel":"Nie"},{"fieldId":"58c156e6e4b01d4b19ddf793","fieldLabel":"Warszawa","valueId":"6a428533-1586-4372-89ec-65fb45366363","valueLabel":"Tak"},{"fieldId":"58c15608e4b01d4b19ddf790","fieldLabel":"Proces rekrutacji","valueId":"c807eec2-8a53-4b55-b7c5-c03180f2059b","valueLabel":"IT Allegro"},{"fieldId":"COUNTRY","fieldLabel":"Country","valueId":"pl","valueLabel":"Poland"},{"fieldId":"58c15788e4b0614667d59733","fieldLabel":"Błonie","valueId":"25f6cb8c-81b3-434a-93ec-6dc851d5808d","valueLabel":"Nie"},{"fieldId":"58c15798e4b01d4b19ddf79b","fieldLabel":"Wrocław","valueId":"31873284-1e97-427d-8918-6ce504344351","valueLabel":"Nie"},{"fieldId":"58c13159e4b01d4b19ddf729","fieldLabel":"Department","valueId":"1013462","valueLabel":"Technology - Product Development"},{"fieldId":"58c13159e4b01d4b19ddf728","fieldLabel":"Brands","valueId":"4ccb4fab-6c3f-4ed0-9140-8533fe17447f","valueLabel":"Allegro"},{"fieldId":"5dee624907e370138f7ad0bd","fieldLabel":"Kompetencje Allegro","valueId":"236b702e-45b9-4c4b-82d4-01c640aaf881","valueLabel":"Nie"},{"fieldId":"58c156b9e4b01d4b19ddf792","fieldLabel":"Poznań","valueId":"ac20917b-cb6a-4280-aff3-4fae2532a33e","valueLabel":"Tak"},{"fieldId":"5cdab2c84cedfd0006e7758c","fieldLabel":"Key words","valueLabel":"lider, leader, analityk, analyst, business"}],"ref":"https://api.smartrecruiters.com/v1/companies/allegro/postings/743999732498611","creator":{"name":"Angelika Szymkiewicz"},"language":{"code":"pl","label":"Polish","labelNative":"polski"}},{"id":"743999732293504","name":"Big Data Engineer","uuid":"ef47f1e0-2de5-4727-8e1e-9ea14809062f","refNumber":"REF1795E","company":{"identifier":"Allegro","name":"Allegro"},"releasedDate":"2021-01-22T15:30:09.000Z","location":{"city":"Poznań, Warszawa, Kraków, Toruń","country":"pl","remote":false},"industry":{"id":"internet","label":"Internet"},"department":{"id":"1013462","label":"Technology - Product Development"},"function":{"id":"information_technology","label":"Information Technology"},"typeOfEmployment":{"label":"Full-time"},"experienceLevel":{"id":"mid_senior_level","label":"Mid-Senior Level"},"customField":[{"fieldId":"58c1575ee4b01d4b19ddf797","fieldLabel":"Kraków","valueId":"2cfcfcc1-da44-43f7-8eaa-3907faf2797c","valueLabel":"Tak"},{"fieldId":"58c15608e4b01d4b19ddf790","fieldLabel":"Proces rekrutacji","valueId":"c807eec2-8a53-4b55-b7c5-c03180f2059b","valueLabel":"IT Allegro"},{"fieldId":"COUNTRY","fieldLabel":"Country","valueId":"pl","valueLabel":"Poland"},{"fieldId":"58c158e9e4b0614667d5973e","fieldLabel":"Więcej lokalizacji","valueId":"3a186e8d-a82c-4955-af81-fcef9a63928a","valueLabel":"Tak"},{"fieldId":"58c13159e4b01d4b19ddf729","fieldLabel":"Department","valueId":"1013462","valueLabel":"Technology - Product Development"},{"fieldId":"58c13159e4b01d4b19ddf728","fieldLabel":"Brands","valueId":"4ccb4fab-6c3f-4ed0-9140-8533fe17447f","valueLabel":"Allegro"},{"fieldId":"58c156b9e4b01d4b19ddf792","fieldLabel":"Poznań","valueId":"ac20917b-cb6a-4280-aff3-4fae2532a33e","valueLabel":"Tak"},{"fieldId":"58c156e6e4b01d4b19ddf793","fieldLabel":"Warszawa","valueId":"6a428533-1586-4372-89ec-65fb45366363","valueLabel":"Tak"},{"fieldId":"5cdab2c84cedfd0006e7758c","fieldLabel":"Key words","valueLabel":"Java, Scala, Spark, Druid, Airflow, Big Data, bigdata, Google Cloud Platform, Dataflow, BigQuery, unix, linux, developer, programista, programmer, dev, inżynier, engineer, Spark, Hadoop"}],"ref":"https://api.smartrecruiters.com/v1/companies/allegro/postings/743999732293504","creator":{"name":"Adriana Gesiarz"},"language":{"code":"pl","label":"Polish","labelNative":"polski"}}],"events":[{"created":1596114124000,"duration":5400000,"id":"272249143","name":"Allegro Tech Talks #13 - Cyfrodziewczyny","date_in_series_pattern":false,"status":"past","time":1596643200000,"local_date":"2020-08-05","local_time":"18:00","updated":1596650397000,"utc_offset":7200000,"waitlist_count":0,"yes_rsvp_count":40,"venue":{"id":26906060,"name":"Online event","repinned":false,"country":"","localized_country_name":""},"is_online_event":true,"group":{"created":1425052059000,"name":"allegro Tech","id":18465254,"join_mode":"open","lat":52.2599983215332,"lon":21.020000457763672,"urlname":"allegrotech","who":"Techs","localized_location":"Warsaw, Poland","state":"","country":"pl","region":"en_US","timezone":"Europe/Warsaw"},"link":"https://www.meetup.com/allegrotech/events/272249143/","description":"Allegro Tech Live to nowa (w 100% zdalna) odsłona naszych stacjonarnych meetupów Allegro Tech Talks. Zazwyczaj spotykaliśmy się w naszych biurach, ale tym razem to…","how_to_find_us":"https://www.facebook.com/allegro.tech/","visibility":"public","member_pay_fee":false},{"created":1593525736000,"duration":3600000,"id":"271624844","name":"Allegro Tech Live #12 - Wszystko o licencjach Open Source","date_in_series_pattern":false,"status":"past","time":1593619200000,"local_date":"2020-07-01","local_time":"18:00","updated":1593625843000,"utc_offset":7200000,"waitlist_count":0,"yes_rsvp_count":34,"venue":{"id":26906060,"name":"Online event","repinned":false,"country":"","localized_country_name":""},"is_online_event":true,"group":{"created":1425052059000,"name":"allegro Tech","id":18465254,"join_mode":"open","lat":52.2599983215332,"lon":21.020000457763672,"urlname":"allegrotech","who":"Techs","localized_location":"Warsaw, Poland","state":"","country":"pl","region":"en_US","timezone":"Europe/Warsaw"},"link":"https://www.meetup.com/allegrotech/events/271624844/","description":"Allegro Tech Live to nowa (w 100% zdalna) odsłona naszych stacjonarnych meetupów Allegro Tech Talks. Zazwyczaj spotykaliśmy się w naszych biurach, ale tym razem to…","how_to_find_us":"https://www.facebook.com/allegro.tech","visibility":"public","member_pay_fee":false},{"created":1592578931000,"duration":7200000,"id":"271396824","name":"Allegro Tech Live #11","date_in_series_pattern":false,"status":"past","time":1593014400000,"local_date":"2020-06-24","local_time":"18:00","updated":1593026541000,"utc_offset":7200000,"waitlist_count":0,"yes_rsvp_count":62,"venue":{"id":26906060,"name":"Online event","repinned":false,"country":"","localized_country_name":""},"is_online_event":true,"group":{"created":1425052059000,"name":"allegro Tech","id":18465254,"join_mode":"open","lat":52.2599983215332,"lon":21.020000457763672,"urlname":"allegrotech","who":"Techs","localized_location":"Warsaw, Poland","state":"","country":"pl","region":"en_US","timezone":"Europe/Warsaw"},"link":"https://www.meetup.com/allegrotech/events/271396824/","description":"Allegro Tech Live to nowa (w 100% zdalna) odsłona naszych stacjonarnych meetupów Allegro Tech Talks. Zazwyczaj spotykaliśmy się w naszych biurach, ale tym razem to…","how_to_find_us":"https://www.facebook.com/allegro.tech","visibility":"public","member_pay_fee":false},{"created":1591785401000,"duration":7200000,"id":"271201706","name":"Allegro Tech Live #10 - ML","date_in_series_pattern":false,"status":"past","time":1592409600000,"local_date":"2020-06-17","local_time":"18:00","updated":1592418866000,"utc_offset":7200000,"waitlist_count":0,"yes_rsvp_count":69,"venue":{"id":26906060,"name":"Online event","repinned":false,"country":"","localized_country_name":""},"is_online_event":true,"group":{"created":1425052059000,"name":"allegro Tech","id":18465254,"join_mode":"open","lat":52.2599983215332,"lon":21.020000457763672,"urlname":"allegrotech","who":"Techs","localized_location":"Warsaw, Poland","state":"","country":"pl","region":"en_US","timezone":"Europe/Warsaw"},"link":"https://www.meetup.com/allegrotech/events/271201706/","description":"Allegro Tech Live to nowa (w 100% zdalna) odsłona naszych stacjonarnych meetupów Allegro Tech Talks. Zazwyczaj spotykaliśmy się w naszych biurach, ale tym razem to…","how_to_find_us":"https://www.facebook.com/allegro.tech/","visibility":"public","member_pay_fee":false}],"podcasts":[{"creator":{"name":["Ireneusz Gawlik"]},"title":"Badania i rozwój ML w Allegro","link":"https://podcast.allegro.tech/badania_i_rozwoj_ml_w_allegro","pubDate":"Mon, 29 Jun 2020 00:00:00 GMT","author":{"name":["Ireneusz Gawlik"]},"enclosure":{"url":"https://www.buzzsprout.com/887914/2998819-13-badania-i-rozwoj-ml-w-allegro-ireneusz-gawlik.mp3","type":"audio/mpeg"},"content":"Gdzie kryje się Machine Learning w Allegro? Jakie projekty już dzisiaj korzystają z mocy sztucznej inteligencji? Jak na codzień pracuje grupa badaczy nie tylko aplikująca, ale też rozwijająca algorytmy uczenia maszynowego? Irek, Team Manager w grupie Allegro ML Research, opowiada więcej o tym jak się tworzy AI w Polsce.","contentSnippet":"Gdzie kryje się Machine Learning w Allegro? Jakie projekty już dzisiaj korzystają z mocy sztucznej inteligencji? Jak na codzień pracuje grupa badaczy nie tylko aplikująca, ale też rozwijająca algorytmy uczenia maszynowego? Irek, Team Manager w grupie Allegro ML Research, opowiada więcej o tym jak się tworzy AI w Polsce.","guid":"https://podcast.allegro.tech/badania_i_rozwoj_ml_w_allegro","isoDate":"2020-06-29T00:00:00.000Z","itunes":{"author":"Ireneusz Gawlik","summary":"Gdzie kryje się Machine Learning w Allegro? Jakie projekty już dzisiaj korzystają z mocy sztucznej inteligencji? Jak na codzień pracuje grupa badaczy nie tylko aplikująca, ale też rozwijająca algorytmy uczenia maszynowego? Irek, Team Manager w grupie Allegro ML Research, opowiada więcej o tym jak się tworzy AI w Polsce.","explicit":"false"}},{"creator":{"name":["Kasia Wróbel"]},"title":"Projektowanie UX w Allegro, czyli wszystko zależy od doświadczenia","link":"https://podcast.allegro.tech/projektowanie_ux_w_allegro","pubDate":"Mon, 15 Jun 2020 00:00:00 GMT","author":{"name":["Kasia Wróbel"]},"enclosure":{"url":"https://www.buzzsprout.com/887914/3011998-11-projektowanie-ux-w-allegro-czyli-wszystko-zalezy-od-doswiadczenia-katarzyna-wrobel.mp3","type":"audio/mpeg"},"content":"Projektowanie na styku produktu, biznesu i potrzeb użytkownika. Budzenie ciekawości, czyli czy jesteś w stanie zmienić coś, co sprawia problem tak wielu osobom na raz? Kasia opowiada o \"układaniu klocków\", których użytkownik wie jak użyć.","contentSnippet":"Projektowanie na styku produktu, biznesu i potrzeb użytkownika. Budzenie ciekawości, czyli czy jesteś w stanie zmienić coś, co sprawia problem tak wielu osobom na raz? Kasia opowiada o \"układaniu klocków\", których użytkownik wie jak użyć.","guid":"https://podcast.allegro.tech/projektowanie_ux_w_allegro","isoDate":"2020-06-15T00:00:00.000Z","itunes":{"author":"Kasia Wróbel","summary":"Projektowanie na styku produktu, biznesu i potrzeb użytkownika. Budzenie ciekawości, czyli czy jesteś w stanie zmienić coś, co sprawia problem tak wielu osobom na raz? Kasia opowiada o \"układaniu klocków\", których użytkownik wie jak użyć.","explicit":"false"}},{"creator":{"name":["Kamil Borzym"]},"title":"Mobile w Allegro","link":"https://podcast.allegro.tech/mobile_w_allegro","pubDate":"Mon, 01 Jun 2020 00:00:00 GMT","author":{"name":["Kamil Borzym"]},"enclosure":{"url":"https://www.buzzsprout.com/887914/3011989-11-mobile-w-allegro-kamil-borzym.mp3","type":"audio/mpeg"},"content":"Cały świat obserwuje jak telefony przejmują władzę nad Internetem. Ponad połowa ruchu na Allegro pochodzi z urządzeń mobilnych, a większość tego ruchu to aplikacje natywne. Żarty się skończyły ;) Kamil opowie Wam jak specjaliści mobilni z Allegro skalują development swojej aplikacji.","contentSnippet":"Cały świat obserwuje jak telefony przejmują władzę nad Internetem. Ponad połowa ruchu na Allegro pochodzi z urządzeń mobilnych, a większość tego ruchu to aplikacje natywne. Żarty się skończyły ;) Kamil opowie Wam jak specjaliści mobilni z Allegro skalują development swojej aplikacji.","guid":"https://podcast.allegro.tech/mobile_w_allegro","isoDate":"2020-06-01T00:00:00.000Z","itunes":{"author":"Kamil Borzym","summary":"Cały świat obserwuje jak telefony przejmują władzę nad Internetem. Ponad połowa ruchu na Allegro pochodzi z urządzeń mobilnych, a większość tego ruchu to aplikacje natywne. Żarty się skończyły ;) Kamil opowie Wam jak specjaliści mobilni z Allegro skalują development swojej aplikacji.","explicit":"false"}},{"creator":{"name":["Martyna Niszczota"]},"title":"Misja zmiana branży zakończona sukcesem! Jak wygląda dzień Product Managera","link":"https://podcast.allegro.tech/misja_zmiana_branzy_zakonczona_sukcesem","pubDate":"Mon, 18 May 2020 00:00:00 GMT","author":{"name":["Martyna Niszczota"]},"enclosure":{"url":"https://www.buzzsprout.com/887914/2799574-9-ep09_martyna_niszczota.mp3","type":"audio/mpeg"},"content":"Czy pracując w IT trzeba rozmawiać z ludźmi? Budowanie relacji to mity i każdy skupia się na swoim kodzie? Gdzie w całej tej układance jest Klient, czy jest tylko odbiorcą naszych rozwiązań? Martyna odpowie na te pytania i przybliży nam jak wygląda jej dzień w roli Product Managera. Posiada ona background HRowy, dlatego jej spojrzenie na świat IT może różnić się od tego standardowego.","contentSnippet":"Czy pracując w IT trzeba rozmawiać z ludźmi? Budowanie relacji to mity i każdy skupia się na swoim kodzie? Gdzie w całej tej układance jest Klient, czy jest tylko odbiorcą naszych rozwiązań? Martyna odpowie na te pytania i przybliży nam jak wygląda jej dzień w roli Product Managera. Posiada ona background HRowy, dlatego jej spojrzenie na świat IT może różnić się od tego standardowego.","guid":"https://podcast.allegro.tech/misja_zmiana_branzy_zakonczona_sukcesem","isoDate":"2020-05-18T00:00:00.000Z","itunes":{"author":"Martyna Niszczota","summary":"Czy pracując w IT trzeba rozmawiać z ludźmi? Budowanie relacji to mity i każdy skupia się na swoim kodzie? Gdzie w całej tej układance jest Klient, czy jest tylko odbiorcą naszych rozwiązań? Martyna odpowie na te pytania i przybliży nam jak wygląda jej dzień w roli Product Managera. Posiada ona background HRowy, dlatego jej spojrzenie na świat IT może różnić się od tego standardowego.","explicit":"false"}}]},"__N_SSG":true},"page":"/","query":{},"buildId":"EUAbPy5IIJA7QtQS0vQ8C","nextExport":false,"isFallback":false,"gsp":true}</script><script nomodule="" src="/_next/static/chunks/polyfills-3cb8f7ab5d3930a1a759.js"></script><script src="/_next/static/chunks/main-23f4f2620a057c84f895.js" async=""></script><script src="/_next/static/chunks/webpack-50bee04d1dc61f8adf5b.js" async=""></script><script src="/_next/static/chunks/framework.b1290caeda235fc7bf40.js" async=""></script><script src="/_next/static/chunks/b651a894a35ea1e7d121ee74b6c7e8aa31faf049.71d68ec00d361c7548dd.js" async=""></script><script src="/_next/static/chunks/6753331a3a54fd35bc7d92cbaee2b67e833d028a.8bec33551a885abbb7ba.js" async=""></script><script src="/_next/static/chunks/pages/_app-3ea8ca3d27590bc99614.js" async=""></script><script src="/_next/static/chunks/pages/index-ea5413c5f647a984c342.js" async=""></script><script src="/_next/static/EUAbPy5IIJA7QtQS0vQ8C/_buildManifest.js" async=""></script><script src="/_next/static/EUAbPy5IIJA7QtQS0vQ8C/_ssgManifest.js" async=""></script></body></html>