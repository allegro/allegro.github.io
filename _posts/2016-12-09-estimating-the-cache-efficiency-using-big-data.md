---
layout: post
title: Estimating the cache efficiency using big-data
author: [filip.marszelewski]
tags: [tech, cache, big-data]
---

Caching is a good and well-known technique used to increase application performance. In microservices architecture, it
can also help to limit requests count between services, thus decrease overall system load. Usually small or medium
data sets, which are often read and rarely changed, are considered as a good candidate for caching. In this article
I focus on slightly different approach, by telling the story about items partial cache in Allegro listing.

## Background

### Some numbers to understand the problem

Allegro is the largest e-commerce site in Central Europe. Every day you can search and purchase in set of about
50 millions of items. You can see the search results with items data at
Allegro [listing page](http://allegro.pl/laptopy-491). Items data is stored in
[Cassandra](http://cassandra.apache.org/), which is super-fast, scalable and reliable NoSQL database.

As a team responsible for Allegro listing reliability and performance, we started to look for a way to
decrease Cassandra load generated by our application. The first obvious idea that appeared was to cache items in
listing application, however we placed two constraints:

- we did not want to build distributed cache — Cassandra is very efficient and performs well, actually it can act as
a cache itself so building another system that does nearly the same seems unreasonable;
- it is very impractical to cache all 50 millions of items in each instance of listing application due to very large
memory usage.

### Hypothesis - caching small fraction of items can help saving many requests

We asked ourselves a few difficult questions:
 
- is every of 50 millions items requested equally frequent?
- maybe some of items are not requested at all in some short period of time?
- maybe some of them are requested very often and we many requests can be saved by caching only few items?

There is no possibility to guess answers without data, so we decided to use our internal
[Hadoop](http://hadoop.apache.org/)-[Spark](http://spark.apache.org/) big-data stack
and [Jupyter](http://jupyter.org/) Notebooks to prove or debunk the hypothesis:
*„it is possible to significantly decrease traffic from listing application to Cassandra
by caching only a small percent of listing items”*.

## Estimating the cache efficiency using big-data

Whenever you search on Allegro by phrase or browse the category tree, the id of each item found is stored in Hadoop.
So let's get all ids found in given period of time (we had choosen 24 hour period for first try).

We define *n* as a number of finds and *f(n)* as a number of items which were found exactly *n* times.
For example, *f(7)* tells us how many items were found exactly 7 times in last 24 hours.

![Items count as a function of finds count](/img/articles/2016-12-09-estimating-the-cache-efficiency-using-big-data/items_chart.png)

Data analysis provided us few interesting facts, which are not visible in the chart:

- there are 46 millions of items found — thus some items were not requested at all;
- standard deviation of items' request count is pretty high, which means that some items are requested often and 
some of them rarely;
- the most often requested item was found 665,592 times
(for better readability *n* is limited to 100 in the chart, but domain of *f* is 1 ... 665,592).

Now let's consider what happens with Cassandra requests count.
We define *g(n)* as a number of Cassandra SELECTs generated by items which were found exactly *n* times.
Because each item found is one SELECT to Cassandra, *g(n)* is simply *f(n) * n*.

![SELECTs count as a function of finds count](/img/articles/2016-12-09-estimating-the-cache-efficiency-using-big-data/requests_chart.png)

Let’s compare two drawings above. It is simple to understand the cache efficiency now graphically.
We put vertical line on some threshold (for example, 10 finds per item).
In the upper chart the field of the red part is amount of cached items, while field of whole figure is
amount of all items. Similarly in the lower chart, the red part represents amount of cached requests whereas
whole figure is total amount of requests. As we can see, it is possible to save big percent of requests by
caching significantly smaller fraction of items. That proves our hyphotesis and encourages us that idea of
partial items cache is worth implementing.

### Computations for real-life situation

Computations for 24 hours and whole traffic do not fit our situation. We cannot cache items for one day, because 
changes in their data happen. Moreover, whole traffic taken into account is a distributed cache case, which violates
our initial constraints. So we repeated computations for 15 minutes time window and with traffic limited to
single service instance. We generated similar charts, but there were no straightforward conclusions from them.
We decided to investigate more detailed data to extract cache hit ratio as a function of fraction of items in cache.

Below we use following definitions:
- *n* - number of finds
- *f(n)* - number of items found exactly *n* times
- *f<sub>all</sub>* - all items count (1,043,397 in our case)
- *f<sub>cached</sub>(n) = f<sub>all</sub> - (f(1) + f(2) + ... + f(n))* - number of items cached for caching threshold *n*
- *fraction of items in cache = f<sub>cached</sub>(n) / f<sub>all</sub>*
- *g(n) = f(n) * n* - number of SELECTs generated by items found exactly *n* times
- *g<sub>all</sub>* - all SELECTs count (1,577,607 in our case)
- *g<sub>cached</sub>(n) = g<sub>all</sub> - (g(1) + g(2) + ... + g(n))* - number of cache hits for caching threshold *n*
- *cache hit ratio = g<sub>cached</sub>(n) / g<sub>all</sub>*

Here are te results for *n* up to 10:

|*n* |*f(n)*|*f<sub>cached</sub>(n)*|*fraction of items in cache*|*g(n)*|*g<sub>cached</sub>(n)*|*cache hit ratio*|
|---:|-----:|----------------------:|---------------------------:|-----:|----------------------:|----------------:|
| 1 | 853589 | 189808 | 0.182 | 853589 | 724018 | 0.459 |
| 2 | 114447 | 75361 | 0.072 | 228894 | 495124 | 0.314 |
| 3 | 34329 | 41032 | 0.039 | 102987 | 392137 | 0.249 |
| 4 | 14413 | 26619 | 0.026 | 57652 | 334485 | 0.212 |
| 5 | 7287 | 19332 | 0.019 | 36435 | 298050 | 0.189 |
| 6 | 4448 | 14884 | 0.014 | 26688 | 271362 | 0.172 |
| 7 | 2957 | 11927 | 0.011 | 20699 | 250663 | 0.159 |
| 8 | 2036 | 9891 | 0.009 | 16288 | 234375 | 0.149 |
| 9 | 1491 | 8400 | 0.008 | 13419 | 220956 | 0.140 |
| 10 | 1033 | 7367 | 0.007 | 10330 | 210626 | 0.134 |

![cache hit ratio as a function of fraction of items in cache](/img/articles/2016-12-09-estimating-the-cache-efficiency-using-big-data/cache_hits_vs_items_in_cache.png)

Chart generated from results (one results row is one blue dot in the chart) gives us clear conclusions:
- cache hit ratio is much higher than fraction of items in cache,
- each cache size should does its job, but cache efficiency grows slower than cache size,
thus smaller cache is preferred

## Cache implementation

### Algorithm

With very optimistic data, we started implementing cache in our [spring-boot](https://projects.spring.io/spring-boot/) 
based listing application.
We can desribe our very simple algorithm as follows: *„We count requests for each item. When request for item appears,
we serve its data from cache if it is present.
If not, data from Cassandra is loaded and if requests count for that item is above threshold, item data is cached
and requests counter cleared.”*

![Caching alghoritm with counters](/img/articles/2016-12-09-estimating-the-cache-efficiency-using-big-data/algorithm_with_counters.png)

The one weak point of our approach was clearing the counters. We could not allow to grow counters map size infinitely.
As the first approach we decided just to clear all counter when they reached given constant size. Of course this
solution affects cache efficiency, but not too much - as can be observed in the charts below.

We needed two special libraries for implementing our alghoritm.

### Primitives map

Our first need was primitive hash map for storing request counters. Why do we need primitive map?
The memory overhead for storing 1 million of counters is at least 24 MB (at least 12 bytes extra for each `Long` key and
12 bytes extra for each `Integer` value). And we need few or several millions of counters. 

We have read [Large HashMap benchmark](http://java-performance.info/large-hashmap-overview-jdk-fastutil-goldman-sachs-hppc-koloboke-trove/)
and gave a try to [Koloboke](https://koloboke.com/) library, which seemed to be the fastest and the most efficient
large map implementation for Java.

### Cache engine

The second dependency used is cache library. We were looking for something with elegant API allowing explicit puts
and gets. We did not want to utilize Spring annotations, because methods in our application
operate on collections, whereas cache operates on single items.
We decided to try [Caffeine](https://github.com/ben-manes/caffeine), which provides well-known
[Guava Cache](https://github.com/google/guava/wiki/CachesExplained) inspired API, but has
[better performance](https://github.com/ben-manes/caffeine/wiki/Benchmarks).

## In production - it works!

![Items cache efficiency in production](/img/articles/2016-12-09-estimating-the-cache-efficiency-using-big-data/efficiency.png)

Our cache works as expected! We achieved 27% cache hit ratio in peak hours by caching only 200,000 items per service
instance (remember there is 50,000,000 items in total). Thus, we significantly decreased Cassandra cluster load and
network traffic at the very small cost of 0.5 GB of RAM memory in each service instance.

As it was mentioned before, counter clearings slightly affected cache efficiency - it can be seen as the „teeth”
in the chart. 

## Further improvements - get rid of counters

The counters were the weakest point of our solution. They occupied large amount of memory and cache efficiency was
temporary decreased after counters clearing. We looked for a way to get rid of counters.
We can read about one of Caffeine features on its github project page:
*„size-based eviction when a maximum is exceeded based on frequency and recency”*.
We placed new hypothesis: *counters are unnecessary*, cache should does its job without their.

![Caching alghoritm without counters](/img/articles/2016-12-09-estimating-the-cache-efficiency-using-big-data/algorithm_without_counters.png)

To evaluate this idea, we deployed version without counters (algorithm in the drawing above) to one production instance
of service. Then we compared both versions.

![Version with counters vs without counters](/img/articles/2016-12-09-estimating-the-cache-efficiency-using-big-data/counters_comparison.png)

The chart above represents rate (per second) of cached requests (*cachedIdsCount.m1_rate*), actual Cassandra
requests (*cassandraIdsCount.m1_rate*) and the sum of both (*allIdsCount.m1_rate*). Instance *mesos-slave2047* had
version with counters, while *mesos-slave1025* had improved version without counters.
As we can see, with this improvement we simplified algorithm, decreased memory demand and removed unwanted „teeth”
caused by counter clearings. Internal cache eviction logic works better than our simple counters algorithm.

## What we have learned

We are proud of new value made from very practical big-data usage. We proved that cache can be successfully 
implemented in not obvious case and caching only a small part of large data set can significantly decrease resources
usage. All that gain comes from data characteristics. So our final conclusions and recommendations are:
- think about what and how you cache - it is much more than only add `@Cached` annotation in code;
- make usage from data - it may contain hidden treasures;
- make usage from cache library logic - it contains sophisticated algorithms that optimize cache efficiency, let them
do their job;
- look for cache usage in not obvious cases.
